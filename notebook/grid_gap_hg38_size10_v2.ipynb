{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 24 21:05:07 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   24C    P8     7W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 31%   26C    P8    22W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  On   | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 31%   32C    P8     1W / 250W |  10772MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  On   | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 31%   25C    P8    20W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  On   | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 31%   27C    P8     9W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 35%   58C    P2   118W / 250W |   9244MiB / 10989MiB |     97%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 41%   62C    P2    74W / 250W |   8857MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  On   | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 41%   63C    P2   163W / 250W |   8853MiB / 10989MiB |     93%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  GeForce RTX 208...  On   | 00000000:40:00.0 Off |                  N/A |\n",
      "| 38%   61C    P2   127W / 250W |   8857MiB / 10989MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  GeForce RTX 208...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 40%   67C    P2   218W / 250W |   8857MiB / 10989MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    2     30140      C   ...m63/miniconda3/envs/research/bin/python 10761MiB |\n",
      "|    5     36472      C   ...a3/envs/tensorflow-gpu-rdkit/bin/python   387MiB |\n",
      "|    5     40811      C   python                                      4423MiB |\n",
      "|    5     40813      C   python                                      4423MiB |\n",
      "|    6       471      C   python                                      4423MiB |\n",
      "|    6       474      C   python                                      4423MiB |\n",
      "|    7     20261      C   python                                      4423MiB |\n",
      "|    7     20263      C   python                                      4419MiB |\n",
      "|    8     34696      C   python                                      4423MiB |\n",
      "|    8     34698      C   python                                      4423MiB |\n",
      "|    9     33263      C   python                                      4423MiB |\n",
      "|    9     33265      C   python                                      4423MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, TimeDistributed, Dense, RepeatVector, CuDNNLSTM, GRU, Bidirectional, Input, CuDNNGRU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "import difflib\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "seq_length = 10\n",
    "val_loss_hist = []\n",
    "\n",
    "K.clear_session()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "X_train=np.load('prepData/X_train_gap_hg38_v3_chr2_size10.npy')\n",
    "X_val=np.load('prepData/X_val_gap_hg38_v3_chr2_size10.npy')\n",
    "X_test=np.load('prepData/X_test_gap_hg38_v3_chr2_size10.npy')\n",
    "y_train=np.load('prepData/y_train_gap_hg38_v3_chr2_size10.npy')\n",
    "y_val=np.load('prepData/y_val_gap_hg38_v3_chr2_size10.npy')\n",
    "y_test=np.load('prepData/y_test_gap_hg38_v3_chr2_size10.npy')\n",
    "\n",
    "y_train1 = np.load('prepData/y_train1_gap_hg38_v3_chr2_size10.npy')\n",
    "y_val1 = np.load('prepData/y_val1_gap_hg38_v3_chr2_size10.npy')\n",
    "y_test1 = np.load('prepData/y_test1_gap_hg38_v3_chr2_size10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 0 1]]\n",
      "\n",
      " [[1 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 0 0 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  [1 0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  ...\n",
      "  [0 1 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  [1 0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10, 5)\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/1\n",
      " - 228s - loss: 0.4573 - acc: 0.8700 - val_loss: 0.3607 - val_acc: 0.9133\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/2\n",
      " - 225s - loss: 0.5177 - acc: 0.8454 - val_loss: 0.4107 - val_acc: 0.8971\n",
      "Epoch 2/2\n",
      " - 225s - loss: 0.3737 - acc: 0.9030 - val_loss: 0.3558 - val_acc: 0.9126\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/10\n",
      " - 229s - loss: 0.4853 - acc: 0.8614 - val_loss: 0.3734 - val_acc: 0.9130\n",
      "Epoch 2/10\n",
      " - 227s - loss: 0.3426 - acc: 0.9153 - val_loss: 0.3319 - val_acc: 0.9221\n",
      "Epoch 3/10\n",
      " - 226s - loss: 0.3124 - acc: 0.9224 - val_loss: 0.3072 - val_acc: 0.9283\n",
      "Epoch 4/10\n",
      " - 224s - loss: 0.2933 - acc: 0.9277 - val_loss: 0.2912 - val_acc: 0.9323\n",
      "Epoch 5/10\n",
      " - 225s - loss: 0.2804 - acc: 0.9312 - val_loss: 0.2787 - val_acc: 0.9356\n",
      "Epoch 6/10\n",
      " - 225s - loss: 0.2720 - acc: 0.9333 - val_loss: 0.2746 - val_acc: 0.9368\n",
      "Epoch 7/10\n",
      " - 225s - loss: 0.2662 - acc: 0.9347 - val_loss: 0.2715 - val_acc: 0.9367\n",
      "Epoch 8/10\n",
      " - 226s - loss: 0.2618 - acc: 0.9357 - val_loss: 0.2655 - val_acc: 0.9387\n",
      "Epoch 9/10\n",
      " - 226s - loss: 0.2581 - acc: 0.9368 - val_loss: 0.2611 - val_acc: 0.9402\n",
      "Epoch 10/10\n",
      " - 225s - loss: 0.2549 - acc: 0.9377 - val_loss: 0.2595 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_3/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_4/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_5/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_6/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_4:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_7:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_8:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_11:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_12:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 0.25946872557912554]]\n",
      "0 0.37342648005911283\n",
      "1 0.33193594907011303\n",
      "2 0.3071597013941833\n",
      "3 0.29119593236063207\n",
      "4 0.27866011497165477\n",
      "5 0.27458589879529816\n",
      "6 0.2714666390844754\n",
      "7 0.26547548719814845\n",
      "8 0.26109861017337865\n",
      "9 0.25946872557912554\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/1\n",
      " - 221s - loss: 0.3277 - acc: 0.9152 - val_loss: 0.2366 - val_acc: 0.9486\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/2\n",
      " - 221s - loss: 0.3619 - acc: 0.9044 - val_loss: 0.2676 - val_acc: 0.9398\n",
      "Epoch 2/2\n",
      " - 220s - loss: 0.2432 - acc: 0.9418 - val_loss: 0.2317 - val_acc: 0.9493\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/10\n",
      " - 222s - loss: 0.3465 - acc: 0.9095 - val_loss: 0.2477 - val_acc: 0.9456\n",
      "Epoch 2/10\n",
      " - 220s - loss: 0.2299 - acc: 0.9457 - val_loss: 0.2275 - val_acc: 0.9497\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "source": [
    "nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "def decoder(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    for i in range(size):\n",
    "        if array[i].tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "            result=result+\"0\" \n",
    "        elif array[i].tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "            result=result+\"A\"\n",
    "        elif array[i].tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "            result=result+\"C\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "            result=result+\"G\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "            result=result+\"T\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "            result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "#model5 = load_model('model/seq2seq_nogap_camFam3_1mut.h5')\n",
    "def decoderY(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    \n",
    "    if array.tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "        result=result+\"0\" \n",
    "    elif array.tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "        result=result+\"A\"\n",
    "    elif array.tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "        result=result+\"C\"\n",
    "    elif array.tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "        result=result+\"G\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "        result=result+\"T\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "        result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def printHitMiss(a,b):\n",
    "    if a==b:\n",
    "        return 'Hit'\n",
    "    else:\n",
    "        return 'Miss'\n",
    "    \n",
    "print(X_test.shape)\n",
    "def accuracy(a, b):\n",
    "    count = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == b[i]:\n",
    "            count = count+1\n",
    "    return count/len(a)\n",
    "\n",
    "def accuracy2(a, b, c):\n",
    "    count = 0\n",
    "    count2 =0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != c[i]:\n",
    "            count2 = count2 +1\n",
    "        if a[i] != c[i] and b[i]==c[i]:\n",
    "            count = count+1\n",
    "    return count/count2\n",
    "\n",
    "def isMutation(a, b):\n",
    "    if a!= b:\n",
    "        print(\"mutation\")\n",
    "\n",
    "def predict(model):\n",
    "    x_true=[]\n",
    "    y_hat_list = []\n",
    "    y_true = []\n",
    "    predictions = model.predict(X_test, batch_size=250, verbose=0)\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        #print(prediction)\n",
    "        # print(prediction)\n",
    "        #x_index = np.argmax(testX[i], axis=1)\n",
    "        #print(prediction[i])\n",
    "        x_str = decoder(X_test[i])\n",
    "\n",
    "        #index = np.argmax(prediction)\n",
    "        index = np.random.choice(6, 3, p=prediction)\n",
    "        result = [nucleotide[index]]\n",
    "\n",
    "        print(''.join(x_str), ' -> ', ''.join(result),\n",
    "              \" true: \", ''.join(decoderY(y_test[i])), printHitMiss(''.join(result), ''.join(decoderY(y_test[i]))))\n",
    "        x_true.append(''.join(x_str[5]))\n",
    "        y_hat_list.append(''.join(result))\n",
    "        y_true.append(''.join(decoderY(y_test[i])))\n",
    "    sm=difflib.SequenceMatcher(None,y_hat_list,y_true)\n",
    "    sm2=difflib.SequenceMatcher(None,y_hat_list,x_true)\n",
    "    print()\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat_list, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat_list, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat_list, y_true)))\n",
    "    return x_true, y_hat_list, y_true\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def lstm_model(latent_dim, half):\n",
    "    encoder_inputs = Input(shape=(None, 5))\n",
    "    \n",
    "    encoder = Bidirectional(CuDNNLSTM(half, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = concatenate([forward_h, backward_h])\n",
    "    state_c = concatenate([forward_c, backward_c])\n",
    "    \n",
    "    \n",
    "    # only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder\n",
    "    decoder_inputs = Input(shape=(None, 5))\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(5, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Run training\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def modelFit(epoch, batchSize, latent_dim, half, X_train, y_train, y_train1):\n",
    "    model1, encoder_model1, decoder_model1 = lstm_model(latent_dim, half)\n",
    "    hist1 = model1.fit([X_train, y_train1], y_train,\n",
    "          batch_size=batchSize,\n",
    "          epochs=epoch,\n",
    "          validation_data=([X_val,y_val1], y_val),\n",
    "          verbose = 2\n",
    "         )\n",
    "    return hist1, model1, encoder_model1, decoder_model1\n",
    "\n",
    "def grid_search(latent, half,train_size, X_train, y_train, y_train1):\n",
    "    hist1, model1, encoder_model1, decoder_model1 = modelFit(1, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist2 ,model2, encoder_model2, decoder_model2 = modelFit(2, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist3 ,model3, encoder_model3, decoder_model3 = modelFit(10, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist4 ,model4, encoder_model4, decoder_model4 = modelFit(30, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist5 ,model5, encoder_model5, decoder_model5 = modelFit(50, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist6 ,model6, encoder_model6, decoder_model6 = modelFit(80, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist7 ,model7, encoder_model7, decoder_model7 = modelFit(100, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist8 ,model8, encoder_model8, decoder_model8 = modelFit(500, 100, latent, half)\n",
    "\n",
    "    model1.save(\"models/gap_hg38_v2_{}_{}_1.h5\".format(train_size,half))\n",
    "    model2.save(\"models/gap_hg38_v2_{}_{}_2.h5\".format(train_size,half))\n",
    "    model3.save(\"models/gap_hg38_v2_{}_{}_10.h5\".format(train_size,half))\n",
    "    #model4.save(\"models/_gap_hg38_v2_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #model5.save(\"models/_gap_hg38_v2_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #model6.save(\"models/_gap_hg38_v2_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #model7.save(\"models/_gap_hg38_v2_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #model8.save(\"_gap_hg38_v2_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    encoder_model1.save(\"models/Egap_hg38_v2_{}_{}_1.h5\".format(train_size,half))\n",
    "    encoder_model2.save(\"models/Egap_hg38_v2_{}_{}_2.h5\".format(train_size,half))\n",
    "    encoder_model3.save(\"models/Egap_hg38_v2_{}_{}_10.h5\".format(train_size,half))\n",
    "    #encoder_model4.save(\"models/E_gap_hg38_v2_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #encoder_model5.save(\"models/E_gap_hg38_v2_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #encoder_model6.save(\"models/E_gap_hg38_v2_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #encoder_model7.save(\"models/E_gap_hg38_v2_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #encoder_model8.save(\"E_gap_hg38_v2_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    decoder_model1.save(\"models/Dgap_hg38_v2_{}_{}_1.h5\".format(train_size,half))\n",
    "    decoder_model2.save(\"models/Dgap_hg38_v2_{}_{}_2.h5\".format(train_size,half))\n",
    "    decoder_model3.save(\"models/Dgap_hg38_v2_{}_{}_10.h5\".format(train_size,half))\n",
    "    #decoder_model4.save(\"models/D_gap_hg38_v2_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #decoder_model5.save(\"models/D_gap_hg38_v2_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #decoder_model6.save(\"models/D_gap_hg38_v2_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #decoder_model7.save(\"models/D_gap_hg38_v2_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #decoder_model8.save(\"D_gap_hg38_v2_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    count = [i for i in range(len(hist3.history['val_loss']))]\n",
    "    val_loss_hist.append([hist3.history['val_loss'].index(min(hist3.history['val_loss'])),min(hist3.history['val_loss'])])\n",
    "    print(val_loss_hist)\n",
    "    for i, value in zip(count, hist3.history['val_loss']):\n",
    "        print(i, value)\n",
    "\n",
    "grid_search(16, 8, 000, X_train, y_train, y_train1)        \n",
    "grid_search(32, 16, 000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 000, X_train, y_train, y_train1)\n",
    "#grid_search(8192, 4096, 000, X_train, y_train, y_train1)\n",
    "\n",
    "with open('loss_hist.txt', 'wb') as fp:\n",
    "    pickle.dump(val_loss_hist, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
