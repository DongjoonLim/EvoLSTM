{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx512, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Collecting scikit-learn\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b41b0f3ab38>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/scikit-learn/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b41b0f3a630>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/scikit-learn/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x2b41b0f3a748>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/scikit-learn/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install keras\n",
    "# !pip3 install cudnnenv\n",
    "# !pip install tensorflow-gpu\n",
    "# !pip install matplotlib\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, TimeDistributed, Dense, RepeatVector, CuDNNLSTM, GRU, Bidirectional, Input, CuDNNGRU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "import difflib\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 18:58:41.723539 47340782964288 deprecation_wrapper.py:119] From /home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";\n",
    "\n",
    "K.clear_session()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:12000]\n",
    "X_val=np.load('prepData/X_val_camFam3_1mutOnly_v3_chr2.npy')[:10000]\n",
    "X_test=np.load('prepData/X_test_camFam3_1mutOnly_v3_chr2.npy')[:4000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:12000]\n",
    "y_val=np.load('prepData/y_val_camFam3_1mutOnly_v3_chr2.npy')[:10000]\n",
    "y_test=np.load('prepData/y_test_camFam3_1mutOnly_v3_chr2.npy')[:4000]\n",
    "\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:12000]\n",
    "y_val1 = np.load('prepData/y_val1_camFam3_1mutOnly_v3_chr2.npy')[:10000]\n",
    "y_test1 = np.load('prepData/y_test1_camFam3_1mutOnly_v3_chr2.npy')[:4000]\n",
    "\n",
    "def concat(input1, input2):\n",
    "    result = []\n",
    "    for x, y in zip(input1, input2):\n",
    "        result.append(np.hstack((x, y)))\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "y_val1 = concat(X_val, y_val1)\n",
    "y_test1 = concat(X_test, y_test1)\n",
    "    \n",
    "# X_train=np.load('prepData/X_train_camFam3_1mut.npy')\n",
    "# X_val=np.load('prepData/X_val_camFam3_1mut.npy')\n",
    "# X_test=np.load('prepData/X_test_camFam3_1mut.npy')\n",
    "# y_train=np.load('prepData/y_train_camFam3_1mut.npy')\n",
    "# y_val=np.load('prepData/y_val_camFam3_1mut.npy')\n",
    "# y_test=np.load('prepData/y_test_camFam3_1mut.npy')\n",
    "\n",
    "# X_train=np.load('prepData20/X_train_camFam3_1mutOnly.npy')\n",
    "# X_val=np.load('prepData20/X_val_camFam3_1mutOnly.npy')\n",
    "# X_test=np.load('prepData20/X_test_camFam3_1mutOnly.npy')\n",
    "# y_train=np.load('prepData20/y_train_camFam3_1mutOnly.npy')\n",
    "# y_val=np.load('prepData20/y_val_camFam3_1mutOnly.npy')\n",
    "# y_test=np.load('prepData20/y_test_camFam3_1mutOnly.npy')\n",
    "nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "#model5 = load_model('model/seq2seq_nogap_camFam3_1mutOnly.h5')\n",
    "def decoder(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    for i in range(size):\n",
    "        if array[i].tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "            result=result+\"0\" \n",
    "        elif array[i].tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "            result=result+\"A\"\n",
    "        elif array[i].tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "            result=result+\"C\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "            result=result+\"G\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "            result=result+\"T\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "            result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "#model5 = load_model('model/seq2seq_nogap_camFam3_1mutOnly.h5')\n",
    "def decoderY(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    \n",
    "    if array.tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "        result=result+\"0\" \n",
    "    elif array.tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "        result=result+\"A\"\n",
    "    elif array.tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "        result=result+\"C\"\n",
    "    elif array.tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "        result=result+\"G\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "        result=result+\"T\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "        result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def printHitMiss(a,b):\n",
    "    if a==b:\n",
    "        return 'Hit'\n",
    "    else:\n",
    "        return 'Miss'\n",
    "    \n",
    "print(X_test.shape)\n",
    "def accuracy(a, b):\n",
    "    count = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == b[i]:\n",
    "            count = count+1\n",
    "    return count/len(a)\n",
    "\n",
    "def accuracy2(a, b, c):\n",
    "    count = 0\n",
    "    count2 =0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != c[i]:\n",
    "            count2 = count2 +1\n",
    "        if a[i] != c[i] and b[i]==c[i]:\n",
    "            count = count+1\n",
    "    return count/count2\n",
    "\n",
    "def isMutation(a, b):\n",
    "    if a!= b:\n",
    "        print(\"mutation\")\n",
    "\n",
    "def predict(model):\n",
    "    x_true=[]\n",
    "    y_hat_list = []\n",
    "    y_true = []\n",
    "    predictions = model.predict(X_test, batch_size=250, verbose=0)\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        #print(prediction)\n",
    "        # print(prediction)\n",
    "        #x_index = np.argmax(testX[i], axis=1)\n",
    "        #print(prediction[i])\n",
    "        x_str = decoder(X_test[i])\n",
    "\n",
    "        #index = np.argmax(prediction)\n",
    "        index = np.random.choice(6, 3, p=prediction)\n",
    "        result = [nucleotide[index]]\n",
    "\n",
    "        print(''.join(x_str), ' -> ', ''.join(result),\n",
    "              \" true: \", ''.join(decoderY(y_test[i])), printHitMiss(''.join(result), ''.join(decoderY(y_test[i]))))\n",
    "        x_true.append(''.join(x_str[5]))\n",
    "        y_hat_list.append(''.join(result))\n",
    "        y_true.append(''.join(decoderY(y_test[i])))\n",
    "    sm=difflib.SequenceMatcher(None,y_hat_list,y_true)\n",
    "    sm2=difflib.SequenceMatcher(None,y_hat_list,x_true)\n",
    "    print()\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat_list, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat_list, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat_list, y_true)))\n",
    "    return x_true, y_hat_list, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 19:01:17.775820 47340782964288 deprecation_wrapper.py:119] From /home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0814 19:01:17.971458 47340782964288 deprecation.py:323] From /home/dongjoon/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 171s 14ms/step - loss: 1.6385 - acc: 0.2548 - val_loss: 1.4875 - val_acc: 0.3102\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 1.4533 - acc: 0.3092 - val_loss: 1.3591 - val_acc: 0.3960\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 1.3566 - acc: 0.3518 - val_loss: 1.3124 - val_acc: 0.4012\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 1.2985 - acc: 0.4145 - val_loss: 1.2333 - val_acc: 0.5321\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 1.1640 - acc: 0.6007 - val_loss: 1.0554 - val_acc: 0.7358\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 7s 572us/step - loss: 1.6860 - acc: 0.2472 - val_loss: 1.5180 - val_acc: 0.3181\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 1.4540 - acc: 0.3185 - val_loss: 1.3581 - val_acc: 0.3489\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 1.3439 - acc: 0.3781 - val_loss: 1.2992 - val_acc: 0.3821\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 1.2895 - acc: 0.4156 - val_loss: 1.2518 - val_acc: 0.4170\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 1.2248 - acc: 0.4693 - val_loss: 1.1680 - val_acc: 0.4918\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 1.1145 - acc: 0.5553 - val_loss: 1.0647 - val_acc: 0.5590\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 1.0217 - acc: 0.5995 - val_loss: 0.9910 - val_acc: 0.6084\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.9499 - acc: 0.6567 - val_loss: 0.9191 - val_acc: 0.6907\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.8764 - acc: 0.7408 - val_loss: 0.8404 - val_acc: 0.7807\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.8044 - acc: 0.8170 - val_loss: 0.7667 - val_acc: 0.8543\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 7s 600us/step - loss: 1.5840 - acc: 0.2446 - val_loss: 1.4119 - val_acc: 0.3232\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 1.4044 - acc: 0.2929 - val_loss: 1.3774 - val_acc: 0.3494\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 1.3763 - acc: 0.3491 - val_loss: 1.3360 - val_acc: 0.3934\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 1.3124 - acc: 0.3912 - val_loss: 1.2518 - val_acc: 0.4891\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 1.1950 - acc: 0.4953 - val_loss: 1.0958 - val_acc: 0.5942\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 1.0746 - acc: 0.5308 - val_loss: 1.0143 - val_acc: 0.5962\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 1.0258 - acc: 0.5309 - val_loss: 0.9817 - val_acc: 0.5962\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 1.0026 - acc: 0.5309 - val_loss: 0.9644 - val_acc: 0.5962\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.9883 - acc: 0.5309 - val_loss: 0.9523 - val_acc: 0.5962\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.9762 - acc: 0.5309 - val_loss: 0.9377 - val_acc: 0.5962\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.9490 - acc: 0.5464 - val_loss: 0.8910 - val_acc: 0.6508\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.8860 - acc: 0.7010 - val_loss: 0.8259 - val_acc: 0.7872\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.8236 - acc: 0.8532 - val_loss: 0.7749 - val_acc: 0.8984\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.7710 - acc: 0.8961 - val_loss: 0.7268 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.7224 - acc: 0.8969 - val_loss: 0.6813 - val_acc: 0.9004\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.6743 - acc: 0.8975 - val_loss: 0.6350 - val_acc: 0.9004\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.6248 - acc: 0.8994 - val_loss: 0.5931 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.5806 - acc: 0.9000 - val_loss: 0.5566 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.5456 - acc: 0.9000 - val_loss: 0.5312 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.5190 - acc: 0.9000 - val_loss: 0.5096 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 8s 651us/step - loss: 1.6975 - acc: 0.2338 - val_loss: 1.5527 - val_acc: 0.3293\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 1.4604 - acc: 0.3347 - val_loss: 1.3856 - val_acc: 0.4315\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 1.3405 - acc: 0.4506 - val_loss: 1.2670 - val_acc: 0.5521\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 1.2060 - acc: 0.5421 - val_loss: 1.1238 - val_acc: 0.5969\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 1.0840 - acc: 0.5573 - val_loss: 1.0351 - val_acc: 0.6052\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 1.0160 - acc: 0.5720 - val_loss: 0.9865 - val_acc: 0.6169\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.9746 - acc: 0.5869 - val_loss: 0.9545 - val_acc: 0.6263\n",
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.9434 - acc: 0.5981 - val_loss: 0.9277 - val_acc: 0.6373\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.9149 - acc: 0.6106 - val_loss: 0.8987 - val_acc: 0.6521\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.8793 - acc: 0.6375 - val_loss: 0.8554 - val_acc: 0.6878\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.8263 - acc: 0.6908 - val_loss: 0.7994 - val_acc: 0.7416\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.7651 - acc: 0.7314 - val_loss: 0.7453 - val_acc: 0.7623\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.7054 - acc: 0.7674 - val_loss: 0.6910 - val_acc: 0.8060\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.6398 - acc: 0.8453 - val_loss: 0.6222 - val_acc: 0.8893\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.5660 - acc: 0.8977 - val_loss: 0.5480 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4994 - acc: 0.9000 - val_loss: 0.4897 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4606 - acc: 0.9000 - val_loss: 0.4626 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4423 - acc: 0.9000 - val_loss: 0.4494 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4325 - acc: 0.9000 - val_loss: 0.4434 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4266 - acc: 0.9000 - val_loss: 0.4388 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.4228 - acc: 0.9000 - val_loss: 0.4368 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.4202 - acc: 0.9000 - val_loss: 0.4330 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4184 - acc: 0.9000 - val_loss: 0.4333 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4170 - acc: 0.9000 - val_loss: 0.4332 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.4159 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4150 - acc: 0.9000 - val_loss: 0.4311 - val_acc: 0.9000\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4143 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4136 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4131 - acc: 0.9000 - val_loss: 0.4312 - val_acc: 0.9000\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4125 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 8s 634us/step - loss: 1.6142 - acc: 0.3127 - val_loss: 1.4512 - val_acc: 0.3931\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 1.3590 - acc: 0.4500 - val_loss: 1.2217 - val_acc: 0.5589\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 1.1485 - acc: 0.5621 - val_loss: 1.0432 - val_acc: 0.6137\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 1.0250 - acc: 0.6050 - val_loss: 0.9520 - val_acc: 0.6423\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.9502 - acc: 0.6535 - val_loss: 0.8845 - val_acc: 0.7095\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.8785 - acc: 0.7196 - val_loss: 0.8105 - val_acc: 0.7682\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.7503 - acc: 0.8561 - val_loss: 0.6500 - val_acc: 0.8998\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.6043 - acc: 0.9000 - val_loss: 0.5549 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.5307 - acc: 0.9000 - val_loss: 0.5057 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4898 - acc: 0.9000 - val_loss: 0.4776 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4659 - acc: 0.9000 - val_loss: 0.4632 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.4518 - acc: 0.9000 - val_loss: 0.4543 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4432 - acc: 0.9000 - val_loss: 0.4490 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4379 - acc: 0.9000 - val_loss: 0.4465 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.4345 - acc: 0.9000 - val_loss: 0.4454 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.4323 - acc: 0.9000 - val_loss: 0.4439 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4308 - acc: 0.9000 - val_loss: 0.4443 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4297 - acc: 0.9000 - val_loss: 0.4432 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.4289 - acc: 0.9000 - val_loss: 0.4443 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4283 - acc: 0.9000 - val_loss: 0.4432 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4278 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4273 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4269 - acc: 0.9000 - val_loss: 0.4427 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4265 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4262 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4259 - acc: 0.9000 - val_loss: 0.4423 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4255 - acc: 0.9000 - val_loss: 0.4418 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4252 - acc: 0.9000 - val_loss: 0.4428 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4249 - acc: 0.9000 - val_loss: 0.4429 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4246 - acc: 0.9000 - val_loss: 0.4428 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4244 - acc: 0.9000 - val_loss: 0.4418 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.4241 - acc: 0.9000 - val_loss: 0.4423 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4239 - acc: 0.9000 - val_loss: 0.4432 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4236 - acc: 0.9000 - val_loss: 0.4420 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4234 - acc: 0.9000 - val_loss: 0.4436 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.4232 - acc: 0.9000 - val_loss: 0.4416 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4230 - acc: 0.9000 - val_loss: 0.4428 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4228 - acc: 0.9000 - val_loss: 0.4426 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.4226 - acc: 0.9000 - val_loss: 0.4421 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.4223 - acc: 0.9000 - val_loss: 0.4429 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4221 - acc: 0.9000 - val_loss: 0.4426 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4219 - acc: 0.9000 - val_loss: 0.4422 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4217 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4215 - acc: 0.9000 - val_loss: 0.4421 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4212 - acc: 0.9000 - val_loss: 0.4414 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4210 - acc: 0.9000 - val_loss: 0.4422 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4207 - acc: 0.9000 - val_loss: 0.4415 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4204 - acc: 0.9000 - val_loss: 0.4406 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4202 - acc: 0.9000 - val_loss: 0.4430 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4199 - acc: 0.9000 - val_loss: 0.4406 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 8s 672us/step - loss: 1.6201 - acc: 0.2872 - val_loss: 1.4603 - val_acc: 0.3241\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 252us/step - loss: 1.4286 - acc: 0.3808 - val_loss: 1.3271 - val_acc: 0.4890\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 1.2727 - acc: 0.5209 - val_loss: 1.1692 - val_acc: 0.6138\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 1.1293 - acc: 0.6493 - val_loss: 1.0592 - val_acc: 0.6865\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 1.0338 - acc: 0.6846 - val_loss: 0.9865 - val_acc: 0.6991\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.9706 - acc: 0.7029 - val_loss: 0.9358 - val_acc: 0.7204\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.9221 - acc: 0.7389 - val_loss: 0.8903 - val_acc: 0.7771\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.8778 - acc: 0.7819 - val_loss: 0.8455 - val_acc: 0.8135\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.8353 - acc: 0.8132 - val_loss: 0.8040 - val_acc: 0.8387\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.7933 - acc: 0.8358 - val_loss: 0.7652 - val_acc: 0.8562\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.7485 - acc: 0.8628 - val_loss: 0.7226 - val_acc: 0.8819\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 3s 248us/step - loss: 0.6940 - acc: 0.8890 - val_loss: 0.6720 - val_acc: 0.8971\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.6328 - acc: 0.8990 - val_loss: 0.6206 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.5812 - acc: 0.9000 - val_loss: 0.5804 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.5411 - acc: 0.9000 - val_loss: 0.5487 - val_acc: 0.9000\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.5095 - acc: 0.9000 - val_loss: 0.5218 - val_acc: 0.9000\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4848 - acc: 0.9000 - val_loss: 0.4957 - val_acc: 0.9000\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4646 - acc: 0.9000 - val_loss: 0.4695 - val_acc: 0.9000\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4507 - acc: 0.9000 - val_loss: 0.4590 - val_acc: 0.9000\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4433 - acc: 0.9000 - val_loss: 0.4538 - val_acc: 0.9000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4386 - acc: 0.9000 - val_loss: 0.4510 - val_acc: 0.9000\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4350 - acc: 0.9000 - val_loss: 0.4483 - val_acc: 0.9000\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4322 - acc: 0.9000 - val_loss: 0.4467 - val_acc: 0.9000\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4298 - acc: 0.9000 - val_loss: 0.4463 - val_acc: 0.9000\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4277 - acc: 0.9000 - val_loss: 0.4445 - val_acc: 0.9000\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4257 - acc: 0.9000 - val_loss: 0.4436 - val_acc: 0.9000\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4239 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4220 - acc: 0.9000 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4202 - acc: 0.9000 - val_loss: 0.4409 - val_acc: 0.9000\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4185 - acc: 0.9000 - val_loss: 0.4396 - val_acc: 0.9000\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4168 - acc: 0.9000 - val_loss: 0.4384 - val_acc: 0.9000\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4155 - acc: 0.9000 - val_loss: 0.4398 - val_acc: 0.9000\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4143 - acc: 0.9000 - val_loss: 0.4383 - val_acc: 0.9000\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4134 - acc: 0.9000 - val_loss: 0.4378 - val_acc: 0.9000\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4127 - acc: 0.9000 - val_loss: 0.4371 - val_acc: 0.9000\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4120 - acc: 0.9000 - val_loss: 0.4365 - val_acc: 0.9000\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4115 - acc: 0.9000 - val_loss: 0.4355 - val_acc: 0.9000\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4110 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4104 - acc: 0.9000 - val_loss: 0.4378 - val_acc: 0.9000\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4100 - acc: 0.9000 - val_loss: 0.4339 - val_acc: 0.9000\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4096 - acc: 0.9000 - val_loss: 0.4349 - val_acc: 0.9000\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4092 - acc: 0.9000 - val_loss: 0.4349 - val_acc: 0.9000\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4088 - acc: 0.9000 - val_loss: 0.4312 - val_acc: 0.9000\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4085 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4082 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4078 - acc: 0.9000 - val_loss: 0.4336 - val_acc: 0.9000\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4075 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4072 - acc: 0.9000 - val_loss: 0.4313 - val_acc: 0.9000\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4070 - acc: 0.9000 - val_loss: 0.4307 - val_acc: 0.9000\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4067 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.4063 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4061 - acc: 0.9000 - val_loss: 0.4310 - val_acc: 0.9000\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4058 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4056 - acc: 0.9000 - val_loss: 0.4312 - val_acc: 0.9000\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4049 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4046 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4041 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4037 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 61/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4316 - val_acc: 0.9000\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4027 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4250 - val_acc: 0.9000\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4020 - acc: 0.9000 - val_loss: 0.4267 - val_acc: 0.9000\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4015 - acc: 0.9000 - val_loss: 0.4271 - val_acc: 0.9000\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4011 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4006 - acc: 0.9000 - val_loss: 0.4233 - val_acc: 0.9000\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.4001 - acc: 0.9000 - val_loss: 0.4270 - val_acc: 0.9000\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.3995 - acc: 0.9000 - val_loss: 0.4257 - val_acc: 0.9000\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3988 - acc: 0.9000 - val_loss: 0.4245 - val_acc: 0.9000\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3978 - acc: 0.9000 - val_loss: 0.4257 - val_acc: 0.8999\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3966 - acc: 0.9000 - val_loss: 0.4240 - val_acc: 0.9000\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3948 - acc: 0.8999 - val_loss: 0.4221 - val_acc: 0.8999\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3920 - acc: 0.8999 - val_loss: 0.4190 - val_acc: 0.8999\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3887 - acc: 0.8999 - val_loss: 0.4160 - val_acc: 0.8998\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3849 - acc: 0.8998 - val_loss: 0.4142 - val_acc: 0.8994\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3810 - acc: 0.8998 - val_loss: 0.4103 - val_acc: 0.8993\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3775 - acc: 0.8998 - val_loss: 0.4049 - val_acc: 0.8995\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3741 - acc: 0.8998 - val_loss: 0.4079 - val_acc: 0.8989\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 8s 670us/step - loss: 1.6517 - acc: 0.2218 - val_loss: 1.5004 - val_acc: 0.2223\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 1.4439 - acc: 0.2932 - val_loss: 1.3818 - val_acc: 0.3256\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 1.3618 - acc: 0.3597 - val_loss: 1.3224 - val_acc: 0.3876\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 1.2975 - acc: 0.4134 - val_loss: 1.2427 - val_acc: 0.4538\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 1.1944 - acc: 0.5112 - val_loss: 1.1294 - val_acc: 0.5423\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 1.0885 - acc: 0.5649 - val_loss: 1.0468 - val_acc: 0.5565\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 1.0263 - acc: 0.5610 - val_loss: 1.0069 - val_acc: 0.5570\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.9938 - acc: 0.5673 - val_loss: 0.9831 - val_acc: 0.5928\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.9653 - acc: 0.6040 - val_loss: 0.9505 - val_acc: 0.6483\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.8931 - acc: 0.7465 - val_loss: 0.8174 - val_acc: 0.8906\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.7355 - acc: 0.8991 - val_loss: 0.6903 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.6428 - acc: 0.9000 - val_loss: 0.6349 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.5983 - acc: 0.9000 - val_loss: 0.6101 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.5753 - acc: 0.9000 - val_loss: 0.5985 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.5623 - acc: 0.9000 - val_loss: 0.5909 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.5535 - acc: 0.9000 - val_loss: 0.5883 - val_acc: 0.9000\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.5469 - acc: 0.9000 - val_loss: 0.5835 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.5412 - acc: 0.9000 - val_loss: 0.5814 - val_acc: 0.9000\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.5362 - acc: 0.9000 - val_loss: 0.5792 - val_acc: 0.9000\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.5313 - acc: 0.9000 - val_loss: 0.5774 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.5264 - acc: 0.9000 - val_loss: 0.5734 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.5214 - acc: 0.9000 - val_loss: 0.5695 - val_acc: 0.9000\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.5162 - acc: 0.9000 - val_loss: 0.5623 - val_acc: 0.9000\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.5103 - acc: 0.9000 - val_loss: 0.5543 - val_acc: 0.9000\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.5025 - acc: 0.9000 - val_loss: 0.5379 - val_acc: 0.9000\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4920 - acc: 0.9000 - val_loss: 0.5165 - val_acc: 0.9000\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4790 - acc: 0.9000 - val_loss: 0.4986 - val_acc: 0.9000\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4659 - acc: 0.9000 - val_loss: 0.4841 - val_acc: 0.9000\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4546 - acc: 0.9000 - val_loss: 0.4763 - val_acc: 0.9000\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4455 - acc: 0.9000 - val_loss: 0.4681 - val_acc: 0.9000\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4383 - acc: 0.9000 - val_loss: 0.4636 - val_acc: 0.9000\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4324 - acc: 0.9000 - val_loss: 0.4573 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4278 - acc: 0.9000 - val_loss: 0.4555 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4240 - acc: 0.9000 - val_loss: 0.4523 - val_acc: 0.9000\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4208 - acc: 0.9000 - val_loss: 0.4488 - val_acc: 0.9000\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4180 - acc: 0.9000 - val_loss: 0.4444 - val_acc: 0.9000\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4155 - acc: 0.9000 - val_loss: 0.4445 - val_acc: 0.9000\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4133 - acc: 0.9000 - val_loss: 0.4402 - val_acc: 0.9000\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4331 - val_acc: 0.9000\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4091 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.4073 - acc: 0.9000 - val_loss: 0.4293 - val_acc: 0.9000\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4279 - val_acc: 0.9000\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4038 - acc: 0.9000 - val_loss: 0.4267 - val_acc: 0.9000\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4006 - acc: 0.9000 - val_loss: 0.4246 - val_acc: 0.9000\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3996 - acc: 0.9000 - val_loss: 0.4239 - val_acc: 0.9000\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3989 - acc: 0.9000 - val_loss: 0.4273 - val_acc: 0.9000\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3974 - acc: 0.9000 - val_loss: 0.4257 - val_acc: 0.9000\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3959 - acc: 0.9000 - val_loss: 0.4246 - val_acc: 0.9000\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3941 - acc: 0.9000 - val_loss: 0.4259 - val_acc: 0.9000\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3920 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3895 - acc: 0.9000 - val_loss: 0.4199 - val_acc: 0.9000\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3868 - acc: 0.9000 - val_loss: 0.4165 - val_acc: 0.9000\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3837 - acc: 0.9000 - val_loss: 0.4153 - val_acc: 0.9000\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3808 - acc: 0.9000 - val_loss: 0.4125 - val_acc: 0.9000\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3780 - acc: 0.9000 - val_loss: 0.4069 - val_acc: 0.9000\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3754 - acc: 0.9000 - val_loss: 0.4071 - val_acc: 0.9000\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3729 - acc: 0.9000 - val_loss: 0.4021 - val_acc: 0.9000\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3715 - acc: 0.8999 - val_loss: 0.4018 - val_acc: 0.9000\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3695 - acc: 0.8999 - val_loss: 0.4013 - val_acc: 0.8999\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 3s 241us/step - loss: 0.3677 - acc: 0.8999 - val_loss: 0.3986 - val_acc: 0.8999\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 3s 248us/step - loss: 0.3664 - acc: 0.8999 - val_loss: 0.3976 - val_acc: 0.8999\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.3648 - acc: 0.8999 - val_loss: 0.3954 - val_acc: 0.8999\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3638 - acc: 0.8999 - val_loss: 0.3982 - val_acc: 0.8992\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3627 - acc: 0.8999 - val_loss: 0.3937 - val_acc: 0.8994\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3613 - acc: 0.8998 - val_loss: 0.3930 - val_acc: 0.8997\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3603 - acc: 0.8999 - val_loss: 0.3951 - val_acc: 0.8990\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3594 - acc: 0.8999 - val_loss: 0.3876 - val_acc: 0.8996\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3584 - acc: 0.8999 - val_loss: 0.3875 - val_acc: 0.8997\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3578 - acc: 0.8998 - val_loss: 0.3874 - val_acc: 0.8995\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3564 - acc: 0.8999 - val_loss: 0.3873 - val_acc: 0.8990\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3560 - acc: 0.8999 - val_loss: 0.3857 - val_acc: 0.8993\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3550 - acc: 0.8999 - val_loss: 0.3832 - val_acc: 0.8997\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3546 - acc: 0.9000 - val_loss: 0.3852 - val_acc: 0.8985\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3548 - acc: 0.8999 - val_loss: 0.3812 - val_acc: 0.8998\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3542 - acc: 0.8999 - val_loss: 0.3847 - val_acc: 0.8994\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3527 - acc: 0.9001 - val_loss: 0.3816 - val_acc: 0.8996\n",
      "Epoch 81/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3528 - acc: 0.9001 - val_loss: 0.3826 - val_acc: 0.8994\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3518 - acc: 0.9002 - val_loss: 0.3783 - val_acc: 0.8997\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3514 - acc: 0.9001 - val_loss: 0.3841 - val_acc: 0.8988\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3509 - acc: 0.9001 - val_loss: 0.3806 - val_acc: 0.8994\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3503 - acc: 0.9001 - val_loss: 0.3819 - val_acc: 0.8983\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3500 - acc: 0.9003 - val_loss: 0.3805 - val_acc: 0.8984\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.3493 - acc: 0.9003 - val_loss: 0.3765 - val_acc: 0.8991\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3492 - acc: 0.9001 - val_loss: 0.3775 - val_acc: 0.8992\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3482 - acc: 0.9002 - val_loss: 0.3824 - val_acc: 0.8980\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3493 - acc: 0.9002 - val_loss: 0.3744 - val_acc: 0.8989\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3478 - acc: 0.9001 - val_loss: 0.3804 - val_acc: 0.8983\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3473 - acc: 0.9002 - val_loss: 0.3752 - val_acc: 0.8991\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3470 - acc: 0.9003 - val_loss: 0.3748 - val_acc: 0.8989\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3468 - acc: 0.9002 - val_loss: 0.3770 - val_acc: 0.8982\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3462 - acc: 0.9002 - val_loss: 0.3743 - val_acc: 0.8986\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3465 - acc: 0.9000 - val_loss: 0.3739 - val_acc: 0.8994\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3460 - acc: 0.9004 - val_loss: 0.3708 - val_acc: 0.8995\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3460 - acc: 0.9004 - val_loss: 0.3759 - val_acc: 0.8988\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3448 - acc: 0.9004 - val_loss: 0.3728 - val_acc: 0.8996\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.3446 - acc: 0.9003 - val_loss: 0.3769 - val_acc: 0.8983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_5 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_2/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_2/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_7/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_7/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_11 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_15 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_12/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_12/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_16 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_20 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_17/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_17/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_21 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_4/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_4/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_25 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_22/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_22/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_26 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_5/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_5/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_30 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_27/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_27/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_31 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_6/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_6/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_35 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_32/strided_slice_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'cu_dnnlstm_32/strided_slice_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_36 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_7/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_7/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_5 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_6:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_7:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_8:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_9:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_13:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_14:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_11 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_15:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_16:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_15 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_20:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_21:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_16 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_22:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_23:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_20 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_27:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_28:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_21 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_29:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_30:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_25 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_34:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_35:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_26 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_36:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_37:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_30 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_41:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_42:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_31 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_43:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_44:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_35 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_48:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_49:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_36 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_50:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_51:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5004367756843566\n",
      "1 1.381820595264435\n",
      "2 1.3223718726634979\n",
      "3 1.2427455365657807\n",
      "4 1.1293810391426087\n",
      "5 1.0467652904987335\n",
      "6 1.0069428837299348\n",
      "7 0.9831100088357926\n",
      "8 0.9504714620113373\n",
      "9 0.817373737692833\n",
      "10 0.6902780532836914\n",
      "11 0.6348887217044831\n",
      "12 0.610075494647026\n",
      "13 0.5984706139564514\n",
      "14 0.590919925570488\n",
      "15 0.5883345741033554\n",
      "16 0.5835208386182785\n",
      "17 0.5813926661014557\n",
      "18 0.5792099764943123\n",
      "19 0.5773839738965034\n",
      "20 0.5733857905864715\n",
      "21 0.5695111092925071\n",
      "22 0.5622768452763558\n",
      "23 0.5542644512653351\n",
      "24 0.5379283693432808\n",
      "25 0.5164515525102615\n",
      "26 0.4985894259810448\n",
      "27 0.48411704421043394\n",
      "28 0.4763437634706497\n",
      "29 0.46810478925704957\n",
      "30 0.4635571765899658\n",
      "31 0.4572655376791954\n",
      "32 0.4555209544301033\n",
      "33 0.45225921005010605\n",
      "34 0.44884748220443726\n",
      "35 0.4443752774596214\n",
      "36 0.4444532850384712\n",
      "37 0.4402347132563591\n",
      "38 0.43310449153184893\n",
      "39 0.4319944050908089\n",
      "40 0.429329976439476\n",
      "41 0.42948613554239273\n",
      "42 0.42793955296278\n",
      "43 0.42673676133155825\n",
      "44 0.4261710196733475\n",
      "45 0.4265619653463364\n",
      "46 0.4267640459537506\n",
      "47 0.424583782851696\n",
      "48 0.4238566854596138\n",
      "49 0.4272684735059738\n",
      "50 0.4257018542289734\n",
      "51 0.4245641005039215\n",
      "52 0.42588829189538957\n",
      "53 0.4286751106381416\n",
      "54 0.4199452215433121\n",
      "55 0.4164505073428154\n",
      "56 0.4153246536850929\n",
      "57 0.41245495170354846\n",
      "58 0.40691174775362016\n",
      "59 0.4070891636610031\n",
      "60 0.40207754254341127\n",
      "61 0.40182330816984174\n",
      "62 0.4012533748149872\n",
      "63 0.39861222118139267\n",
      "64 0.3976081022620201\n",
      "65 0.39541089683771136\n",
      "66 0.3981906691193581\n",
      "67 0.39367396861314774\n",
      "68 0.39300275266170503\n",
      "69 0.3950515621900558\n",
      "70 0.3876418778300285\n",
      "71 0.3874667212367058\n",
      "72 0.3873659947514534\n",
      "73 0.3872517004609108\n",
      "74 0.3856980141997337\n",
      "75 0.3832463976740837\n",
      "76 0.38518132984638215\n",
      "77 0.38121461153030395\n",
      "78 0.3847219240665436\n",
      "79 0.3816281521320343\n",
      "80 0.38260030925273897\n",
      "81 0.3783017510175705\n",
      "82 0.3840685075521469\n",
      "83 0.3806453263759613\n",
      "84 0.38185044467449186\n",
      "85 0.3804581728577614\n",
      "86 0.37650085777044295\n",
      "87 0.37751214802265165\n",
      "88 0.38239904433488847\n",
      "89 0.3743671807646751\n",
      "90 0.38044794112443925\n",
      "91 0.37519749462604524\n",
      "92 0.3747850677371025\n",
      "93 0.3770112717151642\n",
      "94 0.374324666261673\n",
      "95 0.37393892735242845\n",
      "96 0.3707548850774765\n",
      "97 0.37588169664144516\n",
      "98 0.37278934568166733\n",
      "99 0.37690447121858595\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 8s 684us/step - loss: 1.3201 - acc: 0.4279 - val_loss: 1.0113 - val_acc: 0.6498\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.7032 - acc: 0.8165 - val_loss: 0.5343 - val_acc: 0.8890\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.4736 - acc: 0.8966 - val_loss: 0.4642 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4183 - acc: 0.9000 - val_loss: 0.4330 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.4074 - acc: 0.9000 - val_loss: 0.4258 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 9s 730us/step - loss: 1.2492 - acc: 0.4744 - val_loss: 0.8235 - val_acc: 0.7817\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.5490 - acc: 0.8793 - val_loss: 0.4481 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4200 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.4099 - acc: 0.9000 - val_loss: 0.4279 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4351 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4040 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4242 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4012 - acc: 0.9000 - val_loss: 0.4330 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3999 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3974 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 9s 749us/step - loss: 1.2813 - acc: 0.4539 - val_loss: 0.9191 - val_acc: 0.7135\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.6100 - acc: 0.8643 - val_loss: 0.4650 - val_acc: 0.8999\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.4238 - acc: 0.9000 - val_loss: 0.4331 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.4072 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.4046 - acc: 0.9000 - val_loss: 0.4353 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.4028 - acc: 0.9000 - val_loss: 0.4402 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.4012 - acc: 0.9000 - val_loss: 0.4332 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3994 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3971 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3942 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3908 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3877 - acc: 0.9000 - val_loss: 0.4334 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3840 - acc: 0.9000 - val_loss: 0.4198 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3798 - acc: 0.9000 - val_loss: 0.4272 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3739 - acc: 0.9000 - val_loss: 0.4206 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3655 - acc: 0.9000 - val_loss: 0.3969 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3558 - acc: 0.9000 - val_loss: 0.3900 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3490 - acc: 0.9000 - val_loss: 0.3841 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 3s 249us/step - loss: 0.3431 - acc: 0.9000 - val_loss: 0.3752 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3375 - acc: 0.9001 - val_loss: 0.3693 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 9s 766us/step - loss: 1.3062 - acc: 0.4289 - val_loss: 0.9802 - val_acc: 0.6361\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.6661 - acc: 0.8301 - val_loss: 0.5135 - val_acc: 0.8996\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.4409 - acc: 0.8999 - val_loss: 0.4471 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.4115 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.4058 - acc: 0.9000 - val_loss: 0.4269 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4037 - acc: 0.9000 - val_loss: 0.4275 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.4026 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.3999 - acc: 0.9000 - val_loss: 0.4361 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3973 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3938 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3901 - acc: 0.9000 - val_loss: 0.4333 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3865 - acc: 0.9000 - val_loss: 0.4264 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3836 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3807 - acc: 0.9000 - val_loss: 0.4279 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3781 - acc: 0.9000 - val_loss: 0.4163 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3751 - acc: 0.9000 - val_loss: 0.4152 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3726 - acc: 0.9000 - val_loss: 0.4244 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3691 - acc: 0.9000 - val_loss: 0.4125 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3643 - acc: 0.9000 - val_loss: 0.4113 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3575 - acc: 0.9000 - val_loss: 0.4036 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3509 - acc: 0.9000 - val_loss: 0.3832 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3450 - acc: 0.9000 - val_loss: 0.3783 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3390 - acc: 0.9003 - val_loss: 0.3703 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.3348 - acc: 0.9006 - val_loss: 0.3670 - val_acc: 0.9003\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.3322 - acc: 0.9011 - val_loss: 0.3711 - val_acc: 0.9009\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3293 - acc: 0.9013 - val_loss: 0.3595 - val_acc: 0.9013\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3277 - acc: 0.9020 - val_loss: 0.3627 - val_acc: 0.9006\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3253 - acc: 0.9023 - val_loss: 0.3583 - val_acc: 0.9015\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3246 - acc: 0.9026 - val_loss: 0.3604 - val_acc: 0.9014\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 9s 789us/step - loss: 1.3291 - acc: 0.4053 - val_loss: 1.0495 - val_acc: 0.6056\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.7042 - acc: 0.8156 - val_loss: 0.4836 - val_acc: 0.8996\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4304 - acc: 0.8999 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4085 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4361 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4036 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.4019 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.4000 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3979 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.3942 - acc: 0.9000 - val_loss: 0.4333 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3904 - acc: 0.9000 - val_loss: 0.4313 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3862 - acc: 0.9000 - val_loss: 0.4323 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3821 - acc: 0.9000 - val_loss: 0.4234 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3790 - acc: 0.9000 - val_loss: 0.4234 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.3749 - acc: 0.9000 - val_loss: 0.4149 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3705 - acc: 0.9000 - val_loss: 0.4065 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 3s 250us/step - loss: 0.3607 - acc: 0.9000 - val_loss: 0.4002 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3527 - acc: 0.9000 - val_loss: 0.3849 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3455 - acc: 0.9000 - val_loss: 0.3761 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3404 - acc: 0.9000 - val_loss: 0.3711 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3368 - acc: 0.9001 - val_loss: 0.3691 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 3s 251us/step - loss: 0.3338 - acc: 0.9003 - val_loss: 0.3661 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3316 - acc: 0.9004 - val_loss: 0.3637 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3305 - acc: 0.9006 - val_loss: 0.3628 - val_acc: 0.9001\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3282 - acc: 0.9007 - val_loss: 0.3595 - val_acc: 0.9005\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 3s 252us/step - loss: 0.3268 - acc: 0.9009 - val_loss: 0.3576 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3260 - acc: 0.9010 - val_loss: 0.3511 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3247 - acc: 0.9011 - val_loss: 0.3587 - val_acc: 0.9009\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3238 - acc: 0.9011 - val_loss: 0.3570 - val_acc: 0.9011\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 3s 255us/step - loss: 0.3235 - acc: 0.9012 - val_loss: 0.3515 - val_acc: 0.9002\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3225 - acc: 0.9016 - val_loss: 0.3492 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3219 - acc: 0.9016 - val_loss: 0.3518 - val_acc: 0.9010\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3214 - acc: 0.9018 - val_loss: 0.3515 - val_acc: 0.9008\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 3s 253us/step - loss: 0.3201 - acc: 0.9020 - val_loss: 0.3499 - val_acc: 0.9012\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3190 - acc: 0.9023 - val_loss: 0.3469 - val_acc: 0.9012\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.3186 - acc: 0.9022 - val_loss: 0.3513 - val_acc: 0.9013\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.3182 - acc: 0.9023 - val_loss: 0.3488 - val_acc: 0.9017\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.3178 - acc: 0.9025 - val_loss: 0.3504 - val_acc: 0.9007\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3166 - acc: 0.9028 - val_loss: 0.3485 - val_acc: 0.9013\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3173 - acc: 0.9028 - val_loss: 0.3490 - val_acc: 0.9015\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3155 - acc: 0.9032 - val_loss: 0.3481 - val_acc: 0.9014\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3158 - acc: 0.9032 - val_loss: 0.3539 - val_acc: 0.9004\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3149 - acc: 0.9032 - val_loss: 0.3497 - val_acc: 0.9011\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.3142 - acc: 0.9037 - val_loss: 0.3407 - val_acc: 0.9018\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3143 - acc: 0.9036 - val_loss: 0.3479 - val_acc: 0.9018\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3133 - acc: 0.9039 - val_loss: 0.3441 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3127 - acc: 0.9039 - val_loss: 0.3459 - val_acc: 0.9023\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3127 - acc: 0.9041 - val_loss: 0.3466 - val_acc: 0.9018\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3121 - acc: 0.9044 - val_loss: 0.3493 - val_acc: 0.9020\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3123 - acc: 0.9042 - val_loss: 0.3447 - val_acc: 0.9019\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 10s 834us/step - loss: 1.3181 - acc: 0.4260 - val_loss: 1.0163 - val_acc: 0.6130\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.6608 - acc: 0.8242 - val_loss: 0.4657 - val_acc: 0.8999\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.4192 - acc: 0.9000 - val_loss: 0.4256 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.4074 - acc: 0.9000 - val_loss: 0.4325 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.4045 - acc: 0.9000 - val_loss: 0.4243 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.4028 - acc: 0.9000 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.4008 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.3973 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3933 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3886 - acc: 0.9000 - val_loss: 0.4293 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3843 - acc: 0.9000 - val_loss: 0.4183 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3801 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3756 - acc: 0.9000 - val_loss: 0.4160 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3684 - acc: 0.9000 - val_loss: 0.4057 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3624 - acc: 0.9000 - val_loss: 0.4064 - val_acc: 0.8999\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3546 - acc: 0.9000 - val_loss: 0.3960 - val_acc: 0.9000\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3483 - acc: 0.9000 - val_loss: 0.3833 - val_acc: 0.9000\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3432 - acc: 0.9002 - val_loss: 0.3749 - val_acc: 0.9000\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.3399 - acc: 0.9002 - val_loss: 0.3697 - val_acc: 0.9000\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3361 - acc: 0.9003 - val_loss: 0.3636 - val_acc: 0.9000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3340 - acc: 0.9004 - val_loss: 0.3696 - val_acc: 0.9004\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3324 - acc: 0.9006 - val_loss: 0.3641 - val_acc: 0.9000\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3306 - acc: 0.9007 - val_loss: 0.3619 - val_acc: 0.9007\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3286 - acc: 0.9007 - val_loss: 0.3593 - val_acc: 0.9006\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3272 - acc: 0.9010 - val_loss: 0.3535 - val_acc: 0.9002\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3258 - acc: 0.9010 - val_loss: 0.3574 - val_acc: 0.9007\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3249 - acc: 0.9014 - val_loss: 0.3560 - val_acc: 0.9005\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3238 - acc: 0.9014 - val_loss: 0.3533 - val_acc: 0.9006\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3245 - acc: 0.9015 - val_loss: 0.3521 - val_acc: 0.9009\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3225 - acc: 0.9016 - val_loss: 0.3516 - val_acc: 0.9001\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3217 - acc: 0.9017 - val_loss: 0.3509 - val_acc: 0.9008\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3218 - acc: 0.9019 - val_loss: 0.3518 - val_acc: 0.9005\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3216 - acc: 0.9021 - val_loss: 0.3552 - val_acc: 0.9001\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3198 - acc: 0.9020 - val_loss: 0.3493 - val_acc: 0.9008\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3193 - acc: 0.9023 - val_loss: 0.3484 - val_acc: 0.9005\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3188 - acc: 0.9022 - val_loss: 0.3505 - val_acc: 0.9008\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3180 - acc: 0.9026 - val_loss: 0.3473 - val_acc: 0.9007\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.3176 - acc: 0.9025 - val_loss: 0.3548 - val_acc: 0.9006\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3179 - acc: 0.9026 - val_loss: 0.3469 - val_acc: 0.9009\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3171 - acc: 0.9028 - val_loss: 0.3446 - val_acc: 0.9005\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3164 - acc: 0.9027 - val_loss: 0.3456 - val_acc: 0.9008\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3156 - acc: 0.9030 - val_loss: 0.3441 - val_acc: 0.9009\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3163 - acc: 0.9033 - val_loss: 0.3428 - val_acc: 0.9005\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3154 - acc: 0.9032 - val_loss: 0.3500 - val_acc: 0.9010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3148 - acc: 0.9036 - val_loss: 0.3459 - val_acc: 0.9013\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3141 - acc: 0.9034 - val_loss: 0.3466 - val_acc: 0.9016\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 3s 259us/step - loss: 0.3139 - acc: 0.9036 - val_loss: 0.3505 - val_acc: 0.9014\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3130 - acc: 0.9039 - val_loss: 0.3436 - val_acc: 0.9014\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3134 - acc: 0.9039 - val_loss: 0.3501 - val_acc: 0.9010\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 3s 256us/step - loss: 0.3126 - acc: 0.9039 - val_loss: 0.3519 - val_acc: 0.9012\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3127 - acc: 0.9039 - val_loss: 0.3443 - val_acc: 0.9018\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.3121 - acc: 0.9040 - val_loss: 0.3450 - val_acc: 0.9017\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 3s 258us/step - loss: 0.3115 - acc: 0.9042 - val_loss: 0.3581 - val_acc: 0.9005\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3117 - acc: 0.9042 - val_loss: 0.3458 - val_acc: 0.9016\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3111 - acc: 0.9041 - val_loss: 0.3461 - val_acc: 0.9015\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 3s 254us/step - loss: 0.3109 - acc: 0.9042 - val_loss: 0.3494 - val_acc: 0.9010\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3104 - acc: 0.9045 - val_loss: 0.3451 - val_acc: 0.9018\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3103 - acc: 0.9047 - val_loss: 0.3492 - val_acc: 0.9006\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3095 - acc: 0.9046 - val_loss: 0.3526 - val_acc: 0.8999\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3095 - acc: 0.9043 - val_loss: 0.3523 - val_acc: 0.9003\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3089 - acc: 0.9049 - val_loss: 0.3456 - val_acc: 0.9008\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3082 - acc: 0.9046 - val_loss: 0.3529 - val_acc: 0.9017\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3085 - acc: 0.9046 - val_loss: 0.3441 - val_acc: 0.9016\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3072 - acc: 0.9046 - val_loss: 0.3555 - val_acc: 0.9020\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3071 - acc: 0.9048 - val_loss: 0.3481 - val_acc: 0.9019\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3064 - acc: 0.9049 - val_loss: 0.3494 - val_acc: 0.9015\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 3s 260us/step - loss: 0.3063 - acc: 0.9048 - val_loss: 0.3429 - val_acc: 0.8999\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3069 - acc: 0.9047 - val_loss: 0.3490 - val_acc: 0.9018\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3059 - acc: 0.9048 - val_loss: 0.3444 - val_acc: 0.9009\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3053 - acc: 0.9050 - val_loss: 0.3463 - val_acc: 0.9020\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3057 - acc: 0.9050 - val_loss: 0.3482 - val_acc: 0.9016\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3053 - acc: 0.9048 - val_loss: 0.3462 - val_acc: 0.9008\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3044 - acc: 0.9051 - val_loss: 0.3488 - val_acc: 0.9010\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3038 - acc: 0.9049 - val_loss: 0.3573 - val_acc: 0.8989\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3038 - acc: 0.9050 - val_loss: 0.3516 - val_acc: 0.9022\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3033 - acc: 0.9049 - val_loss: 0.3503 - val_acc: 0.8997\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3032 - acc: 0.9052 - val_loss: 0.3469 - val_acc: 0.9006\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3027 - acc: 0.9051 - val_loss: 0.3518 - val_acc: 0.8981\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3014 - acc: 0.9052 - val_loss: 0.3515 - val_acc: 0.9017\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3011 - acc: 0.9053 - val_loss: 0.3514 - val_acc: 0.9013\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 11s 877us/step - loss: 1.2679 - acc: 0.4599 - val_loss: 0.8787 - val_acc: 0.7366\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.5798 - acc: 0.8632 - val_loss: 0.4443 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.4144 - acc: 0.9000 - val_loss: 0.4178 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.4075 - acc: 0.9000 - val_loss: 0.4284 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.4052 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.4041 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.4031 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4360 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.4011 - acc: 0.9000 - val_loss: 0.4290 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.4000 - acc: 0.9000 - val_loss: 0.4448 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.3985 - acc: 0.9000 - val_loss: 0.4352 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3964 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3929 - acc: 0.9000 - val_loss: 0.4400 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3883 - acc: 0.9000 - val_loss: 0.4423 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3837 - acc: 0.9000 - val_loss: 0.4233 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3783 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.3728 - acc: 0.9000 - val_loss: 0.4099 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.3630 - acc: 0.9000 - val_loss: 0.3980 - val_acc: 0.9000\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3543 - acc: 0.9000 - val_loss: 0.3874 - val_acc: 0.9000\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3507 - acc: 0.9001 - val_loss: 0.4004 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3438 - acc: 0.9001 - val_loss: 0.3745 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3404 - acc: 0.9002 - val_loss: 0.3849 - val_acc: 0.9004\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3367 - acc: 0.9001 - val_loss: 0.3652 - val_acc: 0.9000\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3349 - acc: 0.9003 - val_loss: 0.3631 - val_acc: 0.9000\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3332 - acc: 0.9003 - val_loss: 0.3652 - val_acc: 0.9001\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3310 - acc: 0.9004 - val_loss: 0.3558 - val_acc: 0.9000\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.3298 - acc: 0.9007 - val_loss: 0.3608 - val_acc: 0.9000\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3280 - acc: 0.9007 - val_loss: 0.3551 - val_acc: 0.9000\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3267 - acc: 0.9010 - val_loss: 0.3583 - val_acc: 0.9006\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3268 - acc: 0.9009 - val_loss: 0.3599 - val_acc: 0.9006\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3257 - acc: 0.9008 - val_loss: 0.3680 - val_acc: 0.8995\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3244 - acc: 0.9010 - val_loss: 0.3594 - val_acc: 0.9003\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3234 - acc: 0.9012 - val_loss: 0.3526 - val_acc: 0.9006\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3230 - acc: 0.9013 - val_loss: 0.3583 - val_acc: 0.9007\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3223 - acc: 0.9013 - val_loss: 0.3617 - val_acc: 0.8993\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3221 - acc: 0.9012 - val_loss: 0.3494 - val_acc: 0.9007\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.3217 - acc: 0.9015 - val_loss: 0.3538 - val_acc: 0.9008\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3199 - acc: 0.9019 - val_loss: 0.3535 - val_acc: 0.9003\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3206 - acc: 0.9017 - val_loss: 0.3435 - val_acc: 0.9005\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3188 - acc: 0.9020 - val_loss: 0.3531 - val_acc: 0.9008\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3183 - acc: 0.9023 - val_loss: 0.3530 - val_acc: 0.9008\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 4s 307us/step - loss: 0.3185 - acc: 0.9021 - val_loss: 0.3573 - val_acc: 0.8991\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3172 - acc: 0.9026 - val_loss: 0.3468 - val_acc: 0.9009\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.3172 - acc: 0.9027 - val_loss: 0.3482 - val_acc: 0.9010\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3160 - acc: 0.9031 - val_loss: 0.3511 - val_acc: 0.9010\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3161 - acc: 0.9031 - val_loss: 0.3437 - val_acc: 0.9010\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3164 - acc: 0.9030 - val_loss: 0.3430 - val_acc: 0.9013\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3149 - acc: 0.9031 - val_loss: 0.3414 - val_acc: 0.9008\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3145 - acc: 0.9034 - val_loss: 0.3434 - val_acc: 0.9013\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3144 - acc: 0.9038 - val_loss: 0.3471 - val_acc: 0.9010\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3138 - acc: 0.9034 - val_loss: 0.3428 - val_acc: 0.9009\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3138 - acc: 0.9037 - val_loss: 0.3501 - val_acc: 0.9013\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3134 - acc: 0.9039 - val_loss: 0.3454 - val_acc: 0.9014\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3135 - acc: 0.9036 - val_loss: 0.3534 - val_acc: 0.9010\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3128 - acc: 0.9038 - val_loss: 0.3419 - val_acc: 0.9017\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3123 - acc: 0.9041 - val_loss: 0.3487 - val_acc: 0.9019\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3124 - acc: 0.9041 - val_loss: 0.3452 - val_acc: 0.9013\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3112 - acc: 0.9040 - val_loss: 0.3401 - val_acc: 0.9020\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3112 - acc: 0.9044 - val_loss: 0.3459 - val_acc: 0.9007\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3107 - acc: 0.9043 - val_loss: 0.3460 - val_acc: 0.9014\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3110 - acc: 0.9043 - val_loss: 0.3485 - val_acc: 0.9021\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 3s 261us/step - loss: 0.3099 - acc: 0.9048 - val_loss: 0.3491 - val_acc: 0.9014\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3099 - acc: 0.9046 - val_loss: 0.3525 - val_acc: 0.9005\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3093 - acc: 0.9046 - val_loss: 0.3515 - val_acc: 0.9015\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3092 - acc: 0.9046 - val_loss: 0.3444 - val_acc: 0.9022\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3087 - acc: 0.9048 - val_loss: 0.3524 - val_acc: 0.9014\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3082 - acc: 0.9050 - val_loss: 0.3562 - val_acc: 0.9020\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3082 - acc: 0.9048 - val_loss: 0.3560 - val_acc: 0.8995\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3074 - acc: 0.9050 - val_loss: 0.3515 - val_acc: 0.9017\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.3070 - acc: 0.9049 - val_loss: 0.3426 - val_acc: 0.9022\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3071 - acc: 0.9050 - val_loss: 0.3414 - val_acc: 0.9029\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3068 - acc: 0.9052 - val_loss: 0.3441 - val_acc: 0.9027\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3062 - acc: 0.9053 - val_loss: 0.3456 - val_acc: 0.9020\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3060 - acc: 0.9052 - val_loss: 0.3461 - val_acc: 0.9021\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3053 - acc: 0.9055 - val_loss: 0.3474 - val_acc: 0.9023\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3057 - acc: 0.9055 - val_loss: 0.3560 - val_acc: 0.9023\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3047 - acc: 0.9056 - val_loss: 0.3477 - val_acc: 0.9027\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3043 - acc: 0.9056 - val_loss: 0.3508 - val_acc: 0.9027\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3052 - acc: 0.9054 - val_loss: 0.3485 - val_acc: 0.9030\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.3033 - acc: 0.9060 - val_loss: 0.3482 - val_acc: 0.9030\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3034 - acc: 0.9059 - val_loss: 0.3523 - val_acc: 0.9026\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3027 - acc: 0.9057 - val_loss: 0.3547 - val_acc: 0.9022\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3024 - acc: 0.9060 - val_loss: 0.3593 - val_acc: 0.8998\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3020 - acc: 0.9063 - val_loss: 0.3488 - val_acc: 0.9023\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3019 - acc: 0.9060 - val_loss: 0.3573 - val_acc: 0.9012\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3013 - acc: 0.9061 - val_loss: 0.3536 - val_acc: 0.9026\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3007 - acc: 0.9062 - val_loss: 0.3432 - val_acc: 0.9034\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.3007 - acc: 0.9060 - val_loss: 0.3449 - val_acc: 0.9033\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.3006 - acc: 0.9064 - val_loss: 0.3567 - val_acc: 0.8999\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.2996 - acc: 0.9065 - val_loss: 0.3469 - val_acc: 0.9036\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.2994 - acc: 0.9065 - val_loss: 0.3515 - val_acc: 0.9011\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.2991 - acc: 0.9063 - val_loss: 0.3515 - val_acc: 0.9038\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2993 - acc: 0.9064 - val_loss: 0.3541 - val_acc: 0.9029\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.2977 - acc: 0.9069 - val_loss: 0.3513 - val_acc: 0.9034\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.2979 - acc: 0.9065 - val_loss: 0.3516 - val_acc: 0.9022\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.2968 - acc: 0.9068 - val_loss: 0.3458 - val_acc: 0.9031\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2963 - acc: 0.9069 - val_loss: 0.3530 - val_acc: 0.9030\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2962 - acc: 0.9072 - val_loss: 0.3545 - val_acc: 0.9019\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.2961 - acc: 0.9073 - val_loss: 0.3694 - val_acc: 0.9011\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.2955 - acc: 0.9072 - val_loss: 0.3504 - val_acc: 0.9018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_40 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_37/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_37/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_41 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_8/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_8/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_45 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_42/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_42/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_46 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_9/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_9/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_50 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_47/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_47/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_51 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_10/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_10/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_55 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_52/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_52/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_56 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_11/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_11/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_60 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_57/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_57/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_61 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_12/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_12/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_65 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_62/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_62/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_66 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_13/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_13/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_70 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_67/strided_slice_16:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'cu_dnnlstm_67/strided_slice_17:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_71 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_14/while/Exit_2:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_14/while/Exit_3:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_40 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_55:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_56:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_41 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_57:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_58:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_45 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_62:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_63:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_46 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_64:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_65:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_50 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_69:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_70:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_51 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_71:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_72:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_55 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_76:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_77:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_56 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_78:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_79:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_60 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_83:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_84:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_61 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_85:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_86:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_65 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_90:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_91:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_66 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_92:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_93:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_70 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_97:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_98:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_71 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_99:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_100:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8786722397804261\n",
      "1 0.44426852524280547\n",
      "2 0.41784110575914385\n",
      "3 0.4284157779812813\n",
      "4 0.42877984523773194\n",
      "5 0.42983678430318834\n",
      "6 0.43748078495264053\n",
      "7 0.43600414037704466\n",
      "8 0.4290067163109779\n",
      "9 0.444763942360878\n",
      "10 0.43515212804079056\n",
      "11 0.4300652256608009\n",
      "12 0.43995304584503175\n",
      "13 0.4423401781916618\n",
      "14 0.42334300935268404\n",
      "15 0.42877653539180755\n",
      "16 0.40986818611621856\n",
      "17 0.39797851264476775\n",
      "18 0.3873813191056252\n",
      "19 0.4004031237959862\n",
      "20 0.3744802078604698\n",
      "21 0.3848657912015915\n",
      "22 0.3651960390806198\n",
      "23 0.3631123176217079\n",
      "24 0.3651905918121338\n",
      "25 0.3557560870051384\n",
      "26 0.36078702807426455\n",
      "27 0.35510501503944397\n",
      "28 0.3582573446631432\n",
      "29 0.3598994854092598\n",
      "30 0.3680406978726387\n",
      "31 0.3593640360236168\n",
      "32 0.3526045316457748\n",
      "33 0.35833515226840973\n",
      "34 0.361654432117939\n",
      "35 0.3493729469180107\n",
      "36 0.3538487347960472\n",
      "37 0.3534805080294609\n",
      "38 0.3435076066851616\n",
      "39 0.35313052773475645\n",
      "40 0.3529809799790382\n",
      "41 0.357251216173172\n",
      "42 0.34675728023052216\n",
      "43 0.3481749126315117\n",
      "44 0.35111013263463975\n",
      "45 0.3437054857611656\n",
      "46 0.3430458238720894\n",
      "47 0.34142823368310926\n",
      "48 0.34340756475925444\n",
      "49 0.34705039858818054\n",
      "50 0.34280020534992217\n",
      "51 0.350106959939003\n",
      "52 0.3453557866811752\n",
      "53 0.3534167954325676\n",
      "54 0.3418846917152405\n",
      "55 0.34872987776994707\n",
      "56 0.3451772949099541\n",
      "57 0.3400867420434952\n",
      "58 0.34593369632959364\n",
      "59 0.34595577389001847\n",
      "60 0.34847459733486175\n",
      "61 0.3490652152895927\n",
      "62 0.3525129359960556\n",
      "63 0.35147602766752245\n",
      "64 0.3443655404448509\n",
      "65 0.35243352741003037\n",
      "66 0.35620937883853915\n",
      "67 0.3560278645157814\n",
      "68 0.35149349093437193\n",
      "69 0.34257480561733245\n",
      "70 0.3414343324303627\n",
      "71 0.34405846178531646\n",
      "72 0.3455654314160347\n",
      "73 0.3460758253931999\n",
      "74 0.34740590035915375\n",
      "75 0.3560159382224083\n",
      "76 0.3476681640744209\n",
      "77 0.3507558265328407\n",
      "78 0.3485081905126572\n",
      "79 0.3482185620069504\n",
      "80 0.3522694009542465\n",
      "81 0.354650704562664\n",
      "82 0.3592881345748901\n",
      "83 0.34881372570991515\n",
      "84 0.35725632041692734\n",
      "85 0.353567470908165\n",
      "86 0.34315891802310944\n",
      "87 0.3448741978406906\n",
      "88 0.35672595173120497\n",
      "89 0.34691502183675765\n",
      "90 0.35145298540592196\n",
      "91 0.35145108729600905\n",
      "92 0.3540916472673416\n",
      "93 0.3512501123547554\n",
      "94 0.35160998463630677\n",
      "95 0.3457934704422951\n",
      "96 0.3530313828587532\n",
      "97 0.35445177733898164\n",
      "98 0.3694234782457352\n",
      "99 0.3504310593008995\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 11s 906us/step - loss: 1.1689 - acc: 0.5094 - val_loss: 0.6758 - val_acc: 0.8515\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.4648 - acc: 0.8947 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.4094 - acc: 0.9000 - val_loss: 0.4362 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.4063 - acc: 0.9000 - val_loss: 0.4399 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.4041 - acc: 0.9000 - val_loss: 0.4381 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 11s 897us/step - loss: 1.0944 - acc: 0.5692 - val_loss: 0.5513 - val_acc: 0.8928\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.4358 - acc: 0.8992 - val_loss: 0.4363 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.4067 - acc: 0.9000 - val_loss: 0.4236 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4016 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3990 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3943 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3879 - acc: 0.9000 - val_loss: 0.4267 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3818 - acc: 0.9000 - val_loss: 0.4222 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 11s 930us/step - loss: 1.1437 - acc: 0.5303 - val_loss: 0.6472 - val_acc: 0.8575\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.4710 - acc: 0.8944 - val_loss: 0.4366 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.4084 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4332 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.4002 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3959 - acc: 0.9000 - val_loss: 0.4246 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3909 - acc: 0.9000 - val_loss: 0.4407 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3855 - acc: 0.9000 - val_loss: 0.4213 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3797 - acc: 0.9000 - val_loss: 0.4139 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.3747 - acc: 0.9000 - val_loss: 0.4149 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 3s 263us/step - loss: 0.3672 - acc: 0.9000 - val_loss: 0.4017 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3592 - acc: 0.9000 - val_loss: 0.4056 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3532 - acc: 0.9000 - val_loss: 0.3884 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3476 - acc: 0.9000 - val_loss: 0.3790 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3438 - acc: 0.9000 - val_loss: 0.3726 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3341 - acc: 0.9003 - val_loss: 0.3703 - val_acc: 0.9003\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3289 - acc: 0.9009 - val_loss: 0.3606 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3267 - acc: 0.9014 - val_loss: 0.3543 - val_acc: 0.9003\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3244 - acc: 0.9016 - val_loss: 0.3543 - val_acc: 0.9008\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 12s 972us/step - loss: 1.0956 - acc: 0.5594 - val_loss: 0.5466 - val_acc: 0.8944\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.4293 - acc: 0.8997 - val_loss: 0.4334 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.4069 - acc: 0.9000 - val_loss: 0.4467 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4394 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.4031 - acc: 0.9000 - val_loss: 0.4425 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.4013 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3990 - acc: 0.9000 - val_loss: 0.4475 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3954 - acc: 0.9000 - val_loss: 0.4303 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3897 - acc: 0.9000 - val_loss: 0.4310 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3841 - acc: 0.9000 - val_loss: 0.4413 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3778 - acc: 0.9000 - val_loss: 0.4227 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 4s 307us/step - loss: 0.3704 - acc: 0.9000 - val_loss: 0.4049 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3613 - acc: 0.9000 - val_loss: 0.3991 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.3543 - acc: 0.9000 - val_loss: 0.3979 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3472 - acc: 0.9000 - val_loss: 0.3951 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3413 - acc: 0.9000 - val_loss: 0.3812 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3368 - acc: 0.9001 - val_loss: 0.3654 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.3324 - acc: 0.9003 - val_loss: 0.3661 - val_acc: 0.9001\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3298 - acc: 0.9005 - val_loss: 0.3657 - val_acc: 0.9003\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3264 - acc: 0.9009 - val_loss: 0.3572 - val_acc: 0.9002\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3246 - acc: 0.9013 - val_loss: 0.3637 - val_acc: 0.9009\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3224 - acc: 0.9017 - val_loss: 0.3601 - val_acc: 0.9001\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3207 - acc: 0.9022 - val_loss: 0.3573 - val_acc: 0.9004\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3196 - acc: 0.9022 - val_loss: 0.3540 - val_acc: 0.9008\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3179 - acc: 0.9026 - val_loss: 0.3637 - val_acc: 0.9012\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3166 - acc: 0.9026 - val_loss: 0.3584 - val_acc: 0.9011\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3154 - acc: 0.9032 - val_loss: 0.3484 - val_acc: 0.9016\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3134 - acc: 0.9035 - val_loss: 0.3597 - val_acc: 0.9014\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3123 - acc: 0.9037 - val_loss: 0.3498 - val_acc: 0.9006\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 3s 265us/step - loss: 0.3114 - acc: 0.9038 - val_loss: 0.3469 - val_acc: 0.9020\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 12s 1ms/step - loss: 1.0740 - acc: 0.5731 - val_loss: 0.5647 - val_acc: 0.8892\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.4484 - acc: 0.8993 - val_loss: 0.4436 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.4147 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.4089 - acc: 0.9000 - val_loss: 0.4370 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4358 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.4024 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3993 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3922 - acc: 0.9000 - val_loss: 0.4245 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3855 - acc: 0.9000 - val_loss: 0.4237 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3805 - acc: 0.9000 - val_loss: 0.4190 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3763 - acc: 0.9000 - val_loss: 0.4195 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.3719 - acc: 0.9000 - val_loss: 0.4151 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3680 - acc: 0.9000 - val_loss: 0.4178 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.3624 - acc: 0.9000 - val_loss: 0.4115 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3549 - acc: 0.9000 - val_loss: 0.3952 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3475 - acc: 0.9000 - val_loss: 0.3745 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3424 - acc: 0.9000 - val_loss: 0.3867 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3390 - acc: 0.9001 - val_loss: 0.3761 - val_acc: 0.8999\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3355 - acc: 0.9002 - val_loss: 0.3778 - val_acc: 0.8999\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3319 - acc: 0.9002 - val_loss: 0.3670 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3281 - acc: 0.9007 - val_loss: 0.3571 - val_acc: 0.9003\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3250 - acc: 0.9013 - val_loss: 0.3592 - val_acc: 0.9005\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 3s 292us/step - loss: 0.3228 - acc: 0.9015 - val_loss: 0.3545 - val_acc: 0.9006\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.3217 - acc: 0.9021 - val_loss: 0.3585 - val_acc: 0.9005\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3204 - acc: 0.9020 - val_loss: 0.3545 - val_acc: 0.9009\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3190 - acc: 0.9026 - val_loss: 0.3554 - val_acc: 0.9011\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3182 - acc: 0.9027 - val_loss: 0.3453 - val_acc: 0.9014\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3164 - acc: 0.9029 - val_loss: 0.3462 - val_acc: 0.9012\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 3s 264us/step - loss: 0.3151 - acc: 0.9035 - val_loss: 0.3520 - val_acc: 0.9007\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3143 - acc: 0.9036 - val_loss: 0.3562 - val_acc: 0.9010\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.3137 - acc: 0.9037 - val_loss: 0.3478 - val_acc: 0.9012\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3129 - acc: 0.9037 - val_loss: 0.3459 - val_acc: 0.9022\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3118 - acc: 0.9042 - val_loss: 0.3519 - val_acc: 0.9022\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3106 - acc: 0.9041 - val_loss: 0.3523 - val_acc: 0.9014\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3097 - acc: 0.9043 - val_loss: 0.3471 - val_acc: 0.9013\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 269us/step - loss: 0.3084 - acc: 0.9046 - val_loss: 0.3489 - val_acc: 0.9007\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.3073 - acc: 0.9046 - val_loss: 0.3549 - val_acc: 0.9008\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3063 - acc: 0.9050 - val_loss: 0.3526 - val_acc: 0.9022\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3064 - acc: 0.9051 - val_loss: 0.3437 - val_acc: 0.9019\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3041 - acc: 0.9055 - val_loss: 0.3445 - val_acc: 0.9022\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.3034 - acc: 0.9054 - val_loss: 0.3480 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.3019 - acc: 0.9058 - val_loss: 0.3523 - val_acc: 0.9012\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3016 - acc: 0.9058 - val_loss: 0.3624 - val_acc: 0.9005\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3002 - acc: 0.9065 - val_loss: 0.3502 - val_acc: 0.9016\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 3s 266us/step - loss: 0.2993 - acc: 0.9062 - val_loss: 0.3556 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.2983 - acc: 0.9062 - val_loss: 0.3549 - val_acc: 0.9024\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2970 - acc: 0.9066 - val_loss: 0.3499 - val_acc: 0.9016\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2959 - acc: 0.9068 - val_loss: 0.3530 - val_acc: 0.9008\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.2944 - acc: 0.9064 - val_loss: 0.3551 - val_acc: 0.9021\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2933 - acc: 0.9070 - val_loss: 0.3583 - val_acc: 0.9023\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 12s 1ms/step - loss: 1.0899 - acc: 0.5528 - val_loss: 0.5538 - val_acc: 0.8876\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.4291 - acc: 0.8995 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.4079 - acc: 0.9000 - val_loss: 0.4314 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.4045 - acc: 0.9000 - val_loss: 0.4402 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 4s 302us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4219 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.4012 - acc: 0.9000 - val_loss: 0.4324 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 4s 308us/step - loss: 0.3983 - acc: 0.9000 - val_loss: 0.4369 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3932 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3875 - acc: 0.9000 - val_loss: 0.4317 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3825 - acc: 0.9000 - val_loss: 0.4278 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.3771 - acc: 0.9000 - val_loss: 0.4245 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3706 - acc: 0.9000 - val_loss: 0.4171 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.3604 - acc: 0.9000 - val_loss: 0.3996 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 4s 310us/step - loss: 0.3509 - acc: 0.9000 - val_loss: 0.3925 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 4s 306us/step - loss: 0.3451 - acc: 0.9001 - val_loss: 0.3833 - val_acc: 0.9000\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3374 - acc: 0.9002 - val_loss: 0.3763 - val_acc: 0.9001\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.3336 - acc: 0.9008 - val_loss: 0.3605 - val_acc: 0.9002\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3283 - acc: 0.9016 - val_loss: 0.3668 - val_acc: 0.9009\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3254 - acc: 0.9025 - val_loss: 0.3582 - val_acc: 0.9011\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3223 - acc: 0.9030 - val_loss: 0.3653 - val_acc: 0.9016\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3210 - acc: 0.9033 - val_loss: 0.3540 - val_acc: 0.9014\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 4s 302us/step - loss: 0.3185 - acc: 0.9034 - val_loss: 0.3476 - val_acc: 0.9018\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3160 - acc: 0.9038 - val_loss: 0.3517 - val_acc: 0.9017\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3152 - acc: 0.9040 - val_loss: 0.3458 - val_acc: 0.9023\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3136 - acc: 0.9045 - val_loss: 0.3485 - val_acc: 0.9023\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3131 - acc: 0.9045 - val_loss: 0.3522 - val_acc: 0.9026\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3111 - acc: 0.9048 - val_loss: 0.3526 - val_acc: 0.9005\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.3105 - acc: 0.9051 - val_loss: 0.3543 - val_acc: 0.8999\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3099 - acc: 0.9052 - val_loss: 0.3458 - val_acc: 0.9035\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3090 - acc: 0.9057 - val_loss: 0.3494 - val_acc: 0.9031\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.3070 - acc: 0.9056 - val_loss: 0.3438 - val_acc: 0.9039\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3072 - acc: 0.9056 - val_loss: 0.3410 - val_acc: 0.9043\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3054 - acc: 0.9059 - val_loss: 0.3554 - val_acc: 0.9012\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3046 - acc: 0.9062 - val_loss: 0.3685 - val_acc: 0.9000\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3039 - acc: 0.9063 - val_loss: 0.3493 - val_acc: 0.9040\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3027 - acc: 0.9062 - val_loss: 0.3467 - val_acc: 0.9034\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3022 - acc: 0.9065 - val_loss: 0.3602 - val_acc: 0.9009\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3006 - acc: 0.9068 - val_loss: 0.3511 - val_acc: 0.9010\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3000 - acc: 0.9064 - val_loss: 0.3459 - val_acc: 0.9033\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2986 - acc: 0.9072 - val_loss: 0.3585 - val_acc: 0.9010\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.2980 - acc: 0.9070 - val_loss: 0.3562 - val_acc: 0.9023\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2967 - acc: 0.9071 - val_loss: 0.3558 - val_acc: 0.8999\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2951 - acc: 0.9075 - val_loss: 0.3631 - val_acc: 0.8985\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2940 - acc: 0.9075 - val_loss: 0.3604 - val_acc: 0.9030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2928 - acc: 0.9079 - val_loss: 0.3585 - val_acc: 0.9020\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.2915 - acc: 0.9081 - val_loss: 0.3604 - val_acc: 0.9007\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2902 - acc: 0.9084 - val_loss: 0.3666 - val_acc: 0.9004\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2898 - acc: 0.9079 - val_loss: 0.3572 - val_acc: 0.9012\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2868 - acc: 0.9089 - val_loss: 0.3622 - val_acc: 0.9022\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2857 - acc: 0.9095 - val_loss: 0.3573 - val_acc: 0.9021\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.2843 - acc: 0.9094 - val_loss: 0.3729 - val_acc: 0.8980\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.2817 - acc: 0.9104 - val_loss: 0.3654 - val_acc: 0.9000\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 4s 303us/step - loss: 0.2806 - acc: 0.9104 - val_loss: 0.3625 - val_acc: 0.9008\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.2783 - acc: 0.9107 - val_loss: 0.3648 - val_acc: 0.9009\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 3s 292us/step - loss: 0.2759 - acc: 0.9116 - val_loss: 0.3792 - val_acc: 0.8969\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2745 - acc: 0.9114 - val_loss: 0.3649 - val_acc: 0.8976\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2716 - acc: 0.9125 - val_loss: 0.3748 - val_acc: 0.8988\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.2704 - acc: 0.9129 - val_loss: 0.3755 - val_acc: 0.8961\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.2675 - acc: 0.9136 - val_loss: 0.3762 - val_acc: 0.8971\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.2652 - acc: 0.9140 - val_loss: 0.3788 - val_acc: 0.8961\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.2627 - acc: 0.9148 - val_loss: 0.3943 - val_acc: 0.8937\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.2608 - acc: 0.9150 - val_loss: 0.3869 - val_acc: 0.8968\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.2581 - acc: 0.9157 - val_loss: 0.3953 - val_acc: 0.8942\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.2546 - acc: 0.9166 - val_loss: 0.3891 - val_acc: 0.8949\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.2527 - acc: 0.9168 - val_loss: 0.3949 - val_acc: 0.8942\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.2499 - acc: 0.9184 - val_loss: 0.4073 - val_acc: 0.8923\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.2477 - acc: 0.9188 - val_loss: 0.4035 - val_acc: 0.8910\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.2448 - acc: 0.9188 - val_loss: 0.4150 - val_acc: 0.8883\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2422 - acc: 0.9199 - val_loss: 0.4072 - val_acc: 0.8905\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2393 - acc: 0.9207 - val_loss: 0.4146 - val_acc: 0.8902\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2358 - acc: 0.9217 - val_loss: 0.4129 - val_acc: 0.8923\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2326 - acc: 0.9232 - val_loss: 0.4101 - val_acc: 0.8954\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 3s 267us/step - loss: 0.2296 - acc: 0.9240 - val_loss: 0.4199 - val_acc: 0.8912\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 3s 272us/step - loss: 0.2273 - acc: 0.9247 - val_loss: 0.4238 - val_acc: 0.8879\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2231 - acc: 0.9258 - val_loss: 0.4272 - val_acc: 0.8870\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2201 - acc: 0.9267 - val_loss: 0.4314 - val_acc: 0.8908\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 3s 262us/step - loss: 0.2178 - acc: 0.9274 - val_loss: 0.4358 - val_acc: 0.8866\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2145 - acc: 0.9280 - val_loss: 0.4453 - val_acc: 0.8864\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.2121 - acc: 0.9294 - val_loss: 0.4414 - val_acc: 0.8859\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2073 - acc: 0.9302 - val_loss: 0.4565 - val_acc: 0.8863\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 12s 1ms/step - loss: 1.1363 - acc: 0.5381 - val_loss: 0.6096 - val_acc: 0.8858\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.4519 - acc: 0.8985 - val_loss: 0.4389 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.4074 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4346 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.4033 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.4015 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.3999 - acc: 0.9000 - val_loss: 0.4386 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3957 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.3903 - acc: 0.9000 - val_loss: 0.4243 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3854 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3762 - acc: 0.9000 - val_loss: 0.4155 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3658 - acc: 0.9000 - val_loss: 0.4097 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3557 - acc: 0.9000 - val_loss: 0.3844 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.3479 - acc: 0.9000 - val_loss: 0.3905 - val_acc: 0.9001\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.3404 - acc: 0.9002 - val_loss: 0.3691 - val_acc: 0.9001\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3352 - acc: 0.9005 - val_loss: 0.3639 - val_acc: 0.9001\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3320 - acc: 0.9009 - val_loss: 0.3661 - val_acc: 0.9008\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.3277 - acc: 0.9019 - val_loss: 0.3574 - val_acc: 0.9007\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3262 - acc: 0.9020 - val_loss: 0.3483 - val_acc: 0.9003\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3242 - acc: 0.9024 - val_loss: 0.3492 - val_acc: 0.9011\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3211 - acc: 0.9028 - val_loss: 0.3608 - val_acc: 0.9010\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3198 - acc: 0.9033 - val_loss: 0.3605 - val_acc: 0.9015\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3180 - acc: 0.9037 - val_loss: 0.3474 - val_acc: 0.9014\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3166 - acc: 0.9039 - val_loss: 0.3448 - val_acc: 0.9013\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3150 - acc: 0.9042 - val_loss: 0.3507 - val_acc: 0.9015\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3149 - acc: 0.9045 - val_loss: 0.3415 - val_acc: 0.9029\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3137 - acc: 0.9047 - val_loss: 0.3385 - val_acc: 0.9033\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3124 - acc: 0.9050 - val_loss: 0.3393 - val_acc: 0.9036\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3124 - acc: 0.9049 - val_loss: 0.3588 - val_acc: 0.8997\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3110 - acc: 0.9051 - val_loss: 0.3385 - val_acc: 0.9036\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.3096 - acc: 0.9054 - val_loss: 0.3485 - val_acc: 0.9022\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3086 - acc: 0.9053 - val_loss: 0.3603 - val_acc: 0.9002\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3076 - acc: 0.9060 - val_loss: 0.3506 - val_acc: 0.9021\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.3065 - acc: 0.9055 - val_loss: 0.3549 - val_acc: 0.9010\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3055 - acc: 0.9059 - val_loss: 0.3649 - val_acc: 0.9021\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3053 - acc: 0.9059 - val_loss: 0.3419 - val_acc: 0.9024\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3036 - acc: 0.9062 - val_loss: 0.3472 - val_acc: 0.9023\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3028 - acc: 0.9064 - val_loss: 0.3569 - val_acc: 0.9034\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.3020 - acc: 0.9063 - val_loss: 0.3493 - val_acc: 0.9026\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3010 - acc: 0.9064 - val_loss: 0.3582 - val_acc: 0.9033\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.3000 - acc: 0.9067 - val_loss: 0.3479 - val_acc: 0.9032\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2987 - acc: 0.9071 - val_loss: 0.3567 - val_acc: 0.9019\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.2981 - acc: 0.9068 - val_loss: 0.3491 - val_acc: 0.9033\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.2967 - acc: 0.9072 - val_loss: 0.3630 - val_acc: 0.9022\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2953 - acc: 0.9077 - val_loss: 0.3521 - val_acc: 0.9014\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.2949 - acc: 0.9074 - val_loss: 0.3555 - val_acc: 0.9013\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.2927 - acc: 0.9080 - val_loss: 0.3591 - val_acc: 0.9021\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.2917 - acc: 0.9082 - val_loss: 0.3565 - val_acc: 0.9015\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.2901 - acc: 0.9084 - val_loss: 0.3568 - val_acc: 0.9009\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 3s 274us/step - loss: 0.2885 - acc: 0.9081 - val_loss: 0.3655 - val_acc: 0.9007\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.2874 - acc: 0.9086 - val_loss: 0.3625 - val_acc: 0.9008\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 3s 268us/step - loss: 0.2858 - acc: 0.9090 - val_loss: 0.3626 - val_acc: 0.9009\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.2839 - acc: 0.9091 - val_loss: 0.3561 - val_acc: 0.9016\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.2832 - acc: 0.9094 - val_loss: 0.3699 - val_acc: 0.8954\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.2808 - acc: 0.9101 - val_loss: 0.3669 - val_acc: 0.8988\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.2791 - acc: 0.9103 - val_loss: 0.3698 - val_acc: 0.8988\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.2770 - acc: 0.9106 - val_loss: 0.3660 - val_acc: 0.8957\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2745 - acc: 0.9113 - val_loss: 0.3762 - val_acc: 0.8986\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.2723 - acc: 0.9119 - val_loss: 0.3748 - val_acc: 0.8986\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.2701 - acc: 0.9124 - val_loss: 0.3692 - val_acc: 0.9000\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.2676 - acc: 0.9126 - val_loss: 0.3782 - val_acc: 0.8964\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.2663 - acc: 0.9128 - val_loss: 0.3840 - val_acc: 0.8937\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.2628 - acc: 0.9139 - val_loss: 0.3723 - val_acc: 0.8966\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2608 - acc: 0.9140 - val_loss: 0.3902 - val_acc: 0.8932\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2574 - acc: 0.9151 - val_loss: 0.3901 - val_acc: 0.8958\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.2553 - acc: 0.9154 - val_loss: 0.3902 - val_acc: 0.8943\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2520 - acc: 0.9164 - val_loss: 0.3931 - val_acc: 0.8969\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.2494 - acc: 0.9170 - val_loss: 0.3916 - val_acc: 0.8957\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.2466 - acc: 0.9179 - val_loss: 0.3983 - val_acc: 0.8935\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 3s 273us/step - loss: 0.2435 - acc: 0.9186 - val_loss: 0.4088 - val_acc: 0.8890\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.2410 - acc: 0.9194 - val_loss: 0.4044 - val_acc: 0.8910\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.2377 - acc: 0.9203 - val_loss: 0.4155 - val_acc: 0.8871\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2338 - acc: 0.9216 - val_loss: 0.4208 - val_acc: 0.8923\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2311 - acc: 0.9223 - val_loss: 0.4155 - val_acc: 0.8882\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.2283 - acc: 0.9231 - val_loss: 0.4359 - val_acc: 0.8820\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.2246 - acc: 0.9242 - val_loss: 0.4186 - val_acc: 0.8931\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.2210 - acc: 0.9256 - val_loss: 0.4349 - val_acc: 0.8915\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.2176 - acc: 0.9261 - val_loss: 0.4339 - val_acc: 0.8897\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2145 - acc: 0.9273 - val_loss: 0.4416 - val_acc: 0.8821\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.2116 - acc: 0.9287 - val_loss: 0.4467 - val_acc: 0.8833\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 4s 308us/step - loss: 0.2094 - acc: 0.9293 - val_loss: 0.4468 - val_acc: 0.8901\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.2042 - acc: 0.9303 - val_loss: 0.4628 - val_acc: 0.8826\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.2010 - acc: 0.9316 - val_loss: 0.4654 - val_acc: 0.8817\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.1975 - acc: 0.9329 - val_loss: 0.4552 - val_acc: 0.8844\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.1940 - acc: 0.9343 - val_loss: 0.4739 - val_acc: 0.8786\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 4s 305us/step - loss: 0.1903 - acc: 0.9353 - val_loss: 0.4816 - val_acc: 0.8786\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.1873 - acc: 0.9360 - val_loss: 0.4863 - val_acc: 0.8786\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 4s 303us/step - loss: 0.1845 - acc: 0.9372 - val_loss: 0.4804 - val_acc: 0.8821\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.1805 - acc: 0.9386 - val_loss: 0.4848 - val_acc: 0.8781\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.1767 - acc: 0.9400 - val_loss: 0.4952 - val_acc: 0.8761\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1742 - acc: 0.9407 - val_loss: 0.4993 - val_acc: 0.8795\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.1699 - acc: 0.9422 - val_loss: 0.5159 - val_acc: 0.8696\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.1675 - acc: 0.9430 - val_loss: 0.5116 - val_acc: 0.8770\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.1644 - acc: 0.9438 - val_loss: 0.5200 - val_acc: 0.8770\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.1607 - acc: 0.9448 - val_loss: 0.5321 - val_acc: 0.8755\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.1562 - acc: 0.9473 - val_loss: 0.5301 - val_acc: 0.8744\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.1525 - acc: 0.9485 - val_loss: 0.5353 - val_acc: 0.8760\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.1499 - acc: 0.9493 - val_loss: 0.5592 - val_acc: 0.8721\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.1470 - acc: 0.9506 - val_loss: 0.5567 - val_acc: 0.8759\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.1429 - acc: 0.9519 - val_loss: 0.5626 - val_acc: 0.8742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_75 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_72/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_72/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_76 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_15/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_15/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_80 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_77/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_77/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_81 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_16/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_16/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_85 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_82/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_82/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_86 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_17/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_17/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_90 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_87/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_87/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_91 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_18/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_18/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_95 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_92/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_92/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_96 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_19/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_19/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_100 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_97/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_97/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_101 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_20/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_20/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_105 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_102/strided_slice_16:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'cu_dnnlstm_102/strided_slice_17:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_106 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_21/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_21/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_75 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_104:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_105:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_76 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_106:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_107:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_80 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_111:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_112:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_81 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_113:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_114:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_85 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_118:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_119:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_86 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_120:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_121:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_90 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_125:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_126:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_91 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_127:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_128:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_95 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_132:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_133:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_96 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_134:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_135:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_100 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_139:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_140:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_101 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_141:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_142:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_105 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_146:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_147:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_106 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_148:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_149:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6095756286382675\n",
      "1 0.4388976100087166\n",
      "2 0.4354188597202301\n",
      "3 0.4346166464686394\n",
      "4 0.432036828994751\n",
      "5 0.4302462974190712\n",
      "6 0.43863734692335127\n",
      "7 0.43539054214954376\n",
      "8 0.4242702430486679\n",
      "9 0.4281951573491096\n",
      "10 0.41551555305719373\n",
      "11 0.40966401755809784\n",
      "12 0.38441561460494994\n",
      "13 0.39049617558717725\n",
      "14 0.36905617505311966\n",
      "15 0.3638867610692978\n",
      "16 0.36608292430639267\n",
      "17 0.35736542999744414\n",
      "18 0.34828554183244703\n",
      "19 0.34922057211399077\n",
      "20 0.36081832617521287\n",
      "21 0.36046402126550675\n",
      "22 0.3474120408296585\n",
      "23 0.3447525385022163\n",
      "24 0.35067435204982755\n",
      "25 0.3415193432569504\n",
      "26 0.3384769058227539\n",
      "27 0.33934714674949645\n",
      "28 0.35882283926010133\n",
      "29 0.33847261846065524\n",
      "30 0.3485090506076813\n",
      "31 0.3603460970520973\n",
      "32 0.3505635279417038\n",
      "33 0.35485022485256196\n",
      "34 0.3648706677556038\n",
      "35 0.3419478845596313\n",
      "36 0.34724839240312577\n",
      "37 0.3569214388728142\n",
      "38 0.3492629724740982\n",
      "39 0.35816630959510803\n",
      "40 0.3478750470280647\n",
      "41 0.35669588059186935\n",
      "42 0.3491229155659676\n",
      "43 0.36300581753253935\n",
      "44 0.35208178132772444\n",
      "45 0.3555393552780151\n",
      "46 0.3591007739305496\n",
      "47 0.3564677509665489\n",
      "48 0.35683943420648573\n",
      "49 0.3654767283797264\n",
      "50 0.362452307343483\n",
      "51 0.36263893932104113\n",
      "52 0.3560949856042862\n",
      "53 0.3699261075258255\n",
      "54 0.36692292511463165\n",
      "55 0.36976545929908755\n",
      "56 0.3659708973765373\n",
      "57 0.376215917468071\n",
      "58 0.37479948669672014\n",
      "59 0.36916426092386245\n",
      "60 0.3782178509235382\n",
      "61 0.38396521091461183\n",
      "62 0.3722947958111763\n",
      "63 0.39018877744674685\n",
      "64 0.3900646620988846\n",
      "65 0.3901731932163239\n",
      "66 0.39314825922250746\n",
      "67 0.39156813293695447\n",
      "68 0.3983238372206688\n",
      "69 0.4088011085987091\n",
      "70 0.4044205805659294\n",
      "71 0.41551325500011443\n",
      "72 0.4207695084810257\n",
      "73 0.4155295717716217\n",
      "74 0.4359469398856163\n",
      "75 0.41864906579256056\n",
      "76 0.43493101119995115\n",
      "77 0.43387610793113707\n",
      "78 0.4415704470872879\n",
      "79 0.4467388018965721\n",
      "80 0.44677683532238005\n",
      "81 0.4628126499056816\n",
      "82 0.46544677823781966\n",
      "83 0.455226454436779\n",
      "84 0.47391912192106245\n",
      "85 0.48160325318574904\n",
      "86 0.48631630539894105\n",
      "87 0.4803856810927391\n",
      "88 0.4847650292515755\n",
      "89 0.4952121651172638\n",
      "90 0.4992588359117508\n",
      "91 0.515875700712204\n",
      "92 0.5115513136982918\n",
      "93 0.5199654683470726\n",
      "94 0.5320788598060608\n",
      "95 0.5300687098503113\n",
      "96 0.5352662810683251\n",
      "97 0.559211315214634\n",
      "98 0.5567199322581291\n",
      "99 0.5626256161928177\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.9505 - acc: 0.6310 - val_loss: 0.4398 - val_acc: 0.8998\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.4145 - acc: 0.9000 - val_loss: 0.4265 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.4044 - acc: 0.9000 - val_loss: 0.4381 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.9287 - acc: 0.6387 - val_loss: 0.4543 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.4122 - acc: 0.9000 - val_loss: 0.4356 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.4061 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.4038 - acc: 0.9000 - val_loss: 0.4337 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3990 - acc: 0.9000 - val_loss: 0.4360 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3915 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.3807 - acc: 0.9000 - val_loss: 0.4199 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3704 - acc: 0.9000 - val_loss: 0.3978 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3562 - acc: 0.9000 - val_loss: 0.3944 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.9582 - acc: 0.6348 - val_loss: 0.4564 - val_acc: 0.8992\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.4158 - acc: 0.9000 - val_loss: 0.4462 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.4080 - acc: 0.9000 - val_loss: 0.4251 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.4052 - acc: 0.9000 - val_loss: 0.4255 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.4026 - acc: 0.9000 - val_loss: 0.4415 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.4003 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.4366 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3884 - acc: 0.9000 - val_loss: 0.4253 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3815 - acc: 0.9000 - val_loss: 0.4342 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3709 - acc: 0.9000 - val_loss: 0.4224 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3614 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.8996\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3523 - acc: 0.9001 - val_loss: 0.3928 - val_acc: 0.8984\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.3435 - acc: 0.9002 - val_loss: 0.3840 - val_acc: 0.9001\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3323 - acc: 0.9013 - val_loss: 0.3682 - val_acc: 0.9001\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.3293 - acc: 0.9014 - val_loss: 0.3663 - val_acc: 0.9004\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.3245 - acc: 0.9028 - val_loss: 0.3596 - val_acc: 0.9013\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3209 - acc: 0.9031 - val_loss: 0.3483 - val_acc: 0.9020\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3182 - acc: 0.9039 - val_loss: 0.3664 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3162 - acc: 0.9042 - val_loss: 0.3613 - val_acc: 0.9015\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.3153 - acc: 0.9041 - val_loss: 0.3542 - val_acc: 0.9013\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 14s 1ms/step - loss: 0.9274 - acc: 0.6490 - val_loss: 0.4359 - val_acc: 0.9000\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.4126 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.4056 - acc: 0.9000 - val_loss: 0.4194 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.4056 - acc: 0.9000 - val_loss: 0.4322 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4414 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3998 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3946 - acc: 0.9000 - val_loss: 0.4529 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 4s 302us/step - loss: 0.3879 - acc: 0.9000 - val_loss: 0.4167 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.3787 - acc: 0.9000 - val_loss: 0.4154 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3686 - acc: 0.9000 - val_loss: 0.3985 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3558 - acc: 0.9000 - val_loss: 0.3938 - val_acc: 0.8999\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.3424 - acc: 0.9002 - val_loss: 0.3798 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.3330 - acc: 0.9010 - val_loss: 0.3684 - val_acc: 0.9007\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.3286 - acc: 0.9020 - val_loss: 0.3599 - val_acc: 0.9007\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3219 - acc: 0.9028 - val_loss: 0.3575 - val_acc: 0.9008\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3202 - acc: 0.9032 - val_loss: 0.3512 - val_acc: 0.9023\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3166 - acc: 0.9041 - val_loss: 0.3607 - val_acc: 0.9001\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3151 - acc: 0.9045 - val_loss: 0.3605 - val_acc: 0.9015\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3135 - acc: 0.9049 - val_loss: 0.3496 - val_acc: 0.9028\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3112 - acc: 0.9053 - val_loss: 0.3499 - val_acc: 0.9035\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.3102 - acc: 0.9054 - val_loss: 0.3517 - val_acc: 0.9032\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 4s 303us/step - loss: 0.3084 - acc: 0.9057 - val_loss: 0.3550 - val_acc: 0.9019\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 4s 308us/step - loss: 0.3068 - acc: 0.9058 - val_loss: 0.3499 - val_acc: 0.9025\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3054 - acc: 0.9061 - val_loss: 0.3447 - val_acc: 0.9046\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3030 - acc: 0.9066 - val_loss: 0.3541 - val_acc: 0.8997\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3023 - acc: 0.9066 - val_loss: 0.3511 - val_acc: 0.9028\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 4s 313us/step - loss: 0.3002 - acc: 0.9070 - val_loss: 0.3543 - val_acc: 0.9013\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.2980 - acc: 0.9072 - val_loss: 0.3571 - val_acc: 0.9021\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.2958 - acc: 0.9083 - val_loss: 0.3647 - val_acc: 0.9020\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.2938 - acc: 0.9084 - val_loss: 0.3495 - val_acc: 0.9030\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 14s 1ms/step - loss: 0.9215 - acc: 0.6583 - val_loss: 0.4496 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 4s 312us/step - loss: 0.4137 - acc: 0.9000 - val_loss: 0.4434 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.4061 - acc: 0.9000 - val_loss: 0.4414 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.4046 - acc: 0.9000 - val_loss: 0.4368 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4272 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3993 - acc: 0.9000 - val_loss: 0.4463 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3938 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3856 - acc: 0.9000 - val_loss: 0.4191 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3768 - acc: 0.9000 - val_loss: 0.4085 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.3633 - acc: 0.9000 - val_loss: 0.3976 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3494 - acc: 0.9000 - val_loss: 0.3874 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3385 - acc: 0.9005 - val_loss: 0.3768 - val_acc: 0.9004\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3336 - acc: 0.9009 - val_loss: 0.3609 - val_acc: 0.9005\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3260 - acc: 0.9020 - val_loss: 0.3570 - val_acc: 0.9005\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3228 - acc: 0.9027 - val_loss: 0.3506 - val_acc: 0.9011\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3208 - acc: 0.9033 - val_loss: 0.3627 - val_acc: 0.9011\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.3172 - acc: 0.9037 - val_loss: 0.3600 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3153 - acc: 0.9046 - val_loss: 0.3648 - val_acc: 0.9003\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3142 - acc: 0.9043 - val_loss: 0.3482 - val_acc: 0.9019\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3123 - acc: 0.9049 - val_loss: 0.3511 - val_acc: 0.9016\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.3095 - acc: 0.9052 - val_loss: 0.3501 - val_acc: 0.9031\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3084 - acc: 0.9057 - val_loss: 0.3497 - val_acc: 0.9032\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.3061 - acc: 0.9060 - val_loss: 0.3611 - val_acc: 0.9008\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3054 - acc: 0.9058 - val_loss: 0.3489 - val_acc: 0.9028\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3034 - acc: 0.9067 - val_loss: 0.3492 - val_acc: 0.9034\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.3004 - acc: 0.9070 - val_loss: 0.3552 - val_acc: 0.9027\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2994 - acc: 0.9070 - val_loss: 0.3466 - val_acc: 0.9039\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.2972 - acc: 0.9074 - val_loss: 0.3600 - val_acc: 0.9010\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2955 - acc: 0.9078 - val_loss: 0.3630 - val_acc: 0.9010\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2925 - acc: 0.9082 - val_loss: 0.3646 - val_acc: 0.9011\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.2888 - acc: 0.9091 - val_loss: 0.3620 - val_acc: 0.9009\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.2865 - acc: 0.9097 - val_loss: 0.3718 - val_acc: 0.8987\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.2830 - acc: 0.9104 - val_loss: 0.3670 - val_acc: 0.8994\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.2785 - acc: 0.9114 - val_loss: 0.3742 - val_acc: 0.8989\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2753 - acc: 0.9124 - val_loss: 0.3789 - val_acc: 0.8969\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.2720 - acc: 0.9132 - val_loss: 0.3786 - val_acc: 0.8980\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.2676 - acc: 0.9141 - val_loss: 0.3902 - val_acc: 0.8948\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2636 - acc: 0.9151 - val_loss: 0.3833 - val_acc: 0.8972\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.2580 - acc: 0.9165 - val_loss: 0.3911 - val_acc: 0.8947\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2517 - acc: 0.9181 - val_loss: 0.3967 - val_acc: 0.8949\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.2461 - acc: 0.9195 - val_loss: 0.4080 - val_acc: 0.8925\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.2397 - acc: 0.9216 - val_loss: 0.4150 - val_acc: 0.8872\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.2326 - acc: 0.9239 - val_loss: 0.4141 - val_acc: 0.8916\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2272 - acc: 0.9248 - val_loss: 0.4321 - val_acc: 0.8855\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2190 - acc: 0.9278 - val_loss: 0.4393 - val_acc: 0.8880\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2137 - acc: 0.9292 - val_loss: 0.4311 - val_acc: 0.8860\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.2031 - acc: 0.9323 - val_loss: 0.4566 - val_acc: 0.8881\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.1935 - acc: 0.9359 - val_loss: 0.4710 - val_acc: 0.8841\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.1854 - acc: 0.9384 - val_loss: 0.4780 - val_acc: 0.8814\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.1766 - acc: 0.9409 - val_loss: 0.4890 - val_acc: 0.8804\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 14s 1ms/step - loss: 0.9391 - acc: 0.6403 - val_loss: 0.4529 - val_acc: 0.8999\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.4123 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.4068 - acc: 0.9000 - val_loss: 0.4197 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.4042 - acc: 0.9000 - val_loss: 0.4340 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.4020 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3983 - acc: 0.9000 - val_loss: 0.4359 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3893 - acc: 0.9000 - val_loss: 0.4263 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3813 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.3723 - acc: 0.9000 - val_loss: 0.4033 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3620 - acc: 0.9000 - val_loss: 0.4002 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3511 - acc: 0.9000 - val_loss: 0.3874 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3397 - acc: 0.9002 - val_loss: 0.3667 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.3307 - acc: 0.9012 - val_loss: 0.3680 - val_acc: 0.9010\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3266 - acc: 0.9020 - val_loss: 0.3605 - val_acc: 0.9012\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.3228 - acc: 0.9028 - val_loss: 0.3591 - val_acc: 0.9012\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.3206 - acc: 0.9031 - val_loss: 0.3549 - val_acc: 0.9005\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.3176 - acc: 0.9039 - val_loss: 0.3421 - val_acc: 0.9024\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 3s 276us/step - loss: 0.3146 - acc: 0.9044 - val_loss: 0.3481 - val_acc: 0.9021\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.3139 - acc: 0.9042 - val_loss: 0.3532 - val_acc: 0.9023\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 3s 271us/step - loss: 0.3118 - acc: 0.9046 - val_loss: 0.3476 - val_acc: 0.9034\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3104 - acc: 0.9053 - val_loss: 0.3468 - val_acc: 0.9036\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3083 - acc: 0.9058 - val_loss: 0.3517 - val_acc: 0.9023\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.3069 - acc: 0.9058 - val_loss: 0.3568 - val_acc: 0.9018\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.3054 - acc: 0.9058 - val_loss: 0.3503 - val_acc: 0.9035\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3031 - acc: 0.9065 - val_loss: 0.3523 - val_acc: 0.9024\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.3025 - acc: 0.9061 - val_loss: 0.3582 - val_acc: 0.9015\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2997 - acc: 0.9069 - val_loss: 0.3514 - val_acc: 0.9026\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.2983 - acc: 0.9069 - val_loss: 0.3591 - val_acc: 0.9022\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2954 - acc: 0.9077 - val_loss: 0.3629 - val_acc: 0.9007\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2943 - acc: 0.9076 - val_loss: 0.3652 - val_acc: 0.9009\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2924 - acc: 0.9081 - val_loss: 0.3579 - val_acc: 0.9022\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.2890 - acc: 0.9090 - val_loss: 0.3637 - val_acc: 0.8982\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.2863 - acc: 0.9096 - val_loss: 0.3764 - val_acc: 0.8994\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.2825 - acc: 0.9099 - val_loss: 0.3780 - val_acc: 0.9008\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 3s 292us/step - loss: 0.2794 - acc: 0.9110 - val_loss: 0.3690 - val_acc: 0.9005\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 4s 308us/step - loss: 0.2754 - acc: 0.9114 - val_loss: 0.3759 - val_acc: 0.8957\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.2722 - acc: 0.9129 - val_loss: 0.3834 - val_acc: 0.8976\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 4s 302us/step - loss: 0.2677 - acc: 0.9142 - val_loss: 0.3864 - val_acc: 0.8985\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 4s 303us/step - loss: 0.2629 - acc: 0.9152 - val_loss: 0.3936 - val_acc: 0.8944\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 4s 301us/step - loss: 0.2584 - acc: 0.9164 - val_loss: 0.3864 - val_acc: 0.8977\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.2534 - acc: 0.9180 - val_loss: 0.4042 - val_acc: 0.8959\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 4s 305us/step - loss: 0.2463 - acc: 0.9204 - val_loss: 0.3994 - val_acc: 0.8932\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 4s 307us/step - loss: 0.2437 - acc: 0.9214 - val_loss: 0.4150 - val_acc: 0.8941\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.2358 - acc: 0.9235 - val_loss: 0.4056 - val_acc: 0.8911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2291 - acc: 0.9257 - val_loss: 0.4171 - val_acc: 0.8915\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.2214 - acc: 0.9282 - val_loss: 0.4322 - val_acc: 0.8912\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.2137 - acc: 0.9301 - val_loss: 0.4369 - val_acc: 0.8880\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.2073 - acc: 0.9326 - val_loss: 0.4457 - val_acc: 0.8846\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.1993 - acc: 0.9347 - val_loss: 0.4592 - val_acc: 0.8860\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.1915 - acc: 0.9368 - val_loss: 0.4762 - val_acc: 0.8856\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.1866 - acc: 0.9380 - val_loss: 0.4751 - val_acc: 0.8840\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 3s 275us/step - loss: 0.1761 - acc: 0.9412 - val_loss: 0.4916 - val_acc: 0.8816\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.1675 - acc: 0.9439 - val_loss: 0.5140 - val_acc: 0.8813\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.1603 - acc: 0.9464 - val_loss: 0.5078 - val_acc: 0.8820\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.1486 - acc: 0.9506 - val_loss: 0.5381 - val_acc: 0.8796\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1415 - acc: 0.9529 - val_loss: 0.5579 - val_acc: 0.8752\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.1328 - acc: 0.9554 - val_loss: 0.5710 - val_acc: 0.8731\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.1254 - acc: 0.9574 - val_loss: 0.5944 - val_acc: 0.8693\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1177 - acc: 0.9606 - val_loss: 0.6011 - val_acc: 0.8702\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.1094 - acc: 0.9631 - val_loss: 0.6275 - val_acc: 0.8729\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.1021 - acc: 0.9657 - val_loss: 0.6488 - val_acc: 0.8688\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.0971 - acc: 0.9673 - val_loss: 0.6554 - val_acc: 0.8706\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.0879 - acc: 0.9709 - val_loss: 0.6713 - val_acc: 0.8698\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.0810 - acc: 0.9731 - val_loss: 0.6858 - val_acc: 0.8671\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.0742 - acc: 0.9754 - val_loss: 0.7290 - val_acc: 0.8668\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.0685 - acc: 0.9777 - val_loss: 0.7540 - val_acc: 0.8637\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.0662 - acc: 0.9780 - val_loss: 0.7710 - val_acc: 0.8649\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.0570 - acc: 0.9819 - val_loss: 0.8074 - val_acc: 0.8664\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.0512 - acc: 0.9838 - val_loss: 0.8063 - val_acc: 0.8658\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 4s 304us/step - loss: 0.0463 - acc: 0.9860 - val_loss: 0.8325 - val_acc: 0.8606\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 4s 310us/step - loss: 0.0461 - acc: 0.9858 - val_loss: 0.8476 - val_acc: 0.8624\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.0545 - acc: 0.9820 - val_loss: 0.8530 - val_acc: 0.8632\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 4s 299us/step - loss: 0.0444 - acc: 0.9862 - val_loss: 0.8787 - val_acc: 0.8599\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 4s 305us/step - loss: 0.0360 - acc: 0.9897 - val_loss: 0.8931 - val_acc: 0.8633\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.0319 - acc: 0.9909 - val_loss: 0.9118 - val_acc: 0.8650\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.0267 - acc: 0.9930 - val_loss: 0.9442 - val_acc: 0.8623\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.0315 - acc: 0.9910 - val_loss: 0.9574 - val_acc: 0.8652\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.0277 - acc: 0.9924 - val_loss: 0.9534 - val_acc: 0.8622\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.0237 - acc: 0.9939 - val_loss: 0.9886 - val_acc: 0.8648\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.0256 - acc: 0.9930 - val_loss: 0.9790 - val_acc: 0.8606\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 14s 1ms/step - loss: 0.8933 - acc: 0.6600 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.4115 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 3s 277us/step - loss: 0.4062 - acc: 0.9000 - val_loss: 0.4265 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.4041 - acc: 0.9000 - val_loss: 0.4193 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.4019 - acc: 0.9000 - val_loss: 0.4347 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 4s 300us/step - loss: 0.3984 - acc: 0.9000 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3908 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3811 - acc: 0.9000 - val_loss: 0.4174 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3690 - acc: 0.9000 - val_loss: 0.4120 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3553 - acc: 0.9000 - val_loss: 0.3881 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 0.3424 - acc: 0.9003 - val_loss: 0.3708 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3334 - acc: 0.9011 - val_loss: 0.3710 - val_acc: 0.9003\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.3285 - acc: 0.9017 - val_loss: 0.3603 - val_acc: 0.9010\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3230 - acc: 0.9027 - val_loss: 0.3622 - val_acc: 0.9008\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3204 - acc: 0.9036 - val_loss: 0.3509 - val_acc: 0.9022\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 3s 292us/step - loss: 0.3177 - acc: 0.9039 - val_loss: 0.3515 - val_acc: 0.9018\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3154 - acc: 0.9045 - val_loss: 0.3550 - val_acc: 0.9012\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 4s 293us/step - loss: 0.3134 - acc: 0.9048 - val_loss: 0.3457 - val_acc: 0.9031\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.3123 - acc: 0.9047 - val_loss: 0.3518 - val_acc: 0.9016\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3103 - acc: 0.9051 - val_loss: 0.3569 - val_acc: 0.9008\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.3078 - acc: 0.9059 - val_loss: 0.3611 - val_acc: 0.9009\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3062 - acc: 0.9062 - val_loss: 0.3575 - val_acc: 0.9022\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.3047 - acc: 0.9062 - val_loss: 0.3614 - val_acc: 0.9009\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 4s 295us/step - loss: 0.3033 - acc: 0.9064 - val_loss: 0.3518 - val_acc: 0.9025\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.3013 - acc: 0.9068 - val_loss: 0.3591 - val_acc: 0.9005\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.3000 - acc: 0.9074 - val_loss: 0.3554 - val_acc: 0.9017\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.2971 - acc: 0.9076 - val_loss: 0.3465 - val_acc: 0.9034\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 4s 297us/step - loss: 0.2948 - acc: 0.9078 - val_loss: 0.3574 - val_acc: 0.9024\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2920 - acc: 0.9085 - val_loss: 0.3510 - val_acc: 0.9027\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.2893 - acc: 0.9088 - val_loss: 0.3670 - val_acc: 0.8992\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 4s 292us/step - loss: 0.2869 - acc: 0.9093 - val_loss: 0.3586 - val_acc: 0.9027\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 4s 294us/step - loss: 0.2828 - acc: 0.9103 - val_loss: 0.3643 - val_acc: 0.8989\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2829 - acc: 0.9103 - val_loss: 0.3718 - val_acc: 0.8982\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 3s 278us/step - loss: 0.2765 - acc: 0.9121 - val_loss: 0.3812 - val_acc: 0.8995\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2718 - acc: 0.9130 - val_loss: 0.3738 - val_acc: 0.8999\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.2669 - acc: 0.9139 - val_loss: 0.3813 - val_acc: 0.8959\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.2627 - acc: 0.9152 - val_loss: 0.3959 - val_acc: 0.8927\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2574 - acc: 0.9169 - val_loss: 0.3851 - val_acc: 0.8945\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2524 - acc: 0.9184 - val_loss: 0.3933 - val_acc: 0.8961\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2462 - acc: 0.9203 - val_loss: 0.3956 - val_acc: 0.8938\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.2403 - acc: 0.9221 - val_loss: 0.4098 - val_acc: 0.8936\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.2330 - acc: 0.9242 - val_loss: 0.4122 - val_acc: 0.8912\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.2274 - acc: 0.9259 - val_loss: 0.4171 - val_acc: 0.8924\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.2207 - acc: 0.9278 - val_loss: 0.4193 - val_acc: 0.8891\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.2144 - acc: 0.9299 - val_loss: 0.4471 - val_acc: 0.8886\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.2061 - acc: 0.9324 - val_loss: 0.4552 - val_acc: 0.8876\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.1978 - acc: 0.9353 - val_loss: 0.4492 - val_acc: 0.8861\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.1919 - acc: 0.9367 - val_loss: 0.4797 - val_acc: 0.8842\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.1831 - acc: 0.9396 - val_loss: 0.4747 - val_acc: 0.8853\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.1751 - acc: 0.9427 - val_loss: 0.4892 - val_acc: 0.8824\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1674 - acc: 0.9448 - val_loss: 0.5076 - val_acc: 0.8807\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1586 - acc: 0.9474 - val_loss: 0.5156 - val_acc: 0.8856\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.1517 - acc: 0.9495 - val_loss: 0.5337 - val_acc: 0.8768\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.1433 - acc: 0.9524 - val_loss: 0.5538 - val_acc: 0.8727\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1364 - acc: 0.9544 - val_loss: 0.5661 - val_acc: 0.8763\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.1305 - acc: 0.9559 - val_loss: 0.5681 - val_acc: 0.8716\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.1212 - acc: 0.9591 - val_loss: 0.5978 - val_acc: 0.8709\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.1129 - acc: 0.9622 - val_loss: 0.6133 - val_acc: 0.8732\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.1042 - acc: 0.9650 - val_loss: 0.6136 - val_acc: 0.8748\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 3s 286us/step - loss: 0.0970 - acc: 0.9677 - val_loss: 0.6554 - val_acc: 0.8729\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0985 - acc: 0.9663 - val_loss: 0.6629 - val_acc: 0.8723\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.0840 - acc: 0.9718 - val_loss: 0.6941 - val_acc: 0.8688\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 4s 298us/step - loss: 0.0781 - acc: 0.9741 - val_loss: 0.7004 - val_acc: 0.8690\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0715 - acc: 0.9768 - val_loss: 0.7332 - val_acc: 0.8683\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0688 - acc: 0.9771 - val_loss: 0.7468 - val_acc: 0.8718\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.0656 - acc: 0.9784 - val_loss: 0.7569 - val_acc: 0.8653\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0575 - acc: 0.9815 - val_loss: 0.7740 - val_acc: 0.8668\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.0551 - acc: 0.9825 - val_loss: 0.7907 - val_acc: 0.8661\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.0474 - acc: 0.9853 - val_loss: 0.8265 - val_acc: 0.8639\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0439 - acc: 0.9872 - val_loss: 0.8396 - val_acc: 0.8631\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.0387 - acc: 0.9888 - val_loss: 0.8598 - val_acc: 0.8647\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.0348 - acc: 0.9902 - val_loss: 0.8869 - val_acc: 0.8575\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0371 - acc: 0.9892 - val_loss: 0.8855 - val_acc: 0.8606\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.0444 - acc: 0.9860 - val_loss: 0.8925 - val_acc: 0.8627\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.0306 - acc: 0.9914 - val_loss: 0.9253 - val_acc: 0.8605\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 3s 290us/step - loss: 0.0257 - acc: 0.9934 - val_loss: 0.9382 - val_acc: 0.8635\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.0232 - acc: 0.9939 - val_loss: 0.9605 - val_acc: 0.8636\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.0421 - acc: 0.9863 - val_loss: 0.9471 - val_acc: 0.8645\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0333 - acc: 0.9898 - val_loss: 0.9612 - val_acc: 0.8609\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.9791 - val_acc: 0.8629\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.0171 - acc: 0.9960 - val_loss: 1.0009 - val_acc: 0.8621\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0184 - acc: 0.9954 - val_loss: 1.0120 - val_acc: 0.8658\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 3s 280us/step - loss: 0.0158 - acc: 0.9962 - val_loss: 1.0345 - val_acc: 0.8642\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 3s 285us/step - loss: 0.0227 - acc: 0.9936 - val_loss: 1.0479 - val_acc: 0.8674\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.0342 - acc: 0.9890 - val_loss: 1.0084 - val_acc: 0.8610\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 1.0399 - val_acc: 0.8632\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 1.0416 - val_acc: 0.8660\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.0113 - acc: 0.9973 - val_loss: 1.0712 - val_acc: 0.8632\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.0108 - acc: 0.9974 - val_loss: 1.0794 - val_acc: 0.8623\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.0104 - acc: 0.9974 - val_loss: 1.1008 - val_acc: 0.8639\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.0104 - acc: 0.9975 - val_loss: 1.0874 - val_acc: 0.8639\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.0102 - acc: 0.9975 - val_loss: 1.1164 - val_acc: 0.8649\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.0106 - acc: 0.9973 - val_loss: 1.1014 - val_acc: 0.8667\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 3s 289us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 1.1269 - val_acc: 0.8628\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0277 - acc: 0.9911 - val_loss: 1.1287 - val_acc: 0.8674\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 3s 282us/step - loss: 0.0435 - acc: 0.9850 - val_loss: 1.0759 - val_acc: 0.8557\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.0225 - acc: 0.9928 - val_loss: 1.0847 - val_acc: 0.8637\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 1.0989 - val_acc: 0.8632\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 0.0089 - acc: 0.9978 - val_loss: 1.1227 - val_acc: 0.8641\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 3s 281us/step - loss: 0.0171 - acc: 0.9950 - val_loss: 1.1094 - val_acc: 0.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_110 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_107/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_107/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_111 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_22/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_22/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_115 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_112/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_112/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_116 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_23/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_23/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_120 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_117/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_117/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_121 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_24/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_24/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_125 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_122/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_122/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_126 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_25/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_25/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_130 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_127/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_127/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_131 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_26/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_26/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_135 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_132/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_132/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_136 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_27/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_27/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_140 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_137/strided_slice_16:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'cu_dnnlstm_137/strided_slice_17:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_141 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_28/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_28/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_110 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_153:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_154:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_111 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_155:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_156:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_115 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_160:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_161:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_116 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_162:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_163:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_120 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_167:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_168:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_121 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_169:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_170:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_125 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_174:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_175:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_126 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_176:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_177:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_130 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_181:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_182:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_131 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_183:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_184:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_135 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_188:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_189:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_136 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_190:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_191:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_140 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_195:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_196:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_141 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_197:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_198:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4334885922074318\n",
      "1 0.42987937211990357\n",
      "2 0.42646721839904783\n",
      "3 0.41931536436080935\n",
      "4 0.4346692451834679\n",
      "5 0.43817056238651275\n",
      "6 0.42680922001600263\n",
      "7 0.4173956632614136\n",
      "8 0.4119512552022934\n",
      "9 0.3881343144178391\n",
      "10 0.3708020815253258\n",
      "11 0.3709831121563911\n",
      "12 0.36029030233621595\n",
      "13 0.362221639752388\n",
      "14 0.35085381925106046\n",
      "15 0.351451233625412\n",
      "16 0.35497591853141786\n",
      "17 0.34572834104299544\n",
      "18 0.3517788055539131\n",
      "19 0.35690314441919324\n",
      "20 0.36109700083732604\n",
      "21 0.3574961271882057\n",
      "22 0.3614120638370514\n",
      "23 0.35183446705341337\n",
      "24 0.35908723264932635\n",
      "25 0.3553580129146576\n",
      "26 0.3464580002427101\n",
      "27 0.35739547908306124\n",
      "28 0.3509622463583946\n",
      "29 0.3670074835419655\n",
      "30 0.35861141443252564\n",
      "31 0.3643028038740158\n",
      "32 0.37182775408029556\n",
      "33 0.38116995990276337\n",
      "34 0.37382332116365435\n",
      "35 0.3812621733546257\n",
      "36 0.3959032654762268\n",
      "37 0.3851198163628578\n",
      "38 0.39328441888093946\n",
      "39 0.39563667714595796\n",
      "40 0.4097863084077835\n",
      "41 0.41224675804376604\n",
      "42 0.4171355438232422\n",
      "43 0.41932867735624313\n",
      "44 0.4471014487743378\n",
      "45 0.45521599233150484\n",
      "46 0.44916351735591886\n",
      "47 0.47972274363040923\n",
      "48 0.4747160941362381\n",
      "49 0.4891998639702797\n",
      "50 0.5076209226250649\n",
      "51 0.5156339779496193\n",
      "52 0.5337140041589737\n",
      "53 0.5537885946035385\n",
      "54 0.5661177983880044\n",
      "55 0.5680905169248581\n",
      "56 0.5978105610609055\n",
      "57 0.6132835534214973\n",
      "58 0.6135785835981369\n",
      "59 0.6553614836931229\n",
      "60 0.6629228222370148\n",
      "61 0.6941436517238617\n",
      "62 0.7003506475687027\n",
      "63 0.7332103800773621\n",
      "64 0.7468133288621902\n",
      "65 0.7568777912855148\n",
      "66 0.7739699208736419\n",
      "67 0.7906788867712021\n",
      "68 0.8265051543712616\n",
      "69 0.8395588481426239\n",
      "70 0.8598400741815567\n",
      "71 0.8868576997518539\n",
      "72 0.8855435031652451\n",
      "73 0.892529987692833\n",
      "74 0.9253497523069382\n",
      "75 0.9382168114185333\n",
      "76 0.9604718995094299\n",
      "77 0.9470566934347153\n",
      "78 0.9611825460195541\n",
      "79 0.9791130089759826\n",
      "80 1.0008880811929703\n",
      "81 1.0120315390825272\n",
      "82 1.0344806718826294\n",
      "83 1.0478956109285356\n",
      "84 1.0084303975105287\n",
      "85 1.0398800909519195\n",
      "86 1.0415565818548203\n",
      "87 1.0711858403682708\n",
      "88 1.0794132608175278\n",
      "89 1.100774309039116\n",
      "90 1.0873961514234542\n",
      "91 1.116405548453331\n",
      "92 1.1013715541362763\n",
      "93 1.1268737244606017\n",
      "94 1.128717594742775\n",
      "95 1.0758506494760514\n",
      "96 1.0846747332811355\n",
      "97 1.098893483877182\n",
      "98 1.1226704812049866\n",
      "99 1.1094269239902497\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 15s 1ms/step - loss: 0.8219 - acc: 0.6949 - val_loss: 0.4225 - val_acc: 0.9000\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 3s 283us/step - loss: 0.4094 - acc: 0.9000 - val_loss: 0.4165 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 3s 287us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4209 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 3s 288us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4412 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.4004 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "def lstm_model(latent_dim, half):\n",
    "    batch_size = 1000  # Batch size for training.\n",
    "    epochs = 45  # Number of epochs to train for.\n",
    "#     latent_dim = 128  # Latent dimensionality of the encoding space.\n",
    "#     half = 64\n",
    "    num_samples = 10000  # Number of samples to train on.\n",
    "    encoder_inputs = Input(shape=(None, 6))\n",
    "    decoder_inputs = Input(shape=(None, 12))\n",
    "    \n",
    "    encoder = CuDNNLSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    _, state_h2, state_c2 = LSTM(latent_dim, return_state=True)(encoder_outputs)\n",
    "    encoder_states = [state_h, state_c, state_h2, state_c2]\n",
    "\n",
    "\n",
    "    out_layer1 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    d_outputs, dh1, dc1 = out_layer1(decoder_inputs,initial_state= [state_h, state_c])\n",
    "    out_layer2 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    final, dh2, dc2 = out_layer2(d_outputs, initial_state= [state_h2, state_c2])\n",
    "    decoder_dense = Dense(5, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(final)\n",
    "    \n",
    "    # Set up the decoder.\n",
    "    decoder_inputs = Input(shape=(None, 12))\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_lstm2 = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=[state_h, state_c])\n",
    "    final , _, _= decoder_lstm2(decoder_outputs, initial_state= [state_h2, state_c2])\n",
    "    decoder_dense = Dense(6, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(final)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    #inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_h1 = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c1 = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, \n",
    "                             decoder_state_input_h1, decoder_state_input_c1]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs[:2])\n",
    "    decoder_outputs, state_h1, state_c1 = decoder_lstm2(\n",
    "        decoder_outputs, initial_state=decoder_states_inputs[-2:])\n",
    "    decoder_states = [state_h, state_c, state_h1, state_c1]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "    # Run training\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def modelFit(epoch, batchSize, latent_dim, half, X_train, y_train, y_train1):\n",
    "    model1, encoder_model1, decoder_model1 = lstm_model(latent_dim, half)\n",
    "    hist1 = model1.fit([X_train, y_train1], y_train,\n",
    "          batch_size=batchSize,\n",
    "          epochs=epoch,\n",
    "          validation_data=([X_val,y_val1], y_val),\n",
    "          verbose = 1\n",
    "         )\n",
    "    return hist1, model1, encoder_model1, decoder_model1\n",
    "\n",
    "def grid_search(latent, half,train_size, X_train, y_train, y_train1):\n",
    "    hist1, model1, encoder_model1, decoder_model1 = modelFit(5, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist2 ,model2, encoder_model2, decoder_model2 = modelFit(10, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist3 ,model3, encoder_model3, decoder_model3 = modelFit(20, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist4 ,model4, encoder_model4, decoder_model4 = modelFit(30, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist5 ,model5, encoder_model5, decoder_model5 = modelFit(50, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist6 ,model6, encoder_model6, decoder_model6 = modelFit(80, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist7 ,model7, encoder_model7, decoder_model7 = modelFit(100, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist8 ,model8, encoder_model8, decoder_model8 = modelFit(500, 100, latent, half)\n",
    "\n",
    "    model1.save(\"models/{}_{}_5_double.h5\".format(train_size,half))\n",
    "    model2.save(\"models/{}_{}_10_double.h5\".format(train_size,half))\n",
    "    model3.save(\"models/{}_{}_20_double.h5\".format(train_size,half))\n",
    "    model4.save(\"models/{}_{}_30_double.h5\".format(train_size,half))\n",
    "    model5.save(\"models/{}_{}_50_double.h5\".format(train_size,half))\n",
    "    model6.save(\"models/{}_{}_80_double.h5\".format(train_size,half))\n",
    "    model7.save(\"models/{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #model8.save(\"{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    encoder_model1.save(\"models/E{}_{}_5_double.h5\".format(train_size,half))\n",
    "    encoder_model2.save(\"models/E{}_{}_10_double.h5\".format(train_size,half))\n",
    "    encoder_model3.save(\"models/E{}_{}_20_double.h5\".format(train_size,half))\n",
    "    encoder_model4.save(\"models/E{}_{}_30_double.h5\".format(train_size,half))\n",
    "    encoder_model5.save(\"models/E{}_{}_50_double.h5\".format(train_size,half))\n",
    "    encoder_model6.save(\"models/E{}_{}_80_double.h5\".format(train_size,half))\n",
    "    encoder_model7.save(\"models/E{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #encoder_model8.save(\"E{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    decoder_model1.save(\"models/D{}_{}_5_double.h5\".format(train_size,half))\n",
    "    decoder_model2.save(\"models/D{}_{}_10_double.h5\".format(train_size,half))\n",
    "    decoder_model3.save(\"models/D{}_{}_20_double.h5\".format(train_size,half))\n",
    "    decoder_model4.save(\"models/D{}_{}_30_double.h5\".format(train_size,half))\n",
    "    decoder_model5.save(\"models/D{}_{}_50_double.h5\".format(train_size,half))\n",
    "    decoder_model6.save(\"models/D{}_{}_80_double.h5\".format(train_size,half))\n",
    "    decoder_model7.save(\"models/D{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #decoder_model8.save(\"D{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    count = [i for i in range(len(hist7.history['val_loss']))]\n",
    "    for i, value in zip(count, hist7.history['val_loss']):\n",
    "        print(i, value)\n",
    "\n",
    "grid_search(4, 2, 12000, X_train, y_train, y_train1)\n",
    "grid_search(32, 16, 12000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 12000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 12000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 12000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 12000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 12000, X_train, y_train, y_train1)\n",
    "grid_search(8192, 4096, 12000, X_train, y_train, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:24000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:24000]\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:24000]\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "\n",
    "grid_search(32, 16, 24000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 24000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 24000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 24000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 24000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 24000, X_train, y_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:48000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:48000]\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:48000]\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "\n",
    "\n",
    "grid_search(32, 16, 48000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 48000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 48000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 48000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 48000, X_train, y_train, y_train1)\n",
    "grid_search(1048, 512, 48000, X_train, y_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:96000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:96000]\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:96000]\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "\n",
    "\n",
    "grid_search(32, 16, 96000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 96000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 96000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 96000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 96000, X_train, y_train, y_train1)\n",
    "grid_search(1096, 512, 96000, X_train, y_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model, encoder_model, decoder_model):\n",
    "    nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "    # Encode the input as state vectors.\n",
    "    #print(input_seq[0,0])\n",
    "    index = 0\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    #print(len(states_value))\n",
    "    #print(states_value)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 12))\n",
    "    target_seq[0][0]= np.hstack((input_seq[0,index], np.array([1,0,0,0,0,0])))\n",
    "    #print(target_seq)\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    probability = 1\n",
    "    \n",
    "    while not stop_condition:\n",
    "        index = index +1\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        #sampled_token_index = np.random.choice(6, 1, p=output_tokens[0, -1, :])[0]\n",
    "        \n",
    "        #print(output_tokens[0, -1, :])\n",
    "        sampled_char = nucleotide[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        #print(decoded_sentence)\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == 10):\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 12))\n",
    "        temp = np.zeros((6))\n",
    "        temp[sampled_token_index] = 1\n",
    "        target_seq[0][0]= np.hstack((input_seq[0, index], temp))\n",
    "        # target_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def get_prob(input_seq, target, model, encoder_model, decoder_model):\n",
    "    nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "    # Encode the input as state vectors.\n",
    "    index = 0\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, 12))\n",
    "    target_seq[0][0]= np.hstack((input_seq[0,index], np.array([1,0,0,0,0,0])))\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    probability = 1.0\n",
    "    \n",
    "    while not stop_condition:\n",
    "        index = index +1\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        #print(output_tokens[0, -1, :])\n",
    "        sampled_token_index = np.argmax(target[index-1])\n",
    "        #sampled_token_index = np.random.choice(6, 1, p=output_tokens[0, -1, :])[0]\n",
    "        probability = probability * output_tokens[0, -1, :][sampled_token_index]\n",
    "        #print(output_tokens[0, -1, :])\n",
    "        sampled_char = nucleotide[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        #print(decoded_sentence)\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == 10):\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 12))\n",
    "        temp = np.zeros((6))\n",
    "        temp[sampled_token_index] = 1\n",
    "        target_seq[0][0]= np.hstack((input_seq[0, index], temp))\n",
    "        # target_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence, probability\n",
    "\n",
    "def decode_gru(input_seq, model, encoder_model, decoder_model):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, 6))\n",
    "    target_seq[0][0]= np.array([1,0,0,0,0,0])\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = nucleotide[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == 10):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 6))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def diffList(a, b):\n",
    "    count = 0\n",
    "    length = len(a)\n",
    "    for i in range(length):\n",
    "        if a[i] != b[i]:\n",
    "            count = count+1\n",
    "    return count\n",
    "def predict2(X_test, y_test, model, encoder_model, decoder_model, gru=False):\n",
    "    x_true =[]\n",
    "    y_hat =[]\n",
    "    y_true =[]\n",
    "    probList=[]\n",
    "    productProb = 0\n",
    "    for seq_index in range(len(X_test)):\n",
    "        input_seq = X_test[seq_index: seq_index + 1]\n",
    "        #print(input_seq[0])\n",
    "        if gru:\n",
    "            decoded_sentence = decode_gru(input_seq, model, encoder_model, decoder_model)\n",
    "        else :\n",
    "            decoded_sentence = decode_sequence(input_seq, model, encoder_model, decoder_model)\n",
    "        _, prob = get_prob(input_seq, y_test[seq_index], model, encoder_model, decoder_model)\n",
    "        probList.append(prob)\n",
    "        productProb = productProb+ math.log(prob)\n",
    "        input_sen = decoder(input_seq[0])\n",
    "        print(input_sen, ' -> ',\n",
    "              decoded_sentence, 'True:', decoder(y_test[seq_index]), \n",
    "              printHitMiss(decoded_sentence, decoder(y_test[seq_index])), \n",
    "              diffList(input_sen, decoded_sentence)\n",
    "             )\n",
    "        print(input_sen, ' -> ',\n",
    "              decoder(y_test[seq_index]), 'True:', decoder(y_test[seq_index]), \n",
    "              prob,\n",
    "              printHitMiss(decoded_sentence, decoder(y_test[seq_index])), \n",
    "              diffList(input_sen, decoded_sentence)\n",
    "             )\n",
    "        print()\n",
    "        x_true.append(input_sen)\n",
    "        y_hat.append(decoded_sentence)\n",
    "        y_true.append(decoder(y_test[seq_index]))\n",
    "    print(\"Mean and std of probabilities : {} , {}  \".format(np.mean(probList), np.std(probList)))\n",
    "    print(\"Sum of log probabilities : {}\".format(productProb))\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat, y_true)))\n",
    "    #print(\"Test loss : {}\".format(keras.losses.categorical_crossentropy(y_true, y_hat)))\n",
    "    return x_true, y_hat, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.summary()\n",
    "#model2.summary()\n",
    "def grid_predict(train_size, half, X_test, y_test):\n",
    "    model1 = load_model(\"{}_{}_5.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_10.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_20.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_30.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_50.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_80.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_100.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_500.h5\".format(train_size,half))\n",
    "\n",
    "    encoder_model1 = load_model(\"E{}_{}_5.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_10.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_20.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_30.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_50.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_80.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_100.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_500.h5\".format(train_size,half))\n",
    "\n",
    "    decoder_model1 =load_model(\"D{}_{}_5.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_10.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_20.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_30.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_50.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_80.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_100.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_500.h5\".format(train_size,half))\n",
    "    #return model1, encoder_model1, decoder_model1\n",
    "    x_true, y_hat, y_true = predict2(X_test, y_test, model1, encoder_model1, decoder_model1, gru=False)\n",
    "model1, encoder_model1, decoder_model1 = grid_predict(12000, 128, X_test, y_test)\n",
    "\n",
    "encoder_model1.summary()\n",
    "decoder_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [i for i in range(len(hist7.history['val_loss']))]\n",
    "for i, value in zip(count, hist7.history['val_loss']):\n",
    "    print(i, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotgraph(hist1):\n",
    "    plt.plot(hist1.history['val_acc'])\n",
    "    plt.plot(hist1.history['acc'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['validation', 'train'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(hist1.history['val_loss'])\n",
    "    plt.plot(hist1.history['loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['validation', 'train'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "x_true, y_hat, y_true = predict2(X_test, y_test, model1, encoder_model1, decoder_model1, gru=False)\n",
    "#plotgraph(hist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true2, y_hat2, y_true2 = predict2(X_test, y_test, model2, encoder_model2, decoder_model2)\n",
    "plotgraph(hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true3, y_hat3, y_true3 = predict2(X_test, y_test, model3, encoder_model3, decoder_model3)\n",
    "plotgraph(hist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true4, y_hat4, y_true4 = predict2(X_test, y_test, model4, encoder_model4, decoder_model4)\n",
    "plotgraph(hist4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true5, y_hat5, y_true5 = predict2(X_test, y_test, model5, encoder_model5, decoder_model5)\n",
    "plotgraph(hist5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true6, y_hat6, y_true6 = predict2(X_test, y_test, model6, encoder_model6, decoder_model6)\n",
    "plotgraph(hist6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true7, y_hat7, y_true7 = predict2(X_test, y_test, model7, encoder_model7, decoder_model7)\n",
    "plotgraph(hist7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true8, y_hat8, y_true8 = predict2(X_test, y_test, model8, encoder_model8, decoder_model8)\n",
    "plotgraph(hist8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding ratio of mutation on predicted sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countMutation(ancestor, target, realDes): \n",
    "    AtoG = 0\n",
    "    AtoC = 0\n",
    "    AtoT= 0\n",
    "    GtoA= 0\n",
    "    GtoC= 0\n",
    "    GtoT= 0\n",
    "    CtoA= 0\n",
    "    CtoG= 0\n",
    "    CtoT= 0\n",
    "    TtoA= 0\n",
    "    TtoG= 0\n",
    "    TtoC= 0\n",
    "    CG = 0\n",
    "    CA = 0\n",
    "    CAGT = 0\n",
    "    CAAT = 0\n",
    "    transitionCorrect = 0\n",
    "    transversionCorrect = 0\n",
    "    for j in range(len(ancestor)):\n",
    "        for i in range(10):\n",
    "\n",
    "                if ancestor[j][i] == 'A' and target[j][i] == 'G':\n",
    "                    AtoG = AtoG+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect+=1\n",
    "                elif ancestor[j][i] == 'A' and target[j][i] == 'C':\n",
    "                    AtoC = AtoC+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'A' and target[j][i] == 'T':\n",
    "                    AtoT = AtoT+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'G' and target[j][i] == 'A':\n",
    "                    GtoA = GtoA+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect +=1\n",
    "                elif ancestor[j][i] == 'G' and target[j][i] == 'C':\n",
    "                    GtoC = GtoC+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'G' and target[j][i] == 'T':\n",
    "                    GtoT = GtoT+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'C' and target[j][i] == 'A':\n",
    "                    CtoA = CtoA+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'C' and target[j][i] == 'G':\n",
    "                    CtoG = CtoG+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'C' and target[j][i] == 'T':\n",
    "                    CtoT = CtoT+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect +=1\n",
    "                elif ancestor[j][i] == 'T' and target[j][i] == 'A':\n",
    "                    TtoA = TtoA+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'T' and target[j][i] == 'G':\n",
    "                    TtoG = TtoG+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'T' and target[j][i] == 'C':\n",
    "                    TtoC = TtoC+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect +=1\n",
    "                else :\n",
    "                    transitionCorrect = transitionCorrect\n",
    "                if i<9 and ancestor[j][i] == 'C' and ancestor[j][i+1] == 'G' :\n",
    "                    CG = CG+1\n",
    "                    if target[j][i] == 'C' and target[j][i+1] == 'A' :\n",
    "                        CA = CA+1\n",
    "                if i<6 and ancestor[j][i] == 'C' and ancestor[j][i+1] == 'A' and ancestor[j][i+2] == 'A' and ancestor[j][i+3] == 'T':\n",
    "                    CAAT = CAAT+1\n",
    "#                     print(ancestor[j])\n",
    "#                     print(target[j])\n",
    "                    if target[j][i] == 'C' and target[j][i+1] == 'A'and target[j][i+2] == 'G' and target[j][i+3] == 'T':\n",
    "                        CAGT = CAGT+1\n",
    "                        #print(CAAT)\n",
    "                #print(CAAT)\n",
    "\n",
    "    print('This is A-->G :{}'.format(AtoG))\n",
    "    print('This is A-->C :{}'.format(AtoC))\n",
    "    print('This is A-->T :{}'.format(AtoT))\n",
    "    print('This is G-->A :{}'.format(GtoA))\n",
    "    print('This is G-->C :{}'.format(GtoC))\n",
    "    print('This is G-->T :{}'.format(GtoT))\n",
    "    print('This is C-->A :{}'.format(CtoA))\n",
    "    print('This is C-->G :{}'.format(CtoG))\n",
    "    print('This is C-->T :{}'.format(CtoT))\n",
    "    print('This is T-->A :{}'.format(TtoA))\n",
    "    print('This is T-->G :{}'.format(TtoG))\n",
    "    print('This is T-->C :{}'.format(TtoC))\n",
    "    numMutation = AtoG+AtoC+AtoT+GtoA+GtoC+GtoT+CtoA+CtoG+CtoT+TtoA+TtoG+TtoC\n",
    "    transition = AtoG+GtoA+CtoT+TtoC\n",
    "    transversion = numMutation-transition\n",
    "    print('Total number of mutations: {}'.format(numMutation))\n",
    "    print('Number of transitions: {}'.format( transition))\n",
    "    print('Number of transversions: {}'.format(transversion))\n",
    "    if transversion !=0 :\n",
    "        print('Ratio of transition/transversion : {}'.format(transition/transversion))\n",
    "    print('Percentage of mutation : {}'.format(numMutation/(10*(len(X_test)))))\n",
    "    print('Percentage of transition predicted correct : {}'.format(transitionCorrect/transition))\n",
    "    if transversion !=0 :\n",
    "        print('Percentage of transversion predicted correct : {}'.format(transversionCorrect/transversion))\n",
    "    \n",
    "    print('This is the ratio of CG-->CA :{}'.format(CA/CG))\n",
    "    print('This is the ratio of CAAT-->CAGT :{}'.format(CAGT/CAAT))\n",
    "    \n",
    "def positionAccuracy(x_true, y_hat, y_true):\n",
    "    n = len(x_true[0])\n",
    "    totalMut = [0] * n\n",
    "    correctMut = [0] * n\n",
    "    posAccuracy = [0] * n\n",
    "    for i in range(n):\n",
    "        count = 0\n",
    "        countCorrect = 0\n",
    "        for j in range(len(x_true)):\n",
    "            if x_true[j][i] != y_true[j][i]:\n",
    "                count = count+1\n",
    "                if y_hat[j][i]== y_true[j][i]:\n",
    "                    countCorrect = countCorrect+1\n",
    "        #print(countCorrect, count)\n",
    "        totalMut[i]=count\n",
    "        correctMut[i]=countCorrect\n",
    "        posAccuracy[i]=countCorrect/float(count)\n",
    "        \n",
    "        \n",
    "    print ('This is the accuracy for each position : {}'.format(posAccuracy))\n",
    "\n",
    "#countMutation(x_true6, y_hat6, y_true6)\n",
    "#countMutation(x_true8, y_hat8, y_true8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countMutation(x_true3, y_hat3, y_true3)\n",
    "#positionAccuracy(x_true, y_hat, y_true)\n",
    "countMutation(x_true4, y_hat4, y_true4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMutation(x_true8, y_hat8, y_true8)\n",
    "#positionAccuracy(x_true2, y_hat2, y_true2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMutation(x_true, y_true, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test5=np.load('prepData/X_test_camFam3_1mutOnly_v3.npy')\n",
    "y_test5=np.load('prepData/y_test_camFam3_1mutOnly_v3.npy')\n",
    "x_true5, y_hat5, y_true5 = predict2(X_test5, y_test5, model1, encoder_model1, decoder_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true6, y_hat6, y_true6 = predict2(X_test5, y_test5, model2, encoder_model2, decoder_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMutation(x_true5, y_hat5, y_true5)\n",
    "countMutation(x_true6, y_hat6, y_true6)\n",
    "countMutation(x_true5, y_true5, y_true5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
