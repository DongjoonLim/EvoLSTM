{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 14 22:37:45 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   22C    P8     6W / 250W |  10776MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 30%   25C    P8    22W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  On   | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 30%   23C    P8     1W / 250W |  10772MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  On   | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 30%   24C    P8    20W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  On   | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8     9W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 30%   21C    P8     1W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 29%   23C    P8    14W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  On   | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 30%   22C    P8     3W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  GeForce RTX 208...  On   | 00000000:40:00.0 Off |                  N/A |\n",
      "| 29%   23C    P8    19W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  GeForce RTX 208...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 30%   24C    P8    12W / 250W |      0MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     23655      C   ...m63/miniconda3/envs/research/bin/python 10765MiB |\n",
      "|    2     25047      C   ...m63/miniconda3/envs/research/bin/python 10761MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, TimeDistributed, Dense, RepeatVector, CuDNNLSTM, GRU, Bidirectional, Input, CuDNNGRU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "import difflib\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "seq_length = 15\n",
    "val_loss_hist = []\n",
    "\n",
    "K.clear_session()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "X_train=np.load('prepData/X_train_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "X_val=np.load('prepData/X_val_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "X_test=np.load('prepData/X_test_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "y_train=np.load('prepData/y_train_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "y_val=np.load('prepData/y_val_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "y_test=np.load('prepData/y_test_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "\n",
    "y_train1 = np.load('prepData/y_train1_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "y_val1 = np.load('prepData/y_val1_gap_pad_camFam3_v3_chr2_size10.npy')\n",
    "y_test1 = np.load('prepData/y_test1_gap_pad_camFam3_v3_chr2_size10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1 0 0 0 0]\n",
      "  [0 1 0 0 0 0]\n",
      "  [0 0 0 1 0 0]\n",
      "  ...\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0]\n",
      "  [0 0 0 1 0 0]\n",
      "  [0 1 0 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0]\n",
      "  [0 1 0 0 0 0]\n",
      "  [0 0 1 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 0 1 0]\n",
      "  [0 0 1 0 0 0]\n",
      "  [0 0 0 0 1 0]\n",
      "  ...\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0]\n",
      "  [0 0 0 0 1 0]\n",
      "  [0 0 0 1 0 0]\n",
      "  ...\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0]\n",
      "  [0 0 0 1 0 0]\n",
      "  [0 0 0 0 1 0]\n",
      "  ...\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]\n",
      "  [1 0 0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 15, 6)\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      " - 1373s - loss: 0.4000 - acc: 0.8836 - val_loss: 0.3295 - val_acc: 0.9153\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/2\n",
      " - 1329s - loss: 0.4021 - acc: 0.8823 - val_loss: 0.3321 - val_acc: 0.9158\n",
      "Epoch 2/2\n",
      " - 1295s - loss: 0.3443 - acc: 0.9087 - val_loss: 0.3146 - val_acc: 0.9217\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      " - 1346s - loss: 0.4234 - acc: 0.8699 - val_loss: 0.3417 - val_acc: 0.9115\n",
      "Epoch 2/10\n",
      " - 1286s - loss: 0.3480 - acc: 0.9059 - val_loss: 0.3119 - val_acc: 0.9220\n",
      "Epoch 3/10\n",
      " - 1311s - loss: 0.3279 - acc: 0.9127 - val_loss: 0.3005 - val_acc: 0.9247\n",
      "Epoch 4/10\n",
      " - 1260s - loss: 0.3213 - acc: 0.9146 - val_loss: 0.2959 - val_acc: 0.9265\n",
      "Epoch 5/10\n",
      " - 1254s - loss: 0.3176 - acc: 0.9157 - val_loss: 0.2935 - val_acc: 0.9276\n",
      "Epoch 6/10\n",
      " - 1251s - loss: 0.3154 - acc: 0.9165 - val_loss: 0.2929 - val_acc: 0.9279\n",
      "Epoch 7/10\n",
      " - 1285s - loss: 0.3136 - acc: 0.9170 - val_loss: 0.2888 - val_acc: 0.9287\n",
      "Epoch 8/10\n",
      " - 1377s - loss: 0.3119 - acc: 0.9176 - val_loss: 0.2887 - val_acc: 0.9289\n",
      "Epoch 9/10\n",
      " - 1231s - loss: 0.3105 - acc: 0.9179 - val_loss: 0.2874 - val_acc: 0.9294\n",
      "Epoch 10/10\n",
      " - 1220s - loss: 0.3091 - acc: 0.9184 - val_loss: 0.2844 - val_acc: 0.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_3/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_4/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_5/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_6/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_4:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_7:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_8:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_11:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_12:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 0.2844229333251715]]\n",
      "0 0.3416686093658209\n",
      "1 0.31185708378255367\n",
      "2 0.30046474426984787\n",
      "3 0.29591935010254383\n",
      "4 0.2934861443191767\n",
      "5 0.2928774592131376\n",
      "6 0.28882020369172096\n",
      "7 0.28866321060061456\n",
      "8 0.28736401227116587\n",
      "9 0.2844229333251715\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      " - 1218s - loss: 0.3228 - acc: 0.9105 - val_loss: 0.2715 - val_acc: 0.9321\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/2\n",
      " - 1244s - loss: 0.3305 - acc: 0.9093 - val_loss: 0.2714 - val_acc: 0.9321\n",
      "Epoch 2/2\n",
      " - 1198s - loss: 0.2913 - acc: 0.9222 - val_loss: 0.2689 - val_acc: 0.9326\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      " - 1204s - loss: 0.3270 - acc: 0.9109 - val_loss: 0.2715 - val_acc: 0.9322\n",
      "Epoch 2/10\n",
      " - 1366s - loss: 0.2911 - acc: 0.9225 - val_loss: 0.2692 - val_acc: 0.9326\n",
      "Epoch 3/10\n",
      " - 1214s - loss: 0.2895 - acc: 0.9228 - val_loss: 0.2684 - val_acc: 0.9327\n",
      "Epoch 4/10\n",
      " - 1247s - loss: 0.2888 - acc: 0.9230 - val_loss: 0.2676 - val_acc: 0.9328\n",
      "Epoch 5/10\n",
      " - 1287s - loss: 0.2883 - acc: 0.9231 - val_loss: 0.2676 - val_acc: 0.9329\n",
      "Epoch 6/10\n",
      " - 1298s - loss: 0.2880 - acc: 0.9233 - val_loss: 0.2675 - val_acc: 0.9330\n",
      "Epoch 7/10\n",
      " - 1281s - loss: 0.2878 - acc: 0.9233 - val_loss: 0.2675 - val_acc: 0.9330\n",
      "Epoch 8/10\n",
      " - 1274s - loss: 0.2876 - acc: 0.9234 - val_loss: 0.2669 - val_acc: 0.9331\n",
      "Epoch 9/10\n",
      " - 1422s - loss: 0.2874 - acc: 0.9234 - val_loss: 0.2667 - val_acc: 0.9331\n",
      "Epoch 10/10\n",
      " - 1284s - loss: 0.2873 - acc: 0.9235 - val_loss: 0.2668 - val_acc: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_7/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_8/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_9/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_10/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_12 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_11/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_12/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_15:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_16:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_19:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_20:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_12 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_23:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_24:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 0.2844229333251715], [8, 0.26674772495031357]]\n",
      "0 0.27153913788497447\n",
      "1 0.26915460416674614\n",
      "2 0.2683993786126375\n",
      "3 0.2675954289287329\n",
      "4 0.2675661608725786\n",
      "5 0.2675082699209452\n",
      "6 0.2675195243358612\n",
      "7 0.26687554074823855\n",
      "8 0.26674772495031357\n",
      "9 0.2667837294638157\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      " - 1323s - loss: 0.3017 - acc: 0.9182 - val_loss: 0.2673 - val_acc: 0.9333\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/2\n",
      " - 1434s - loss: 0.3024 - acc: 0.9179 - val_loss: 0.2663 - val_acc: 0.9333\n",
      "Epoch 2/2\n",
      " - 1393s - loss: 0.2861 - acc: 0.9238 - val_loss: 0.2661 - val_acc: 0.9336\n",
      "Train on 9950000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      " - 1598s - loss: 0.3027 - acc: 0.9180 - val_loss: 0.2680 - val_acc: 0.9330\n",
      "Epoch 2/10\n",
      " - 1571s - loss: 0.2866 - acc: 0.9237 - val_loss: 0.2663 - val_acc: 0.9335\n",
      "Epoch 3/10\n",
      " - 1426s - loss: 0.2856 - acc: 0.9240 - val_loss: 0.2656 - val_acc: 0.9337\n",
      "Epoch 4/10\n",
      " - 1397s - loss: 0.2851 - acc: 0.9242 - val_loss: 0.2652 - val_acc: 0.9338\n",
      "Epoch 5/10\n",
      " - 1293s - loss: 0.2848 - acc: 0.9243 - val_loss: 0.2657 - val_acc: 0.9336\n",
      "Epoch 6/10\n",
      " - 1303s - loss: 0.2846 - acc: 0.9243 - val_loss: 0.2648 - val_acc: 0.9337\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9e4d434ddeb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9e4d434ddeb9>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(latent, half, train_size, X_train, y_train, y_train1)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mhist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mhist2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mhist3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;31m#hist4 ,model4, encoder_model4, decoder_model4 = modelFit(30, 100, latent, half, X_train, y_train, y_train1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m#hist5 ,model5, encoder_model5, decoder_model5 = modelFit(50, 100, latent, half, X_train, y_train, y_train1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9e4d434ddeb9>\u001b[0m in \u001b[0;36mmodelFit\u001b[0;34m(epoch, batchSize, latent_dim, half, X_train, y_train, y_train1)\u001b[0m\n\u001b[1;32m    152\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m           \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m          )\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "def decoder(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    for i in range(size):\n",
    "        if array[i].tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "            result=result+\"0\" \n",
    "        elif array[i].tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "            result=result+\"A\"\n",
    "        elif array[i].tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "            result=result+\"C\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "            result=result+\"G\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "            result=result+\"T\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "            result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "#model5 = load_model('model/seq2seq_nogap_camFam3_1mut.h5')\n",
    "def decoderY(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    \n",
    "    if array.tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "        result=result+\"0\" \n",
    "    elif array.tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "        result=result+\"A\"\n",
    "    elif array.tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "        result=result+\"C\"\n",
    "    elif array.tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "        result=result+\"G\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "        result=result+\"T\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "        result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def printHitMiss(a,b):\n",
    "    if a==b:\n",
    "        return 'Hit'\n",
    "    else:\n",
    "        return 'Miss'\n",
    "    \n",
    "print(X_test.shape)\n",
    "def accuracy(a, b):\n",
    "    count = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == b[i]:\n",
    "            count = count+1\n",
    "    return count/len(a)\n",
    "\n",
    "def accuracy2(a, b, c):\n",
    "    count = 0\n",
    "    count2 =0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != c[i]:\n",
    "            count2 = count2 +1\n",
    "        if a[i] != c[i] and b[i]==c[i]:\n",
    "            count = count+1\n",
    "    return count/count2\n",
    "\n",
    "def isMutation(a, b):\n",
    "    if a!= b:\n",
    "        print(\"mutation\")\n",
    "\n",
    "def predict(model):\n",
    "    x_true=[]\n",
    "    y_hat_list = []\n",
    "    y_true = []\n",
    "    predictions = model.predict(X_test, batch_size=250, verbose=0)\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        #print(prediction)\n",
    "        # print(prediction)\n",
    "        #x_index = np.argmax(testX[i], axis=1)\n",
    "        #print(prediction[i])\n",
    "        x_str = decoder(X_test[i])\n",
    "\n",
    "        #index = np.argmax(prediction)\n",
    "        index = np.random.choice(6, 3, p=prediction)\n",
    "        result = [nucleotide[index]]\n",
    "\n",
    "        print(''.join(x_str), ' -> ', ''.join(result),\n",
    "              \" true: \", ''.join(decoderY(y_test[i])), printHitMiss(''.join(result), ''.join(decoderY(y_test[i]))))\n",
    "        x_true.append(''.join(x_str[5]))\n",
    "        y_hat_list.append(''.join(result))\n",
    "        y_true.append(''.join(decoderY(y_test[i])))\n",
    "    sm=difflib.SequenceMatcher(None,y_hat_list,y_true)\n",
    "    sm2=difflib.SequenceMatcher(None,y_hat_list,x_true)\n",
    "    print()\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat_list, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat_list, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat_list, y_true)))\n",
    "    return x_true, y_hat_list, y_true\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def lstm_model(latent_dim, half):\n",
    "    encoder_inputs = Input(shape=(None, 6))\n",
    "    \n",
    "    encoder = Bidirectional(CuDNNLSTM(half, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = concatenate([forward_h, backward_h])\n",
    "    state_c = concatenate([forward_c, backward_c])\n",
    "    \n",
    "    \n",
    "    # only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder\n",
    "    decoder_inputs = Input(shape=(None, 6))\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(6, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Run training\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def modelFit(epoch, batchSize, latent_dim, half, X_train, y_train, y_train1):\n",
    "    model1, encoder_model1, decoder_model1 = lstm_model(latent_dim, half)\n",
    "    hist1 = model1.fit([X_train, y_train1], y_train,\n",
    "          batch_size=batchSize,\n",
    "          epochs=epoch,\n",
    "          validation_data=([X_val,y_val1], y_val),\n",
    "          verbose = 2\n",
    "         )\n",
    "    return hist1, model1, encoder_model1, decoder_model1\n",
    "\n",
    "def grid_search(latent, half,train_size, X_train, y_train, y_train1):\n",
    "    hist1, model1, encoder_model1, decoder_model1 = modelFit(1, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist2 ,model2, encoder_model2, decoder_model2 = modelFit(2, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist3 ,model3, encoder_model3, decoder_model3 = modelFit(10, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist4 ,model4, encoder_model4, decoder_model4 = modelFit(30, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist5 ,model5, encoder_model5, decoder_model5 = modelFit(50, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist6 ,model6, encoder_model6, decoder_model6 = modelFit(80, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist7 ,model7, encoder_model7, decoder_model7 = modelFit(100, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist8 ,model8, encoder_model8, decoder_model8 = modelFit(500, 100, latent, half)\n",
    "\n",
    "    model1.save(\"models/gap_pad_v2_{}_{}_1.h5\".format(train_size,half))\n",
    "    model2.save(\"models/gap_pad_v2_{}_{}_2.h5\".format(train_size,half))\n",
    "    model3.save(\"models/gap_pad_v2_{}_{}_10.h5\".format(train_size,half))\n",
    "    #model4.save(\"models/_gap_pad_v2_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #model5.save(\"models/_gap_pad_v2_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #model6.save(\"models/_gap_pad_v2_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #model7.save(\"models/_gap_pad_v2_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #model8.save(\"_gap_pad_v2_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    encoder_model1.save(\"models/Egap_pad_v2_{}_{}_1.h5\".format(train_size,half))\n",
    "    encoder_model2.save(\"models/Egap_pad_v2_{}_{}_2.h5\".format(train_size,half))\n",
    "    encoder_model3.save(\"models/Egap_pad_v2_{}_{}_10.h5\".format(train_size,half))\n",
    "    #encoder_model4.save(\"models/E_gap_pad_v2_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #encoder_model5.save(\"models/E_gap_pad_v2_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #encoder_model6.save(\"models/E_gap_pad_v2_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #encoder_model7.save(\"models/E_gap_pad_v2_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #encoder_model8.save(\"E_gap_pad_v2_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    decoder_model1.save(\"models/Dgap_pad_v2_{}_{}_1.h5\".format(train_size,half))\n",
    "    decoder_model2.save(\"models/Dgap_pad_v2_{}_{}_2.h5\".format(train_size,half))\n",
    "    decoder_model3.save(\"models/Dgap_pad_v2_{}_{}_10.h5\".format(train_size,half))\n",
    "    #decoder_model4.save(\"models/D_gap_pad_v2_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #decoder_model5.save(\"models/D_gap_pad_v2_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #decoder_model6.save(\"models/D_gap_pad_v2_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #decoder_model7.save(\"models/D_gap_pad_v2_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #decoder_model8.save(\"D_gap_pad_v2_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    count = [i for i in range(len(hist3.history['val_loss']))]\n",
    "    val_loss_hist.append([hist3.history['val_loss'].index(min(hist3.history['val_loss'])),min(hist3.history['val_loss'])])\n",
    "    print(val_loss_hist)\n",
    "    for i, value in zip(count, hist3.history['val_loss']):\n",
    "        print(i, value)\n",
    "\n",
    "grid_search(16, 8, 000, X_train, y_train, y_train1)        \n",
    "grid_search(32, 16, 000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 000, X_train, y_train, y_train1)\n",
    "#grid_search(8192, 4096, 000, X_train, y_train, y_train1)\n",
    "\n",
    "with open('loss_hist.txt', 'wb') as fp:\n",
    "    pickle.dump(val_loss_hist, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
