{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, TimeDistributed, Dense, RepeatVector, CuDNNLSTM, GRU, Bidirectional, Input, CuDNNGRU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "import difflib\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from keras_transformer import get_model, decode\n",
    "\n",
    "train_index = 80000\n",
    "valid_index = 90000\n",
    "test_index = 100000\n",
    "# X_train=np.load('prepData/X_train_camFam3_v3_chr2_size11.npy')\n",
    "# X_val=np.load('prepData/X_val_camFam3_v3_chr2_size11.npy')\n",
    "# X_test=np.load('prepData/X_test_camFam3_v3_chr2_size11.npy')\n",
    "# y_train=np.load('prepData/y_train_camFam3_v3_chr2_size11.npy')\n",
    "# y_val=np.load('prepData/y_val_camFam3_v3_chr2_size11.npy')\n",
    "# y_test=np.load('prepData/y_test_camFam3_v3_chr2_size11.npy')\n",
    "\n",
    "# y_train1 = np.load('prepData/y_train1_camFam3_v3_chr2_size11.npy')\n",
    "# y_val1 = np.load('prepData/y_val1_camFam3_v3_chr2_size11.npy')\n",
    "# y_test1 = np.load('prepData/y_test1_camFam3_v3_chr2_size11.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242976\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "with open ('ancestral', 'rb') as fp:\n",
    "    source_tokens = pickle.load(fp)\n",
    "with open ('descendant', 'rb') as f:\n",
    "    target_tokens = pickle.load(f)\n",
    "\n",
    "print(len(source_tokens))\n",
    "source_tokens = source_tokens[:100000]\n",
    "target_tokens = target_tokens[:100000]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionaries\n",
    "def build_token_dict(token_list):\n",
    "    token_dict = {\n",
    "        '<PAD>': 0,\n",
    "        '<START>': 1,\n",
    "        '<END>': 2,\n",
    "    }\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = len(token_dict)\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'G', 'A', 'C', 'A', 'G', '-', 'A', 'A', 'A', 'C']\n",
      "['A', 'G', 'A', 'G', 'A', 'G', '-', 'A', 'A', 'A', 'C']\n"
     ]
    }
   ],
   "source": [
    "print(source_tokens[1])\n",
    "print(target_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_token_dict = build_token_dict(source_tokens)\n",
    "target_token_dict = build_token_dict(target_tokens)\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "\n",
    "# Add special tokens\n",
    "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens]\n",
    "\n",
    "# Padding\n",
    "source_max_len = max(map(len, encode_tokens))\n",
    "target_max_len = max(map(len, decode_tokens))\n",
    "\n",
    "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens]\n",
    "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
    "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
    "\n",
    "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]\n",
    "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
    "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 32), ( 384         Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 32)     0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     4224        Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 32)     64          Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 32)     8352        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 32)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 32)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 32)     64          Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     4224        Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 32)     0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 32)     64          Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     4224        Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 32)     8352        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 32)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 32)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 32)     64          Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 32)     64          Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     4224        Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 32)     64          Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 32)     8352        Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 32)     0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 32)     0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 32)     64          Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     4224        Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 32)     64          Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     4224        Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 32)     64          Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 32)     8352        Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 32)     0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 32)     0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 32)     64          Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 12)     12          Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 59,788\n",
      "Trainable params: 59,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "80000/80000 [==============================] - 12s 151us/step - loss: 1.7919 - val_loss: 1.3133\n",
      "Epoch 2/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 1.1177 - val_loss: 1.0231\n",
      "Epoch 3/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.8966 - val_loss: 0.7142\n",
      "Epoch 4/200\n",
      "80000/80000 [==============================] - 5s 58us/step - loss: 0.4759 - val_loss: 0.4805\n",
      "Epoch 5/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.3785 - val_loss: 0.4653\n",
      "Epoch 6/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.3653 - val_loss: 0.4618\n",
      "Epoch 7/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3598 - val_loss: 0.4584\n",
      "Epoch 8/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3568 - val_loss: 0.4569\n",
      "Epoch 9/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3546 - val_loss: 0.4552\n",
      "Epoch 10/200\n",
      "80000/80000 [==============================] - 4s 46us/step - loss: 0.3533 - val_loss: 0.4545\n",
      "Epoch 11/200\n",
      "80000/80000 [==============================] - 4s 47us/step - loss: 0.3517 - val_loss: 0.4531\n",
      "Epoch 12/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3509 - val_loss: 0.4515\n",
      "Epoch 13/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3501 - val_loss: 0.4502\n",
      "Epoch 14/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3492 - val_loss: 0.4541\n",
      "Epoch 15/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3487 - val_loss: 0.4509\n",
      "Epoch 16/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3479 - val_loss: 0.4534\n",
      "Epoch 17/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3473 - val_loss: 0.4516\n",
      "Epoch 18/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3471 - val_loss: 0.4446\n",
      "Epoch 19/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3463 - val_loss: 0.4499\n",
      "Epoch 20/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3460 - val_loss: 0.4514\n",
      "Epoch 21/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3453 - val_loss: 0.4508\n",
      "Epoch 22/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3452 - val_loss: 0.4479\n",
      "Epoch 23/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.3447 - val_loss: 0.4510\n",
      "Epoch 24/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3441 - val_loss: 0.4477\n",
      "Epoch 25/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3439 - val_loss: 0.4490\n",
      "Epoch 26/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3437 - val_loss: 0.4490\n",
      "Epoch 27/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3432 - val_loss: 0.4480\n",
      "Epoch 28/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3429 - val_loss: 0.4454\n",
      "Epoch 29/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3425 - val_loss: 0.4454\n",
      "Epoch 30/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3422 - val_loss: 0.4477\n",
      "Epoch 31/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3421 - val_loss: 0.4460\n",
      "Epoch 32/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3416 - val_loss: 0.4500\n",
      "Epoch 33/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3415 - val_loss: 0.4435\n",
      "Epoch 34/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3411 - val_loss: 0.4442\n",
      "Epoch 35/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.3410 - val_loss: 0.4482\n",
      "Epoch 36/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3407 - val_loss: 0.4480\n",
      "Epoch 37/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3405 - val_loss: 0.4464\n",
      "Epoch 38/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3401 - val_loss: 0.4462\n",
      "Epoch 39/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.3398 - val_loss: 0.4455\n",
      "Epoch 40/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3396 - val_loss: 0.4490\n",
      "Epoch 41/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3391 - val_loss: 0.4479\n",
      "Epoch 42/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.3389 - val_loss: 0.4422\n",
      "Epoch 43/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3384 - val_loss: 0.4469\n",
      "Epoch 44/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3380 - val_loss: 0.4460\n",
      "Epoch 45/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3380 - val_loss: 0.4462\n",
      "Epoch 46/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3375 - val_loss: 0.4477\n",
      "Epoch 47/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3373 - val_loss: 0.4485\n",
      "Epoch 48/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3368 - val_loss: 0.4506\n",
      "Epoch 49/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3368 - val_loss: 0.4465\n",
      "Epoch 50/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3363 - val_loss: 0.4461\n",
      "Epoch 51/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3361 - val_loss: 0.4468\n",
      "Epoch 52/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3357 - val_loss: 0.4461\n",
      "Epoch 53/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3354 - val_loss: 0.4484\n",
      "Epoch 54/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3352 - val_loss: 0.4511\n",
      "Epoch 55/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3346 - val_loss: 0.4497\n",
      "Epoch 56/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3344 - val_loss: 0.4477\n",
      "Epoch 57/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3341 - val_loss: 0.4488\n",
      "Epoch 58/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3339 - val_loss: 0.4467\n",
      "Epoch 59/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3335 - val_loss: 0.4475\n",
      "Epoch 60/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3333 - val_loss: 0.4505\n",
      "Epoch 61/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3326 - val_loss: 0.4493\n",
      "Epoch 62/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.3323 - val_loss: 0.4541\n",
      "Epoch 63/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3321 - val_loss: 0.4553\n",
      "Epoch 64/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3319 - val_loss: 0.4487\n",
      "Epoch 65/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3313 - val_loss: 0.4530\n",
      "Epoch 66/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3312 - val_loss: 0.4512\n",
      "Epoch 67/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3307 - val_loss: 0.4529\n",
      "Epoch 68/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3302 - val_loss: 0.4496\n",
      "Epoch 69/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3301 - val_loss: 0.4493\n",
      "Epoch 70/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3298 - val_loss: 0.4540\n",
      "Epoch 71/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3295 - val_loss: 0.4558\n",
      "Epoch 72/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3290 - val_loss: 0.4537\n",
      "Epoch 73/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3285 - val_loss: 0.4516\n",
      "Epoch 74/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3283 - val_loss: 0.4558\n",
      "Epoch 75/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3281 - val_loss: 0.4524\n",
      "Epoch 76/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.3277 - val_loss: 0.4534\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3272 - val_loss: 0.4579\n",
      "Epoch 78/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3268 - val_loss: 0.4553\n",
      "Epoch 79/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3265 - val_loss: 0.4580\n",
      "Epoch 80/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3261 - val_loss: 0.4547\n",
      "Epoch 81/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3260 - val_loss: 0.4561\n",
      "Epoch 82/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.3254 - val_loss: 0.4578\n",
      "Epoch 83/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3252 - val_loss: 0.4545\n",
      "Epoch 84/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3247 - val_loss: 0.4574\n",
      "Epoch 85/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3245 - val_loss: 0.4579\n",
      "Epoch 86/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3239 - val_loss: 0.4617\n",
      "Epoch 87/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3237 - val_loss: 0.4623\n",
      "Epoch 88/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3233 - val_loss: 0.4606\n",
      "Epoch 89/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3229 - val_loss: 0.4653\n",
      "Epoch 90/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.3226 - val_loss: 0.4598\n",
      "Epoch 91/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.3222 - val_loss: 0.4627\n",
      "Epoch 92/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3219 - val_loss: 0.4635\n",
      "Epoch 93/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3213 - val_loss: 0.4645\n",
      "Epoch 94/200\n",
      "80000/80000 [==============================] - 5s 57us/step - loss: 0.3211 - val_loss: 0.4615\n",
      "Epoch 95/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3209 - val_loss: 0.4633\n",
      "Epoch 96/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3205 - val_loss: 0.4645\n",
      "Epoch 97/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3203 - val_loss: 0.4621\n",
      "Epoch 98/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3196 - val_loss: 0.4649\n",
      "Epoch 99/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3192 - val_loss: 0.4662\n",
      "Epoch 100/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3188 - val_loss: 0.4678\n",
      "Epoch 101/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3185 - val_loss: 0.4665\n",
      "Epoch 102/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3178 - val_loss: 0.4695\n",
      "Epoch 103/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3179 - val_loss: 0.4635\n",
      "Epoch 104/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3176 - val_loss: 0.4652\n",
      "Epoch 105/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3175 - val_loss: 0.4673\n",
      "Epoch 106/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3170 - val_loss: 0.4707\n",
      "Epoch 107/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3163 - val_loss: 0.4703\n",
      "Epoch 108/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3160 - val_loss: 0.4689\n",
      "Epoch 109/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3157 - val_loss: 0.4721\n",
      "Epoch 110/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3154 - val_loss: 0.4700\n",
      "Epoch 111/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.3153 - val_loss: 0.4699\n",
      "Epoch 112/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3148 - val_loss: 0.4688\n",
      "Epoch 113/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3144 - val_loss: 0.4737\n",
      "Epoch 114/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3142 - val_loss: 0.4688\n",
      "Epoch 115/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3137 - val_loss: 0.4699\n",
      "Epoch 116/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.3135 - val_loss: 0.4734\n",
      "Epoch 117/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.3133 - val_loss: 0.4687\n",
      "Epoch 118/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3126 - val_loss: 0.4773\n",
      "Epoch 119/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3126 - val_loss: 0.4734\n",
      "Epoch 120/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3123 - val_loss: 0.4717\n",
      "Epoch 121/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3117 - val_loss: 0.4787\n",
      "Epoch 122/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3117 - val_loss: 0.4792\n",
      "Epoch 123/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3109 - val_loss: 0.4811\n",
      "Epoch 124/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.3108 - val_loss: 0.4770\n",
      "Epoch 125/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.3102 - val_loss: 0.4791\n",
      "Epoch 126/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.3099 - val_loss: 0.4807\n",
      "Epoch 127/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.3100 - val_loss: 0.4779\n",
      "Epoch 128/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.3081 - val_loss: 0.4670\n",
      "Epoch 129/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.2929 - val_loss: 0.4622\n",
      "Epoch 130/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2905 - val_loss: 0.4659\n",
      "Epoch 131/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2893 - val_loss: 0.4633\n",
      "Epoch 132/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2884 - val_loss: 0.4599\n",
      "Epoch 133/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2873 - val_loss: 0.4641\n",
      "Epoch 134/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2871 - val_loss: 0.4621\n",
      "Epoch 135/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2866 - val_loss: 0.4620\n",
      "Epoch 136/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.2858 - val_loss: 0.4645\n",
      "Epoch 137/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.2853 - val_loss: 0.4644\n",
      "Epoch 138/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2852 - val_loss: 0.4622\n",
      "Epoch 139/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2848 - val_loss: 0.4651\n",
      "Epoch 140/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.2849 - val_loss: 0.4634\n",
      "Epoch 141/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2844 - val_loss: 0.4628\n",
      "Epoch 142/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.2836 - val_loss: 0.4701\n",
      "Epoch 143/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.2831 - val_loss: 0.4716\n",
      "Epoch 144/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.2829 - val_loss: 0.4657\n",
      "Epoch 145/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.2823 - val_loss: 0.4680\n",
      "Epoch 146/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2794 - val_loss: 0.4598\n",
      "Epoch 147/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.2777 - val_loss: 0.4561\n",
      "Epoch 148/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2771 - val_loss: 0.4563\n",
      "Epoch 149/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.2759 - val_loss: 0.4602\n",
      "Epoch 150/200\n",
      "80000/80000 [==============================] - 4s 55us/step - loss: 0.2748 - val_loss: 0.4536\n",
      "Epoch 151/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.2744 - val_loss: 0.4512\n",
      "Epoch 152/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.2717 - val_loss: 0.4506\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2701 - val_loss: 0.4469\n",
      "Epoch 154/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2688 - val_loss: 0.4513\n",
      "Epoch 155/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2681 - val_loss: 0.4492\n",
      "Epoch 156/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2680 - val_loss: 0.4479\n",
      "Epoch 157/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2672 - val_loss: 0.4524\n",
      "Epoch 158/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2667 - val_loss: 0.4515\n",
      "Epoch 159/200\n",
      "80000/80000 [==============================] - 5s 58us/step - loss: 0.2663 - val_loss: 0.4480\n",
      "Epoch 160/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2657 - val_loss: 0.4511\n",
      "Epoch 161/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.2652 - val_loss: 0.4437\n",
      "Epoch 162/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2652 - val_loss: 0.4486\n",
      "Epoch 163/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2640 - val_loss: 0.4470\n",
      "Epoch 164/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2636 - val_loss: 0.4457\n",
      "Epoch 165/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2625 - val_loss: 0.4418\n",
      "Epoch 166/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2618 - val_loss: 0.4444\n",
      "Epoch 167/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2605 - val_loss: 0.4365\n",
      "Epoch 168/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.2594 - val_loss: 0.4396\n",
      "Epoch 169/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.2592 - val_loss: 0.4350\n",
      "Epoch 170/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.2581 - val_loss: 0.4369\n",
      "Epoch 171/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2578 - val_loss: 0.4371\n",
      "Epoch 172/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2577 - val_loss: 0.4395\n",
      "Epoch 173/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2568 - val_loss: 0.4326\n",
      "Epoch 174/200\n",
      "80000/80000 [==============================] - 4s 56us/step - loss: 0.2560 - val_loss: 0.4378\n",
      "Epoch 175/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.2554 - val_loss: 0.4331\n",
      "Epoch 176/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2546 - val_loss: 0.4353\n",
      "Epoch 177/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2544 - val_loss: 0.4343\n",
      "Epoch 178/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2540 - val_loss: 0.4355\n",
      "Epoch 179/200\n",
      "80000/80000 [==============================] - 5s 56us/step - loss: 0.2543 - val_loss: 0.4315\n",
      "Epoch 180/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2527 - val_loss: 0.4353\n",
      "Epoch 181/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2524 - val_loss: 0.4333\n",
      "Epoch 182/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2524 - val_loss: 0.4358\n",
      "Epoch 183/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2516 - val_loss: 0.4314\n",
      "Epoch 184/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2512 - val_loss: 0.4308\n",
      "Epoch 185/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2666 - val_loss: 0.4341\n",
      "Epoch 186/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2569 - val_loss: 0.4334\n",
      "Epoch 187/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.2535 - val_loss: 0.4323\n",
      "Epoch 188/200\n",
      "80000/80000 [==============================] - 4s 51us/step - loss: 0.2518 - val_loss: 0.4339\n",
      "Epoch 189/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2494 - val_loss: 0.4253\n",
      "Epoch 190/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2475 - val_loss: 0.4243\n",
      "Epoch 191/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2463 - val_loss: 0.4164\n",
      "Epoch 192/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2441 - val_loss: 0.4109\n",
      "Epoch 193/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2378 - val_loss: 0.3958\n",
      "Epoch 194/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.2344 - val_loss: 0.3948\n",
      "Epoch 195/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.2321 - val_loss: 0.3907\n",
      "Epoch 196/200\n",
      "80000/80000 [==============================] - 4s 49us/step - loss: 0.2298 - val_loss: 0.3828\n",
      "Epoch 197/200\n",
      "80000/80000 [==============================] - 4s 50us/step - loss: 0.2269 - val_loss: 0.3697\n",
      "Epoch 198/200\n",
      "80000/80000 [==============================] - 4s 52us/step - loss: 0.2216 - val_loss: 0.3517\n",
      "Epoch 199/200\n",
      "80000/80000 [==============================] - 4s 53us/step - loss: 0.2148 - val_loss: 0.3403\n",
      "Epoch 200/200\n",
      "80000/80000 [==============================] - 4s 54us/step - loss: 0.2086 - val_loss: 0.3302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25e80c8eb8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build & fit model\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=32,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=128,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=True,  # Use different embeddings for different languages\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    x=[np.array(encode_input[:train_index]), np.array(decode_input[:train_index])],\n",
    "    y=np.array(decode_output[:train_index]),\n",
    "    epochs=200,\n",
    "    batch_size=1000,\n",
    "    validation_data=([np.array(encode_input[train_index:valid_index]), np.array(decode_input[train_index:valid_index])], np.array(decode_output[train_index:valid_index]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "def inference(test):\n",
    "    result = []\n",
    "    for i in range(len(test)):\n",
    "        output = decode(\n",
    "            model,\n",
    "            test[i],\n",
    "            start_token=target_token_dict['<START>'],\n",
    "            end_token=target_token_dict['<END>'],\n",
    "            pad_token=target_token_dict['<PAD>'],\n",
    "        )\n",
    "        #print(output)\n",
    "        result.append(''.join(map(lambda x: target_token_dict_inv[x], output[1:-1])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TA--------- -> TAAAAAAAAAA CA---------\n",
      "A---------- -> AAAAAAAAAAA A----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------T -> AAAAAAAAAGT ----------C\n",
      "---------T- -> AAAAAAAAGTT ---------C-\n",
      "--------T-- -> TTTTTTTTTGA --------C--\n",
      "-------T--- -> TTTTTTTTGAT -------C---\n",
      "------T---A -> AATAATT---A ------C---A\n",
      "-----T---A- -> AATAGT---AT -----C---A-\n",
      "----T---A-- -> AATAT---ATT ----C---A--\n",
      "---T---A--- -> ATATGGTAAAA ---C---A---\n",
      "--T---A---A -> ATT---AAATA --C---A---G\n",
      "-T---A---AT -> AT---AATTAT -C---A---GC\n",
      "T---A---ATT -> TGTTAATGATT C---A---GCT\n",
      "---A---ATTA -> ATTAATGATTA ---A---GCTA\n",
      "--A---ATTA- -> ATAATGATTAA --A---GCTAT\n",
      "-A---ATTA-- -> AA---ATTATA -A---GCTATT\n",
      "A---ATTA--C -> AACGATTATGT A---GCTATTT\n",
      "---ATTA--C- -> TTTATTAATTA ---GCTATTT-\n",
      "--ATTA--C-T -> TTATTATGA-T --GCTATTT-T\n",
      "-ATTA--C-TT -> AATTAATTGTT -GCTATTT-TT\n",
      "ATTA--C-TTT -> ATTATACATTT GCTATTT-TTT\n",
      "TTA--C-TTT- -> TTATACATTTT CTATTT-TTT-\n",
      "TA--C-TTT-T -> TAATTGTTTTT TATTT-TTT-T\n",
      "A--C-TTT-TT -> AAACATTTATT ATTT-TTT-TT\n",
      "--C-TTT-TTT -> AACATTTTTTT TTT-TTT-TTT\n",
      "-C-TTT-TTT- -> AC-TTTTTTTT TT-TTT-TTT-\n",
      "C-TTT-TTT-- -> T-TTTTTTTAT T-TTT-TTT--\n",
      "-TTT-TTT--- -> TTTTTTTTATA -TTT-TTT---\n",
      "TTT-TTT---- -> TTTTTTTCATA TTT-TTT----\n",
      "TT-TTT----C -> TT-TTTTAGAC TT-TTT----C\n",
      "T-TTT----CT -> T-TTTTAGTAT T-TTT----CT\n",
      "-TTT----CTA -> ATTTTTGACTA -TTT----CTA\n",
      "TTT----CTAT -> TTTGATGCTAT TTT----CTAT\n",
      "TT----CTATC -> TTGAGACTATC TT----CTATC\n",
      "T----CTATC- -> TGATACTATCT T----CTATC-\n",
      "----CTATC-- -> TAAGCTATCTT ----CTATC--\n",
      "---CTATC--- -> TATCTATCTAT ---CTATC---\n",
      "--CTATC---- -> ATCTATCAATA --CTATC----\n",
      "-CTATC----- -> ACTATCAATAA -CTATC-----\n",
      "CTATC------ -> TTATCAATATA CTATC------\n",
      "TATC------A -> TATCAAAGATA TATC------A\n",
      "ATC------AT -> ATCAAGATTAT ATC------AT\n",
      "TC------ATA -> TCAGATGTATA TC------ATG\n",
      "C------ATAC -> TGTATTTATAC C------ATGC\n",
      "------ATACA -> AAAAATATACA ------ATGCA\n",
      "-----ATACAG -> AAAAGATACAA -----ATGCAG\n",
      "----ATACAG- -> AAATATACAAA ----ATGCAG-\n",
      "---ATACAG-A -> AATATACAAAA ---ATGCAG-A\n",
      "--ATACAG-A- -> ATATACAGAAA --ATGCAG-A-\n",
      "-ATACAG-A-- -> AATACAGAAAA -ATGCAG-A--\n",
      "ATACAG-A--- -> ATACAGAAGAA ATGCAG-A---\n",
      "TACAG-A---- -> TACAGAggggc TGCAG-A----\n",
      "ACAG-A----- -> ACAGAAGATAA GCAG-A-----\n",
      "CAG-A------ -> TAGAAAAATAT CAG-A------\n",
      "AG-A------- -> AGAAAAATATA AG-A-------\n",
      "G-A-------- -> ATAAAAAATAT G-A--------\n",
      "-A--------- -> AAAAAAAATAA -A---------\n",
      "A---------- -> AAAAAAAAAAA A----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------- -> AAAAAAAAAAA -----------\n",
      "----------C -> TTTTTTTTTTT ----------C\n",
      "---------CA -> TTTTTTTTTCA ---------CA\n",
      "--------CAT -> TTTTTTTTCAT --------CAT\n",
      "-------CATG -> TTTTTTTCATG -------CATG\n",
      "------CATGT -> TTTTTTCATGT ------CATGT\n",
      "-----CATGTT -> AAAAGCATGTT -----CATGTT\n",
      "----CATGTTC -> AAAGCATGTTC ----CATGTTC\n",
      "---CATGTTC- -> AATCATGTTCT ---CATGTTC-\n",
      "--CATGTTC-- -> ATCATGTTCTT --CATGTTC--\n",
      "-CATGTTC--T -> ACATGTTCTTT -CATGTTC--T\n",
      "CATGTTC--TG -> TATGTTCTGTG CATGTTC--TG\n",
      "ATGTTC--TGC -> ATGTTCTGTGC ATGTTC--TGC\n",
      "TGTTC--TGCA -> TGTTCTGTGCA TGTTC--TGCA\n",
      "GTTC--TGCAT -> ATTCTGTGCAT GTTC--TGCAT\n",
      "TTC--TGCATA -> TTCTGTGCATA TTC--TGCATA\n",
      "TC--TGCATAT -> TCTGTGCATAT TC--TGCATAT\n",
      "C--TGCATATT -> T--TGCATATT C--TGCATATT\n"
     ]
    }
   ],
   "source": [
    "decoded = inference(encode_input[90000:91000])\n",
    "for i in range(100):\n",
    "    print(''.join(source_tokens[90000+i]), '->', decoded[i], ''.join(target_tokens[90000+i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
