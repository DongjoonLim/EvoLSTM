{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 24 21:04:53 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   24C    P8     8W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 31%   27C    P8    23W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  On   | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 31%   33C    P8     1W / 250W |  10772MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  On   | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 31%   25C    P8    19W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  On   | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 31%   27C    P8     8W / 250W |     11MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  On   | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 34%   58C    P2   180W / 250W |   9244MiB / 10989MiB |     97%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  On   | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 41%   68C    P2   148W / 250W |   8857MiB / 10989MiB |     90%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  On   | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 42%   62C    P2    63W / 250W |   8853MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  GeForce RTX 208...  On   | 00000000:40:00.0 Off |                  N/A |\n",
      "| 38%   59C    P2   130W / 250W |   8857MiB / 10989MiB |     95%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  GeForce RTX 208...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 40%   66C    P2   217W / 250W |   8857MiB / 10989MiB |     97%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    2     30140      C   ...m63/miniconda3/envs/research/bin/python 10761MiB |\n",
      "|    5     36472      C   ...a3/envs/tensorflow-gpu-rdkit/bin/python   387MiB |\n",
      "|    5     40811      C   python                                      4423MiB |\n",
      "|    5     40813      C   python                                      4423MiB |\n",
      "|    6       471      C   python                                      4423MiB |\n",
      "|    6       474      C   python                                      4423MiB |\n",
      "|    7     20261      C   python                                      4423MiB |\n",
      "|    7     20263      C   python                                      4419MiB |\n",
      "|    8     34696      C   python                                      4423MiB |\n",
      "|    8     34698      C   python                                      4423MiB |\n",
      "|    9     33263      C   python                                      4423MiB |\n",
      "|    9     33265      C   python                                      4423MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10, 5)\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/1\n",
      " - 218s - loss: 0.2270 - acc: 0.9454 - val_loss: 0.2107 - val_acc: 0.9535\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/2\n",
      " - 217s - loss: 0.2284 - acc: 0.9449 - val_loss: 0.2116 - val_acc: 0.9532\n",
      "Epoch 2/2\n",
      " - 216s - loss: 0.2117 - acc: 0.9492 - val_loss: 0.2105 - val_acc: 0.9531\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/10\n",
      " - 218s - loss: 0.2280 - acc: 0.9456 - val_loss: 0.2111 - val_acc: 0.9531\n",
      "Epoch 2/10\n",
      " - 217s - loss: 0.2113 - acc: 0.9492 - val_loss: 0.2098 - val_acc: 0.9536\n",
      "Epoch 3/10\n",
      " - 215s - loss: 0.2107 - acc: 0.9493 - val_loss: 0.2095 - val_acc: 0.9534\n",
      "Epoch 4/10\n",
      " - 216s - loss: 0.2104 - acc: 0.9493 - val_loss: 0.2096 - val_acc: 0.9535\n",
      "Epoch 5/10\n",
      " - 217s - loss: 0.2101 - acc: 0.9493 - val_loss: 0.2097 - val_acc: 0.9534\n",
      "Epoch 6/10\n",
      " - 218s - loss: 0.2100 - acc: 0.9494 - val_loss: 0.2094 - val_acc: 0.9534\n",
      "Epoch 7/10\n",
      " - 216s - loss: 0.2099 - acc: 0.9494 - val_loss: 0.2094 - val_acc: 0.9534\n",
      "Epoch 8/10\n",
      " - 216s - loss: 0.2098 - acc: 0.9494 - val_loss: 0.2092 - val_acc: 0.9533\n",
      "Epoch 9/10\n",
      " - 216s - loss: 0.2097 - acc: 0.9494 - val_loss: 0.2090 - val_acc: 0.9535\n",
      "Epoch 10/10\n",
      " - 216s - loss: 0.2097 - acc: 0.9494 - val_loss: 0.2091 - val_acc: 0.9535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_3/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_4/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_5/concat:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'concatenate_6/concat:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_4:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_7:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_8:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/mcb/dlim63/miniconda3/envs/research/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_11:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'input_12:0' shape=(?, 16) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 0.20899873216769524]]\n",
      "0 0.21109380873186248\n",
      "1 0.20977204321218387\n",
      "2 0.2094783389089363\n",
      "3 0.20960083141390767\n",
      "4 0.20966616190969944\n",
      "5 0.20943903359451463\n",
      "6 0.20941111458199366\n",
      "7 0.20922467786286558\n",
      "8 0.20899873216769524\n",
      "9 0.20906464979584727\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/1\n",
      " - 212s - loss: 0.2214 - acc: 0.9466 - val_loss: 0.2099 - val_acc: 0.9533\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/2\n",
      " - 214s - loss: 0.2215 - acc: 0.9465 - val_loss: 0.2099 - val_acc: 0.9535\n",
      "Epoch 2/2\n",
      " - 212s - loss: 0.2104 - acc: 0.9493 - val_loss: 0.2094 - val_acc: 0.9535\n",
      "Train on 2000000 samples, validate on 35000 samples\n",
      "Epoch 1/10\n",
      " - 215s - loss: 0.2212 - acc: 0.9464 - val_loss: 0.2100 - val_acc: 0.9533\n",
      "Epoch 2/10\n",
      " - 212s - loss: 0.2103 - acc: 0.9493 - val_loss: 0.2093 - val_acc: 0.9536\n",
      "Epoch 3/10\n",
      " - 213s - loss: 0.2097 - acc: 0.9494 - val_loss: 0.2089 - val_acc: 0.9535\n",
      "Epoch 4/10\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, TimeDistributed, Dense, RepeatVector, CuDNNLSTM, GRU, Bidirectional, Input, CuDNNGRU\n",
    "#from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "import difflib\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "seq_length = 10\n",
    "val_loss_hist = []\n",
    "\n",
    "K.clear_session()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "X_train=np.load('prepData/X_train_gap_hg38_v3_chr2_size10.npy')\n",
    "X_val=np.load('prepData/X_val_gap_hg38_v3_chr2_size10.npy')\n",
    "X_test=np.load('prepData/X_test_gap_hg38_v3_chr2_size10.npy')\n",
    "y_train=np.load('prepData/y_train_gap_hg38_v3_chr2_size10.npy')\n",
    "y_val=np.load('prepData/y_val_gap_hg38_v3_chr2_size10.npy')\n",
    "y_test=np.load('prepData/y_test_gap_hg38_v3_chr2_size10.npy')\n",
    "\n",
    "y_train1 = np.load('prepData/y_train1_gap_hg38_v3_chr2_size10.npy')\n",
    "y_val1 = np.load('prepData/y_val1_gap_hg38_v3_chr2_size10.npy')\n",
    "y_test1 = np.load('prepData/y_test1_gap_hg38_v3_chr2_size10.npy')\n",
    "\n",
    "def concat(input1, input2):\n",
    "    result = []\n",
    "    for x, y in zip(input1, input2):\n",
    "        result.append(np.hstack((x, y)))\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "y_val1 = concat(X_val, y_val1)\n",
    "y_test1 = concat(X_test, y_test1)\n",
    "    \n",
    "nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "def decoder(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    for i in range(size):\n",
    "        if array[i].tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "            result=result+\"0\" \n",
    "        elif array[i].tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "            result=result+\"A\"\n",
    "        elif array[i].tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "            result=result+\"C\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "            result=result+\"G\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "            result=result+\"T\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "            result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "#model5 = load_model('model/seq2seq_nogap_1mut.h5')\n",
    "def decoderY(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    \n",
    "    if array.tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "        result=result+\"0\" \n",
    "    elif array.tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "        result=result+\"A\"\n",
    "    elif array.tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "        result=result+\"C\"\n",
    "    elif array.tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "        result=result+\"G\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "        result=result+\"T\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "        result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def printHitMiss(a,b):\n",
    "    if a==b:\n",
    "        return 'Hit'\n",
    "    else:\n",
    "        return 'Miss'\n",
    "    \n",
    "print(X_test.shape)\n",
    "def accuracy(a, b):\n",
    "    count = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == b[i]:\n",
    "            count = count+1\n",
    "    return count/len(a)\n",
    "\n",
    "def accuracy2(a, b, c):\n",
    "    count = 0\n",
    "    count2 =0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != c[i]:\n",
    "            count2 = count2 +1\n",
    "        if a[i] != c[i] and b[i]==c[i]:\n",
    "            count = count+1\n",
    "    return count/count2\n",
    "\n",
    "def isMutation(a, b):\n",
    "    if a!= b:\n",
    "        print(\"mutation\")\n",
    "\n",
    "def predict(model):\n",
    "    x_true=[]\n",
    "    y_hat_list = []\n",
    "    y_true = []\n",
    "    predictions = model.predict(X_test, batch_size=250, verbose=0)\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        #print(prediction)\n",
    "        # print(prediction)\n",
    "        #x_index = np.argmax(testX[i], axis=1)\n",
    "        #print(prediction[i])\n",
    "        x_str = decoder(X_test[i])\n",
    "\n",
    "        #index = np.argmax(prediction)\n",
    "        index = np.random.choice(6, 3, p=prediction)\n",
    "        result = [nucleotide[index]]\n",
    "\n",
    "        print(''.join(x_str), ' -> ', ''.join(result),\n",
    "              \" true: \", ''.join(decoderY(y_test[i])), printHitMiss(''.join(result), ''.join(decoderY(y_test[i]))))\n",
    "        x_true.append(''.join(x_str[5]))\n",
    "        y_hat_list.append(''.join(result))\n",
    "        y_true.append(''.join(decoderY(y_test[i])))\n",
    "    sm=difflib.SequenceMatcher(None,y_hat_list,y_true)\n",
    "    sm2=difflib.SequenceMatcher(None,y_hat_list,x_true)\n",
    "    print()\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat_list, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat_list, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat_list, y_true)))\n",
    "    return x_true, y_hat_list, y_true\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def lstm_model(latent_dim, half):\n",
    "    batch_size = 1000  # Batch size for training.\n",
    "    epochs = 45  # Number of epochs to train for.\n",
    "#     latent_dim = 128  # Latent dimensionality of the encoding space.\n",
    "#     half = 64\n",
    "    num_samples = 10000  # Number of samples to train on.\n",
    "    encoder_inputs = Input(shape=(None, 5))\n",
    "    \n",
    "    encoder = Bidirectional(CuDNNLSTM(half, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = concatenate([forward_h, backward_h])\n",
    "    state_c = concatenate([forward_c, backward_c])\n",
    "    \n",
    "    \n",
    "    # only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder\n",
    "    decoder_inputs = Input(shape=(None, 10))\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(5, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Run training\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def modelFit(epoch, batchSize, latent_dim, half, X_train, y_train, y_train1):\n",
    "    model1, encoder_model1, decoder_model1 = lstm_model(latent_dim, half)\n",
    "    hist1 = model1.fit([X_train, y_train1], y_train,\n",
    "          batch_size=batchSize,\n",
    "          epochs=epoch,\n",
    "          validation_data=([X_val,y_val1], y_val),\n",
    "          verbose = 2\n",
    "         )\n",
    "    return hist1, model1, encoder_model1, decoder_model1\n",
    "\n",
    "def grid_search(latent, half,train_size, X_train, y_train, y_train1):\n",
    "    hist1, model1, encoder_model1, decoder_model1 = modelFit(1, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist2 ,model2, encoder_model2, decoder_model2 = modelFit(2, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist3 ,model3, encoder_model3, decoder_model3 = modelFit(10, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist4 ,model4, encoder_model4, decoder_model4 = modelFit(30, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist5 ,model5, encoder_model5, decoder_model5 = modelFit(50, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist6 ,model6, encoder_model6, decoder_model6 = modelFit(80, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist7 ,model7, encoder_model7, decoder_model7 = modelFit(100, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist8 ,model8, encoder_model8, decoder_model8 = modelFit(500, 100, latent, half)\n",
    "\n",
    "    model1.save(\"models/gap_hg38_{}_{}_1.h5\".format(train_size,half))\n",
    "    model2.save(\"models/gap_hg38_{}_{}_2.h5\".format(train_size,half))\n",
    "    model3.save(\"models/gap_hg38_{}_{}_10.h5\".format(train_size,half))\n",
    "    #model4.save(\"models/_gap_hg38_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #model5.save(\"models/_gap_hg38_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #model6.save(\"models/_gap_hg38_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #model7.save(\"models/_gap_hg38_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #model8.save(\"_gap_hg38_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    encoder_model1.save(\"models/Egap_hg38_{}_{}_1.h5\".format(train_size,half))\n",
    "    encoder_model2.save(\"models/Egap_hg38_{}_{}_2.h5\".format(train_size,half))\n",
    "    encoder_model3.save(\"models/Egap_hg38_{}_{}_10.h5\".format(train_size,half))\n",
    "    #encoder_model4.save(\"models/E_gap_hg38_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #encoder_model5.save(\"models/E_gap_hg38_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #encoder_model6.save(\"models/E_gap_hg38_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #encoder_model7.save(\"models/E_gap_hg38_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #encoder_model8.save(\"E_gap_hg38_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    decoder_model1.save(\"models/Dgap_hg38_{}_{}_1.h5\".format(train_size,half))\n",
    "    decoder_model2.save(\"models/Dgap_hg38_{}_{}_2.h5\".format(train_size,half))\n",
    "    decoder_model3.save(\"models/Dgap_hg38_{}_{}_10.h5\".format(train_size,half))\n",
    "    #decoder_model4.save(\"models/D_gap_hg38_{}_{}_30_double.h5\".format(train_size,half))\n",
    "    #decoder_model5.save(\"models/D_gap_hg38_{}_{}_50_double.h5\".format(train_size,half))\n",
    "    #decoder_model6.save(\"models/D_gap_hg38_{}_{}_80_double.h5\".format(train_size,half))\n",
    "    #decoder_model7.save(\"models/D_gap_hg38_{}_{}_100_double.h5\".format(train_size,half))\n",
    "    #decoder_model8.save(\"D_gap_hg38_{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    count = [i for i in range(len(hist3.history['val_loss']))]\n",
    "    val_loss_hist.append([hist3.history['val_loss'].index(min(hist3.history['val_loss'])),min(hist3.history['val_loss'])])\n",
    "    print(val_loss_hist)\n",
    "    for i, value in zip(count, hist3.history['val_loss']):\n",
    "        print(i, value)\n",
    "\n",
    "grid_search(16, 8, 000, X_train, y_train, y_train1)        \n",
    "grid_search(32, 16, 000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 000, X_train, y_train, y_train1)\n",
    "#grid_search(8192, 4096, 000, X_train, y_train, y_train1)\n",
    "\n",
    "with open('loss_hist.txt', 'wb') as fp:\n",
    "    pickle.dump(val_loss_hist, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
