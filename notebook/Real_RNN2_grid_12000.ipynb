{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 14 19:01:52 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:1A:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    56W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:1C:00.0 Off |                    0 |\r\n",
      "| N/A   35C    P0    53W / 300W |    328MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1    101922      C   /home/dongjoon/jupyter_py3/bin/python        317MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U scikit-learn\n",
    "# !pip install keras\n",
    "# !pip3 install cudnnenv\n",
    "# !pip install tensorflow-gpu\n",
    "# !pip install matplotlib\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, LSTM, TimeDistributed, Dense, RepeatVector, CuDNNLSTM, GRU, Bidirectional, Input, CuDNNGRU\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "import difflib\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import choice\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 19:01:56.620094 47844310998592 deprecation_wrapper.py:119] From /home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "K.clear_session()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:12000]\n",
    "X_val=np.load('prepData/X_val_camFam3_1mutOnly_v3_chr2.npy')[:10000]\n",
    "X_test=np.load('prepData/X_test_camFam3_1mutOnly_v3_chr2.npy')[:4000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:12000]\n",
    "y_val=np.load('prepData/y_val_camFam3_1mutOnly_v3_chr2.npy')[:10000]\n",
    "y_test=np.load('prepData/y_test_camFam3_1mutOnly_v3_chr2.npy')[:4000]\n",
    "\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:12000]\n",
    "y_val1 = np.load('prepData/y_val1_camFam3_1mutOnly_v3_chr2.npy')[:10000]\n",
    "y_test1 = np.load('prepData/y_test1_camFam3_1mutOnly_v3_chr2.npy')[:4000]\n",
    "\n",
    "def concat(input1, input2):\n",
    "    result = []\n",
    "    for x, y in zip(input1, input2):\n",
    "        result.append(np.hstack((x, y)))\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "y_val1 = concat(X_val, y_val1)\n",
    "y_test1 = concat(X_test, y_test1)\n",
    "    \n",
    "# X_train=np.load('prepData/X_train_camFam3_1mut.npy')\n",
    "# X_val=np.load('prepData/X_val_camFam3_1mut.npy')\n",
    "# X_test=np.load('prepData/X_test_camFam3_1mut.npy')\n",
    "# y_train=np.load('prepData/y_train_camFam3_1mut.npy')\n",
    "# y_val=np.load('prepData/y_val_camFam3_1mut.npy')\n",
    "# y_test=np.load('prepData/y_test_camFam3_1mut.npy')\n",
    "\n",
    "# X_train=np.load('prepData20/X_train_camFam3_1mutOnly.npy')\n",
    "# X_val=np.load('prepData20/X_val_camFam3_1mutOnly.npy')\n",
    "# X_test=np.load('prepData20/X_test_camFam3_1mutOnly.npy')\n",
    "# y_train=np.load('prepData20/y_train_camFam3_1mutOnly.npy')\n",
    "# y_val=np.load('prepData20/y_val_camFam3_1mutOnly.npy')\n",
    "# y_test=np.load('prepData20/y_test_camFam3_1mutOnly.npy')\n",
    "nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "#model5 = load_model('model/seq2seq_nogap_camFam3_1mutOnly.h5')\n",
    "def decoder(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    for i in range(size):\n",
    "        if array[i].tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "            result=result+\"0\" \n",
    "        elif array[i].tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "            result=result+\"A\"\n",
    "        elif array[i].tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "            result=result+\"C\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "            result=result+\"G\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "            result=result+\"T\"\n",
    "        elif array[i].tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "            result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "#model5 = load_model('model/seq2seq_nogap_camFam3_1mutOnly.h5')\n",
    "def decoderY(array):\n",
    "    result = \"\"\n",
    "    size = len(array)\n",
    "    \n",
    "    if array.tolist() == [1, 0, 0, 0, 0, 0]:\n",
    "        result=result+\"0\" \n",
    "    elif array.tolist() == [0, 1, 0, 0, 0, 0]:\n",
    "        result=result+\"A\"\n",
    "    elif array.tolist() == [0, 0, 1, 0, 0, 0]:\n",
    "        result=result+\"C\"\n",
    "    elif array.tolist() == [0, 0, 0, 1, 0, 0]:\n",
    "        result=result+\"G\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 1, 0]:\n",
    "        result=result+\"T\"\n",
    "    elif array.tolist() == [0, 0, 0, 0, 0, 1]:\n",
    "        result=result+\"-\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def printHitMiss(a,b):\n",
    "    if a==b:\n",
    "        return 'Hit'\n",
    "    else:\n",
    "        return 'Miss'\n",
    "    \n",
    "print(X_test.shape)\n",
    "def accuracy(a, b):\n",
    "    count = 0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] == b[i]:\n",
    "            count = count+1\n",
    "    return count/len(a)\n",
    "\n",
    "def accuracy2(a, b, c):\n",
    "    count = 0\n",
    "    count2 =0\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != c[i]:\n",
    "            count2 = count2 +1\n",
    "        if a[i] != c[i] and b[i]==c[i]:\n",
    "            count = count+1\n",
    "    return count/count2\n",
    "\n",
    "def isMutation(a, b):\n",
    "    if a!= b:\n",
    "        print(\"mutation\")\n",
    "\n",
    "def predict(model):\n",
    "    x_true=[]\n",
    "    y_hat_list = []\n",
    "    y_true = []\n",
    "    predictions = model.predict(X_test, batch_size=250, verbose=0)\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        #print(prediction)\n",
    "        # print(prediction)\n",
    "        #x_index = np.argmax(testX[i], axis=1)\n",
    "        #print(prediction[i])\n",
    "        x_str = decoder(X_test[i])\n",
    "\n",
    "        #index = np.argmax(prediction)\n",
    "        index = np.random.choice(6, 3, p=prediction)\n",
    "        result = [nucleotide[index]]\n",
    "\n",
    "        print(''.join(x_str), ' -> ', ''.join(result),\n",
    "              \" true: \", ''.join(decoderY(y_test[i])), printHitMiss(''.join(result), ''.join(decoderY(y_test[i]))))\n",
    "        x_true.append(''.join(x_str[5]))\n",
    "        y_hat_list.append(''.join(result))\n",
    "        y_true.append(''.join(decoderY(y_test[i])))\n",
    "    sm=difflib.SequenceMatcher(None,y_hat_list,y_true)\n",
    "    sm2=difflib.SequenceMatcher(None,y_hat_list,x_true)\n",
    "    print()\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat_list, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat_list, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat_list, y_true)))\n",
    "    return x_true, y_hat_list, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 19:02:12.550319 47844310998592 deprecation_wrapper.py:119] From /home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0814 19:02:12.726623 47844310998592 deprecation.py:323] From /home/dongjoon/jupyter_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 115s 10ms/step - loss: 1.6248 - acc: 0.2927 - val_loss: 1.4588 - val_acc: 0.3607\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 1.3683 - acc: 0.4253 - val_loss: 1.2529 - val_acc: 0.4577\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 1.1848 - acc: 0.5026 - val_loss: 1.0998 - val_acc: 0.5634\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 1.0409 - acc: 0.6898 - val_loss: 0.9737 - val_acc: 0.8297\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.9161 - acc: 0.8871 - val_loss: 0.8646 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 3s 284us/step - loss: 1.6224 - acc: 0.2901 - val_loss: 1.4371 - val_acc: 0.4003\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 1.3201 - acc: 0.5042 - val_loss: 1.1747 - val_acc: 0.7211\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 1.0645 - acc: 0.7840 - val_loss: 0.9383 - val_acc: 0.8349\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.8583 - acc: 0.8617 - val_loss: 0.7696 - val_acc: 0.8970\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.7188 - acc: 0.8981 - val_loss: 0.6619 - val_acc: 0.8998\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.6290 - acc: 0.9000 - val_loss: 0.5943 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.5704 - acc: 0.9000 - val_loss: 0.5494 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.5310 - acc: 0.9000 - val_loss: 0.5185 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.5037 - acc: 0.9000 - val_loss: 0.4983 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4842 - acc: 0.9000 - val_loss: 0.4826 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 4s 296us/step - loss: 1.7176 - acc: 0.2349 - val_loss: 1.5707 - val_acc: 0.2920\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 1.4560 - acc: 0.4193 - val_loss: 1.3698 - val_acc: 0.5521\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 1.2807 - acc: 0.6450 - val_loss: 1.1980 - val_acc: 0.6852\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 1.0930 - acc: 0.8491 - val_loss: 0.9970 - val_acc: 0.8816\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.9200 - acc: 0.8943 - val_loss: 0.8480 - val_acc: 0.8990\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.7977 - acc: 0.8994 - val_loss: 0.7446 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.7068 - acc: 0.9000 - val_loss: 0.6640 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.6358 - acc: 0.9000 - val_loss: 0.6027 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.5787 - acc: 0.9000 - val_loss: 0.5540 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.5347 - acc: 0.9000 - val_loss: 0.5179 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.5023 - acc: 0.9000 - val_loss: 0.4924 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4788 - acc: 0.9000 - val_loss: 0.4741 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4616 - acc: 0.9000 - val_loss: 0.4610 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4491 - acc: 0.9000 - val_loss: 0.4528 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4398 - acc: 0.9000 - val_loss: 0.4457 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4329 - acc: 0.9000 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4276 - acc: 0.9000 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4236 - acc: 0.9000 - val_loss: 0.4355 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4206 - acc: 0.9000 - val_loss: 0.4342 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4181 - acc: 0.9000 - val_loss: 0.4333 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 4s 302us/step - loss: 1.6703 - acc: 0.2203 - val_loss: 1.5260 - val_acc: 0.2030\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 1.4138 - acc: 0.5620 - val_loss: 1.3195 - val_acc: 0.7655\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 1.2259 - acc: 0.8351 - val_loss: 1.1148 - val_acc: 0.8843\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 1.0252 - acc: 0.8921 - val_loss: 0.9250 - val_acc: 0.8936\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.8503 - acc: 0.8970 - val_loss: 0.7727 - val_acc: 0.8967\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.7077 - acc: 0.8989 - val_loss: 0.6498 - val_acc: 0.8989\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.5934 - acc: 0.8999 - val_loss: 0.5544 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.5149 - acc: 0.9000 - val_loss: 0.4987 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4726 - acc: 0.9000 - val_loss: 0.4712 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4490 - acc: 0.9000 - val_loss: 0.4546 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4351 - acc: 0.9000 - val_loss: 0.4461 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4270 - acc: 0.9000 - val_loss: 0.4398 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4221 - acc: 0.9000 - val_loss: 0.4370 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4190 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4170 - acc: 0.9000 - val_loss: 0.4355 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4155 - acc: 0.9000 - val_loss: 0.4356 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4144 - acc: 0.9000 - val_loss: 0.4350 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4135 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4128 - acc: 0.9000 - val_loss: 0.4337 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4122 - acc: 0.9000 - val_loss: 0.4339 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4117 - acc: 0.9000 - val_loss: 0.4328 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4112 - acc: 0.9000 - val_loss: 0.4329 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4108 - acc: 0.9000 - val_loss: 0.4326 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.4105 - acc: 0.9000 - val_loss: 0.4326 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4101 - acc: 0.9000 - val_loss: 0.4318 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4098 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4095 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4092 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4089 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4086 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 4s 320us/step - loss: 1.6084 - acc: 0.3697 - val_loss: 1.4042 - val_acc: 0.6302\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 1.3258 - acc: 0.5492 - val_loss: 1.1989 - val_acc: 0.6330\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 1.1408 - acc: 0.6403 - val_loss: 1.0315 - val_acc: 0.7495\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.9777 - acc: 0.7608 - val_loss: 0.8742 - val_acc: 0.8279\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.7949 - acc: 0.8683 - val_loss: 0.6845 - val_acc: 0.8992\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.6129 - acc: 0.8999 - val_loss: 0.5550 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.5167 - acc: 0.9000 - val_loss: 0.4970 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4732 - acc: 0.9000 - val_loss: 0.4699 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4509 - acc: 0.9000 - val_loss: 0.4564 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4382 - acc: 0.9000 - val_loss: 0.4490 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4305 - acc: 0.9000 - val_loss: 0.4447 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4254 - acc: 0.9000 - val_loss: 0.4403 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4220 - acc: 0.9000 - val_loss: 0.4380 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4195 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4176 - acc: 0.9000 - val_loss: 0.4344 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4161 - acc: 0.9000 - val_loss: 0.4348 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4150 - acc: 0.9000 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4140 - acc: 0.9000 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4132 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4125 - acc: 0.9000 - val_loss: 0.4325 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4120 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4114 - acc: 0.9000 - val_loss: 0.4314 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4109 - acc: 0.9000 - val_loss: 0.4329 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4106 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4102 - acc: 0.9000 - val_loss: 0.4284 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4098 - acc: 0.9000 - val_loss: 0.4336 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4095 - acc: 0.9000 - val_loss: 0.4311 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4092 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4089 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4087 - acc: 0.9000 - val_loss: 0.4293 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4084 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4081 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4079 - acc: 0.9000 - val_loss: 0.4281 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4077 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4075 - acc: 0.9000 - val_loss: 0.4281 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4072 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4071 - acc: 0.9000 - val_loss: 0.4309 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4069 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4067 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4064 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4062 - acc: 0.9000 - val_loss: 0.4274 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4060 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4272 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4058 - acc: 0.9000 - val_loss: 0.4278 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4056 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4054 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4051 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 4s 338us/step - loss: 1.6630 - acc: 0.3363 - val_loss: 1.5140 - val_acc: 0.3865\n",
      "Epoch 2/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 104us/step - loss: 1.4456 - acc: 0.3967 - val_loss: 1.3258 - val_acc: 0.4493\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 1.2529 - acc: 0.5333 - val_loss: 1.1235 - val_acc: 0.6472\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 1.0472 - acc: 0.7358 - val_loss: 0.9218 - val_acc: 0.8654\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.8567 - acc: 0.8877 - val_loss: 0.7602 - val_acc: 0.8991\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.7138 - acc: 0.8998 - val_loss: 0.6446 - val_acc: 0.8999\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.6148 - acc: 0.8999 - val_loss: 0.5692 - val_acc: 0.8999\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.5505 - acc: 0.9000 - val_loss: 0.5219 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.5101 - acc: 0.9000 - val_loss: 0.4931 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4843 - acc: 0.9000 - val_loss: 0.4752 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4674 - acc: 0.9000 - val_loss: 0.4640 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4561 - acc: 0.9000 - val_loss: 0.4557 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4483 - acc: 0.9000 - val_loss: 0.4512 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4430 - acc: 0.9000 - val_loss: 0.4479 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4391 - acc: 0.9000 - val_loss: 0.4463 - val_acc: 0.9000\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4364 - acc: 0.9000 - val_loss: 0.4443 - val_acc: 0.9000\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4344 - acc: 0.9000 - val_loss: 0.4437 - val_acc: 0.9000\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4329 - acc: 0.9000 - val_loss: 0.4436 - val_acc: 0.9000\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4318 - acc: 0.9000 - val_loss: 0.4429 - val_acc: 0.9000\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4309 - acc: 0.9000 - val_loss: 0.4423 - val_acc: 0.9000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4302 - acc: 0.9000 - val_loss: 0.4424 - val_acc: 0.9000\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4296 - acc: 0.9000 - val_loss: 0.4422 - val_acc: 0.9000\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4291 - acc: 0.9000 - val_loss: 0.4424 - val_acc: 0.9000\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.4286 - acc: 0.9000 - val_loss: 0.4421 - val_acc: 0.9000\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4282 - acc: 0.9000 - val_loss: 0.4428 - val_acc: 0.9000\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4278 - acc: 0.9000 - val_loss: 0.4416 - val_acc: 0.9000\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4274 - acc: 0.9000 - val_loss: 0.4414 - val_acc: 0.9000\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4271 - acc: 0.9000 - val_loss: 0.4420 - val_acc: 0.9000\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4267 - acc: 0.9000 - val_loss: 0.4409 - val_acc: 0.9000\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4264 - acc: 0.9000 - val_loss: 0.4417 - val_acc: 0.9000\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4261 - acc: 0.9000 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4258 - acc: 0.9000 - val_loss: 0.4404 - val_acc: 0.9000\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4256 - acc: 0.9000 - val_loss: 0.4403 - val_acc: 0.9000\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4253 - acc: 0.9000 - val_loss: 0.4413 - val_acc: 0.9000\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4250 - acc: 0.9000 - val_loss: 0.4408 - val_acc: 0.9000\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4247 - acc: 0.9000 - val_loss: 0.4410 - val_acc: 0.9000\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4244 - acc: 0.9000 - val_loss: 0.4402 - val_acc: 0.9000\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4242 - acc: 0.9000 - val_loss: 0.4407 - val_acc: 0.9000\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4238 - acc: 0.9000 - val_loss: 0.4408 - val_acc: 0.9000\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4236 - acc: 0.9000 - val_loss: 0.4394 - val_acc: 0.9000\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4233 - acc: 0.9000 - val_loss: 0.4402 - val_acc: 0.9000\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4230 - acc: 0.9000 - val_loss: 0.4392 - val_acc: 0.9000\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4227 - acc: 0.9000 - val_loss: 0.4389 - val_acc: 0.9000\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4224 - acc: 0.9000 - val_loss: 0.4397 - val_acc: 0.9000\n",
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4220 - acc: 0.9000 - val_loss: 0.4383 - val_acc: 0.9000\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4217 - acc: 0.9000 - val_loss: 0.4386 - val_acc: 0.9000\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4214 - acc: 0.9000 - val_loss: 0.4383 - val_acc: 0.9000\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4210 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4206 - acc: 0.9000 - val_loss: 0.4364 - val_acc: 0.9000\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4201 - acc: 0.9000 - val_loss: 0.4364 - val_acc: 0.9000\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4196 - acc: 0.9000 - val_loss: 0.4363 - val_acc: 0.9000\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4191 - acc: 0.9000 - val_loss: 0.4370 - val_acc: 0.9000\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4184 - acc: 0.9000 - val_loss: 0.4359 - val_acc: 0.9000\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4178 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4171 - acc: 0.9000 - val_loss: 0.4346 - val_acc: 0.9000\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4164 - acc: 0.9000 - val_loss: 0.4352 - val_acc: 0.9000\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4157 - acc: 0.9000 - val_loss: 0.4340 - val_acc: 0.9000\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4148 - acc: 0.9000 - val_loss: 0.4325 - val_acc: 0.9000\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4139 - acc: 0.9000 - val_loss: 0.4317 - val_acc: 0.9000\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4130 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.9000\n",
      "Epoch 61/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4121 - acc: 0.9000 - val_loss: 0.4311 - val_acc: 0.9000\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4112 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4104 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4095 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4088 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4081 - acc: 0.9000 - val_loss: 0.4275 - val_acc: 0.9000\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4075 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4070 - acc: 0.9000 - val_loss: 0.4274 - val_acc: 0.9000\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4261 - val_acc: 0.9000\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4061 - acc: 0.9000 - val_loss: 0.4264 - val_acc: 0.9000\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4057 - acc: 0.9000 - val_loss: 0.4246 - val_acc: 0.9000\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4269 - val_acc: 0.9000\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4256 - val_acc: 0.9000\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4044 - acc: 0.9000 - val_loss: 0.4252 - val_acc: 0.9000\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4042 - acc: 0.9000 - val_loss: 0.4261 - val_acc: 0.9000\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4039 - acc: 0.9000 - val_loss: 0.4257 - val_acc: 0.9000\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4037 - acc: 0.9000 - val_loss: 0.4234 - val_acc: 0.9000\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4251 - val_acc: 0.9000\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4032 - acc: 0.9000 - val_loss: 0.4258 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 4s 347us/step - loss: 1.6176 - acc: 0.2584 - val_loss: 1.4689 - val_acc: 0.4712\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 1.3619 - acc: 0.6443 - val_loss: 1.2470 - val_acc: 0.7870\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 1.1456 - acc: 0.8377 - val_loss: 1.0315 - val_acc: 0.8888\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.9459 - acc: 0.8968 - val_loss: 0.8574 - val_acc: 0.8999\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.7917 - acc: 0.9000 - val_loss: 0.7314 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.6834 - acc: 0.9000 - val_loss: 0.6502 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.6136 - acc: 0.9000 - val_loss: 0.5975 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.5680 - acc: 0.9000 - val_loss: 0.5664 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.5358 - acc: 0.9000 - val_loss: 0.5394 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.5069 - acc: 0.9000 - val_loss: 0.5138 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4803 - acc: 0.9000 - val_loss: 0.4921 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4604 - acc: 0.9000 - val_loss: 0.4789 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4467 - acc: 0.9000 - val_loss: 0.4687 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4373 - acc: 0.9000 - val_loss: 0.4627 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4307 - acc: 0.9000 - val_loss: 0.4566 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4257 - acc: 0.9000 - val_loss: 0.4519 - val_acc: 0.9000\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4219 - acc: 0.9000 - val_loss: 0.4474 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4189 - acc: 0.9000 - val_loss: 0.4436 - val_acc: 0.9000\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4166 - acc: 0.9000 - val_loss: 0.4429 - val_acc: 0.9000\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4148 - acc: 0.9000 - val_loss: 0.4380 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4134 - acc: 0.9000 - val_loss: 0.4388 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4123 - acc: 0.9000 - val_loss: 0.4355 - val_acc: 0.9000\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4114 - acc: 0.9000 - val_loss: 0.4371 - val_acc: 0.9000\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4106 - acc: 0.9000 - val_loss: 0.4333 - val_acc: 0.9000\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4100 - acc: 0.9000 - val_loss: 0.4334 - val_acc: 0.9000\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4095 - acc: 0.9000 - val_loss: 0.4334 - val_acc: 0.9000\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4090 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4086 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4082 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4078 - acc: 0.9000 - val_loss: 0.4317 - val_acc: 0.9000\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4075 - acc: 0.9000 - val_loss: 0.4314 - val_acc: 0.9000\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4072 - acc: 0.9000 - val_loss: 0.4312 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4069 - acc: 0.9000 - val_loss: 0.4316 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4067 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4064 - acc: 0.9000 - val_loss: 0.4317 - val_acc: 0.9000\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4062 - acc: 0.9000 - val_loss: 0.4312 - val_acc: 0.9000\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4279 - val_acc: 0.9000\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4057 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4056 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4051 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4049 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4046 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4044 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4041 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4040 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4038 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4037 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4035 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4303 - val_acc: 0.9000\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4033 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4031 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4028 - acc: 0.9000 - val_loss: 0.4310 - val_acc: 0.9000\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4027 - acc: 0.9000 - val_loss: 0.4309 - val_acc: 0.9000\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4024 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4022 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4019 - acc: 0.9000 - val_loss: 0.4310 - val_acc: 0.9000\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4017 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4016 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4274 - val_acc: 0.9000\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4012 - acc: 0.9000 - val_loss: 0.4274 - val_acc: 0.9000\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4011 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4008 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4007 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4004 - acc: 0.9000 - val_loss: 0.4277 - val_acc: 0.9000\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4003 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4001 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3998 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3996 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3994 - acc: 0.9000 - val_loss: 0.4277 - val_acc: 0.9000\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3991 - acc: 0.9000 - val_loss: 0.4278 - val_acc: 0.9000\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3988 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3985 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3982 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3978 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 81/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3974 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.4291 - val_acc: 0.9000\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3963 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3952 - acc: 0.9000 - val_loss: 0.4312 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3940 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3926 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3911 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3896 - acc: 0.9000 - val_loss: 0.4264 - val_acc: 0.9000\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3882 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3869 - acc: 0.9000 - val_loss: 0.4249 - val_acc: 0.9000\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3858 - acc: 0.9000 - val_loss: 0.4236 - val_acc: 0.9000\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3846 - acc: 0.9000 - val_loss: 0.4237 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3835 - acc: 0.9000 - val_loss: 0.4235 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3825 - acc: 0.9000 - val_loss: 0.4229 - val_acc: 0.9000\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3815 - acc: 0.9000 - val_loss: 0.4224 - val_acc: 0.9000\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3807 - acc: 0.9000 - val_loss: 0.4196 - val_acc: 0.9000\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3797 - acc: 0.9000 - val_loss: 0.4213 - val_acc: 0.9000\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3788 - acc: 0.9000 - val_loss: 0.4206 - val_acc: 0.9000\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3778 - acc: 0.9000 - val_loss: 0.4190 - val_acc: 0.9000\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3768 - acc: 0.9000 - val_loss: 0.4165 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_5 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_3/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_4/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_7 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_5/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_6/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_9 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_7/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_8/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_11 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_9/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_10/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_13 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_11/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_12/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_15 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_13/concat:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'concatenate_14/concat:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_5 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_8:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_9:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_7 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_12:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_13:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_9 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_16:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_17:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_11 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_20:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_21:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_13 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_24:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_25:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_15 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_28:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'input_29:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4688580882549287\n",
      "1 1.2470290160179138\n",
      "2 1.0315086144208907\n",
      "3 0.8574006205797196\n",
      "4 0.7313943326473236\n",
      "5 0.6501559627056122\n",
      "6 0.5974877649545669\n",
      "7 0.566421931385994\n",
      "8 0.5393894147872925\n",
      "9 0.513775720000267\n",
      "10 0.49214224189519884\n",
      "11 0.47890583127737046\n",
      "12 0.4687011894583702\n",
      "13 0.46267966836690905\n",
      "14 0.45656452506780626\n",
      "15 0.4519150710105896\n",
      "16 0.4473571854829788\n",
      "17 0.4435927331447601\n",
      "18 0.44285139709711074\n",
      "19 0.43795507043600085\n",
      "20 0.43876630246639253\n",
      "21 0.4354981124401093\n",
      "22 0.43710991203784944\n",
      "23 0.43334481209516523\n",
      "24 0.4334382790327072\n",
      "25 0.43335370600223544\n",
      "26 0.42972027450799943\n",
      "27 0.43151150286197665\n",
      "28 0.432684588432312\n",
      "29 0.43171933859586714\n",
      "30 0.43141226559877394\n",
      "31 0.43124779134988783\n",
      "32 0.4315659961104393\n",
      "33 0.42859404742717744\n",
      "34 0.4317316100001335\n",
      "35 0.43123890459537506\n",
      "36 0.4278994542360306\n",
      "37 0.429233415722847\n",
      "38 0.4294466796517372\n",
      "39 0.42941115140914915\n",
      "40 0.42993014216423037\n",
      "41 0.42945780813694\n",
      "42 0.4301711171865463\n",
      "43 0.42848798871040344\n",
      "44 0.42915634453296664\n",
      "45 0.42985066652297976\n",
      "46 0.4297100183367729\n",
      "47 0.42920227140188216\n",
      "48 0.42823040664196016\n",
      "49 0.4300413101911545\n",
      "50 0.42886035025119784\n",
      "51 0.430293527841568\n",
      "52 0.42882001012563703\n",
      "53 0.4298974373936653\n",
      "54 0.4294572538137436\n",
      "55 0.4310185322165489\n",
      "56 0.4309436789155006\n",
      "57 0.4297987738251686\n",
      "58 0.4283367720246315\n",
      "59 0.4308178877830505\n",
      "60 0.4304925727844238\n",
      "61 0.4309848028421402\n",
      "62 0.4296301457285881\n",
      "63 0.42849117279052734\n",
      "64 0.4273811486363411\n",
      "65 0.4274147963523865\n",
      "66 0.42941182881593704\n",
      "67 0.4293656960129738\n",
      "68 0.4292453271150589\n",
      "69 0.42770072013139726\n",
      "70 0.42848087012767794\n",
      "71 0.42804201543331144\n",
      "72 0.4294579753279686\n",
      "73 0.42764476329088214\n",
      "74 0.4277287009358406\n",
      "75 0.4277614751458168\n",
      "76 0.4298207041621208\n",
      "77 0.42956801772117614\n",
      "78 0.4286316779255867\n",
      "79 0.42997460186481473\n",
      "80 0.43017245680093763\n",
      "81 0.429147852063179\n",
      "82 0.42978978276252744\n",
      "83 0.43120939254760743\n",
      "84 0.4294530752301216\n",
      "85 0.42963596284389494\n",
      "86 0.4287400624155998\n",
      "87 0.42640344381332396\n",
      "88 0.42677955478429797\n",
      "89 0.42490394800901415\n",
      "90 0.42355902284383773\n",
      "91 0.4236715742945671\n",
      "92 0.4234712955355644\n",
      "93 0.4228744640946388\n",
      "94 0.4224040824174881\n",
      "95 0.41963195413351057\n",
      "96 0.4213407337665558\n",
      "97 0.42057482957839964\n",
      "98 0.4190169483423233\n",
      "99 0.416529258787632\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 4s 354us/step - loss: 1.3322 - acc: 0.4639 - val_loss: 0.9591 - val_acc: 0.7564\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.6904 - acc: 0.8760 - val_loss: 0.5131 - val_acc: 0.8999\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4538 - acc: 0.9000 - val_loss: 0.4437 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4183 - acc: 0.9000 - val_loss: 0.4410 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4101 - acc: 0.9000 - val_loss: 0.4337 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 4s 365us/step - loss: 1.2608 - acc: 0.5457 - val_loss: 0.8914 - val_acc: 0.8009\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.6363 - acc: 0.8814 - val_loss: 0.4856 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 97us/step - loss: 0.4395 - acc: 0.9000 - val_loss: 0.4376 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 97us/step - loss: 0.4138 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4084 - acc: 0.9000 - val_loss: 0.4400 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4046 - acc: 0.9000 - val_loss: 0.4341 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4036 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 97us/step - loss: 0.4028 - acc: 0.9000 - val_loss: 0.4307 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 97us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 5s 411us/step - loss: 1.3082 - acc: 0.5301 - val_loss: 0.9005 - val_acc: 0.8111\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.6806 - acc: 0.8800 - val_loss: 0.5440 - val_acc: 0.8994\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4668 - acc: 0.9000 - val_loss: 0.4475 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4226 - acc: 0.9000 - val_loss: 0.4362 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4117 - acc: 0.9000 - val_loss: 0.4351 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4077 - acc: 0.9000 - val_loss: 0.4324 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4055 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4018 - acc: 0.9000 - val_loss: 0.4379 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4003 - acc: 0.9000 - val_loss: 0.4378 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3982 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3962 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3938 - acc: 0.9000 - val_loss: 0.4218 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3916 - acc: 0.9000 - val_loss: 0.4396 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3885 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3850 - acc: 0.9000 - val_loss: 0.4214 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3814 - acc: 0.9000 - val_loss: 0.4193 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3785 - acc: 0.9000 - val_loss: 0.4193 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3755 - acc: 0.9000 - val_loss: 0.4068 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 5s 392us/step - loss: 1.2778 - acc: 0.5508 - val_loss: 0.8808 - val_acc: 0.8067\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.6671 - acc: 0.8843 - val_loss: 0.5231 - val_acc: 0.8999\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4591 - acc: 0.9000 - val_loss: 0.4455 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4190 - acc: 0.9000 - val_loss: 0.4394 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4101 - acc: 0.9000 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4071 - acc: 0.9000 - val_loss: 0.4314 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4052 - acc: 0.9000 - val_loss: 0.4294 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4040 - acc: 0.9000 - val_loss: 0.4330 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4035 - acc: 0.9000 - val_loss: 0.4337 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4017 - acc: 0.9000 - val_loss: 0.4241 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.4002 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3984 - acc: 0.9000 - val_loss: 0.4337 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3969 - acc: 0.9000 - val_loss: 0.4326 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3956 - acc: 0.9000 - val_loss: 0.4388 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3933 - acc: 0.9000 - val_loss: 0.4368 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3912 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3894 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3876 - acc: 0.9000 - val_loss: 0.4270 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3851 - acc: 0.9000 - val_loss: 0.4172 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3830 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3811 - acc: 0.9000 - val_loss: 0.4191 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3793 - acc: 0.9000 - val_loss: 0.4159 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3775 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3744 - acc: 0.9000 - val_loss: 0.4227 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3720 - acc: 0.9000 - val_loss: 0.4122 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3690 - acc: 0.9000 - val_loss: 0.4090 - val_acc: 0.9000\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3657 - acc: 0.9000 - val_loss: 0.4008 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3625 - acc: 0.9000 - val_loss: 0.4081 - val_acc: 0.9000\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3609 - acc: 0.9000 - val_loss: 0.3998 - val_acc: 0.9000\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3579 - acc: 0.9000 - val_loss: 0.4016 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 5s 400us/step - loss: 1.2104 - acc: 0.6161 - val_loss: 0.8262 - val_acc: 0.8395\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.6083 - acc: 0.8895 - val_loss: 0.4846 - val_acc: 0.8999\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4387 - acc: 0.9000 - val_loss: 0.4393 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4144 - acc: 0.9000 - val_loss: 0.4277 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4090 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4066 - acc: 0.9000 - val_loss: 0.4265 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4054 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4042 - acc: 0.9000 - val_loss: 0.4363 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4035 - acc: 0.9000 - val_loss: 0.4263 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4031 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4019 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4010 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4000 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3984 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3951 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3921 - acc: 0.9000 - val_loss: 0.4270 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3891 - acc: 0.9000 - val_loss: 0.4242 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3854 - acc: 0.9000 - val_loss: 0.4204 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3817 - acc: 0.9000 - val_loss: 0.4245 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3770 - acc: 0.9000 - val_loss: 0.4106 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3723 - acc: 0.9000 - val_loss: 0.4114 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3689 - acc: 0.9000 - val_loss: 0.4069 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3657 - acc: 0.9000 - val_loss: 0.4074 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3635 - acc: 0.9000 - val_loss: 0.4055 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3614 - acc: 0.9000 - val_loss: 0.4024 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3593 - acc: 0.9000 - val_loss: 0.3966 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3582 - acc: 0.9000 - val_loss: 0.3951 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3564 - acc: 0.9000 - val_loss: 0.3919 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3552 - acc: 0.9000 - val_loss: 0.3906 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3539 - acc: 0.9000 - val_loss: 0.3956 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3522 - acc: 0.9000 - val_loss: 0.3904 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3516 - acc: 0.9000 - val_loss: 0.3845 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3504 - acc: 0.9000 - val_loss: 0.3891 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3494 - acc: 0.9000 - val_loss: 0.3859 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3478 - acc: 0.9000 - val_loss: 0.3874 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3468 - acc: 0.9000 - val_loss: 0.3888 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3460 - acc: 0.9000 - val_loss: 0.3804 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3457 - acc: 0.9000 - val_loss: 0.3863 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3445 - acc: 0.9001 - val_loss: 0.3934 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3433 - acc: 0.9000 - val_loss: 0.3783 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3416 - acc: 0.9000 - val_loss: 0.3834 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3410 - acc: 0.9001 - val_loss: 0.3821 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3404 - acc: 0.9002 - val_loss: 0.3812 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3390 - acc: 0.9001 - val_loss: 0.3762 - val_acc: 0.8999\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3388 - acc: 0.9002 - val_loss: 0.3774 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.3377 - acc: 0.9002 - val_loss: 0.3721 - val_acc: 0.8999\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3371 - acc: 0.9003 - val_loss: 0.3752 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3361 - acc: 0.9003 - val_loss: 0.3740 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 1s 98us/step - loss: 0.3369 - acc: 0.9004 - val_loss: 0.3740 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3352 - acc: 0.9003 - val_loss: 0.3720 - val_acc: 0.8998\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 5s 416us/step - loss: 1.2628 - acc: 0.5595 - val_loss: 0.8565 - val_acc: 0.8277\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.6237 - acc: 0.8868 - val_loss: 0.4813 - val_acc: 0.8999\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4404 - acc: 0.9000 - val_loss: 0.4322 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4135 - acc: 0.9000 - val_loss: 0.4367 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4080 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4058 - acc: 0.9000 - val_loss: 0.4374 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4042 - acc: 0.9000 - val_loss: 0.4313 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4324 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4013 - acc: 0.9000 - val_loss: 0.4303 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4000 - acc: 0.9000 - val_loss: 0.4201 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3991 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3972 - acc: 0.9000 - val_loss: 0.4346 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3954 - acc: 0.9000 - val_loss: 0.4326 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3934 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3912 - acc: 0.9000 - val_loss: 0.4257 - val_acc: 0.9000\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3891 - acc: 0.9000 - val_loss: 0.4229 - val_acc: 0.9000\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3868 - acc: 0.9000 - val_loss: 0.4272 - val_acc: 0.9000\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3846 - acc: 0.9000 - val_loss: 0.4259 - val_acc: 0.9000\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3822 - acc: 0.9000 - val_loss: 0.4225 - val_acc: 0.9000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3799 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3769 - acc: 0.9000 - val_loss: 0.4190 - val_acc: 0.9000\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3736 - acc: 0.9000 - val_loss: 0.4130 - val_acc: 0.9000\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3699 - acc: 0.9000 - val_loss: 0.4058 - val_acc: 0.9000\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3656 - acc: 0.9000 - val_loss: 0.4015 - val_acc: 0.9000\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3619 - acc: 0.9000 - val_loss: 0.4117 - val_acc: 0.9000\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3591 - acc: 0.9000 - val_loss: 0.3959 - val_acc: 0.9000\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3566 - acc: 0.9000 - val_loss: 0.3944 - val_acc: 0.9000\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3534 - acc: 0.9001 - val_loss: 0.3926 - val_acc: 0.9001\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3514 - acc: 0.9001 - val_loss: 0.3955 - val_acc: 0.9002\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3489 - acc: 0.9001 - val_loss: 0.3851 - val_acc: 0.9000\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3470 - acc: 0.9001 - val_loss: 0.3891 - val_acc: 0.9001\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3457 - acc: 0.9002 - val_loss: 0.3897 - val_acc: 0.9001\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3449 - acc: 0.9003 - val_loss: 0.3832 - val_acc: 0.9002\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3433 - acc: 0.9003 - val_loss: 0.3767 - val_acc: 0.9001\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3421 - acc: 0.9003 - val_loss: 0.3774 - val_acc: 0.9000\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3404 - acc: 0.9005 - val_loss: 0.3778 - val_acc: 0.9000\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3401 - acc: 0.9005 - val_loss: 0.3763 - val_acc: 0.9001\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3390 - acc: 0.9006 - val_loss: 0.3810 - val_acc: 0.9004\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3389 - acc: 0.9007 - val_loss: 0.3758 - val_acc: 0.9000\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3368 - acc: 0.9005 - val_loss: 0.3705 - val_acc: 0.9004\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3362 - acc: 0.9007 - val_loss: 0.3712 - val_acc: 0.9002\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3351 - acc: 0.9009 - val_loss: 0.3675 - val_acc: 0.9002\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3345 - acc: 0.9009 - val_loss: 0.3672 - val_acc: 0.9002\n",
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3340 - acc: 0.9011 - val_loss: 0.3691 - val_acc: 0.9005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3327 - acc: 0.9013 - val_loss: 0.3671 - val_acc: 0.9002\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3324 - acc: 0.9011 - val_loss: 0.3638 - val_acc: 0.9002\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3321 - acc: 0.9013 - val_loss: 0.3708 - val_acc: 0.9004\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3321 - acc: 0.9012 - val_loss: 0.3734 - val_acc: 0.9007\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3318 - acc: 0.9013 - val_loss: 0.3661 - val_acc: 0.9006\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3305 - acc: 0.9015 - val_loss: 0.3638 - val_acc: 0.9008\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3298 - acc: 0.9015 - val_loss: 0.3634 - val_acc: 0.9001\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3294 - acc: 0.9015 - val_loss: 0.3641 - val_acc: 0.9002\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3290 - acc: 0.9017 - val_loss: 0.3643 - val_acc: 0.9006\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3280 - acc: 0.9018 - val_loss: 0.3635 - val_acc: 0.9008\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3277 - acc: 0.9018 - val_loss: 0.3635 - val_acc: 0.9009\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3272 - acc: 0.9018 - val_loss: 0.3632 - val_acc: 0.9009\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3270 - acc: 0.9017 - val_loss: 0.3779 - val_acc: 0.8982\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3270 - acc: 0.9019 - val_loss: 0.3624 - val_acc: 0.9010\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3257 - acc: 0.9022 - val_loss: 0.3578 - val_acc: 0.9005\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3248 - acc: 0.9022 - val_loss: 0.3590 - val_acc: 0.9010\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3253 - acc: 0.9020 - val_loss: 0.3692 - val_acc: 0.8992\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3246 - acc: 0.9023 - val_loss: 0.3559 - val_acc: 0.9010\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3243 - acc: 0.9023 - val_loss: 0.3578 - val_acc: 0.9008\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3234 - acc: 0.9025 - val_loss: 0.3577 - val_acc: 0.9009\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3238 - acc: 0.9022 - val_loss: 0.3604 - val_acc: 0.9011\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3237 - acc: 0.9026 - val_loss: 0.3589 - val_acc: 0.9006\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3227 - acc: 0.9025 - val_loss: 0.3554 - val_acc: 0.9015\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3219 - acc: 0.9029 - val_loss: 0.3572 - val_acc: 0.9003\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3218 - acc: 0.9028 - val_loss: 0.3582 - val_acc: 0.9007\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3223 - acc: 0.9028 - val_loss: 0.3535 - val_acc: 0.9014\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3213 - acc: 0.9031 - val_loss: 0.3527 - val_acc: 0.9012\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3207 - acc: 0.9035 - val_loss: 0.3507 - val_acc: 0.9017\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3204 - acc: 0.9032 - val_loss: 0.3523 - val_acc: 0.9014\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3203 - acc: 0.9031 - val_loss: 0.3581 - val_acc: 0.9008\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3197 - acc: 0.9033 - val_loss: 0.3532 - val_acc: 0.9016\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3196 - acc: 0.9033 - val_loss: 0.3495 - val_acc: 0.9011\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3197 - acc: 0.9034 - val_loss: 0.3555 - val_acc: 0.9012\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3184 - acc: 0.9037 - val_loss: 0.3547 - val_acc: 0.9016\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3183 - acc: 0.9036 - val_loss: 0.3583 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 5s 428us/step - loss: 1.2738 - acc: 0.5587 - val_loss: 0.9412 - val_acc: 0.7819\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.7080 - acc: 0.8741 - val_loss: 0.5464 - val_acc: 0.8993\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4774 - acc: 0.8999 - val_loss: 0.4608 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4269 - acc: 0.9000 - val_loss: 0.4413 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4131 - acc: 0.9000 - val_loss: 0.4352 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4083 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4062 - acc: 0.9000 - val_loss: 0.4291 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4036 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4342 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4326 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4003 - acc: 0.9000 - val_loss: 0.4274 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3994 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3979 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3965 - acc: 0.9000 - val_loss: 0.4346 - val_acc: 0.9000\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3945 - acc: 0.9000 - val_loss: 0.4303 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3932 - acc: 0.9000 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3913 - acc: 0.9000 - val_loss: 0.4289 - val_acc: 0.9000\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3892 - acc: 0.9000 - val_loss: 0.4339 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3876 - acc: 0.9000 - val_loss: 0.4291 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3851 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3828 - acc: 0.9000 - val_loss: 0.4238 - val_acc: 0.9000\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3797 - acc: 0.9000 - val_loss: 0.4231 - val_acc: 0.9000\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3768 - acc: 0.9000 - val_loss: 0.4190 - val_acc: 0.9000\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3736 - acc: 0.9000 - val_loss: 0.4213 - val_acc: 0.9000\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3706 - acc: 0.9000 - val_loss: 0.4145 - val_acc: 0.9000\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3684 - acc: 0.9000 - val_loss: 0.4106 - val_acc: 0.9000\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3661 - acc: 0.9000 - val_loss: 0.4018 - val_acc: 0.9000\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3643 - acc: 0.9000 - val_loss: 0.4120 - val_acc: 0.9000\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3627 - acc: 0.9000 - val_loss: 0.4052 - val_acc: 0.9000\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3606 - acc: 0.9000 - val_loss: 0.4025 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3593 - acc: 0.9000 - val_loss: 0.4015 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3578 - acc: 0.9000 - val_loss: 0.3954 - val_acc: 0.9000\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3562 - acc: 0.9000 - val_loss: 0.3961 - val_acc: 0.9000\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3541 - acc: 0.9000 - val_loss: 0.3987 - val_acc: 0.9000\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3528 - acc: 0.9000 - val_loss: 0.3954 - val_acc: 0.9000\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3512 - acc: 0.9000 - val_loss: 0.3878 - val_acc: 0.9000\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3501 - acc: 0.9000 - val_loss: 0.3879 - val_acc: 0.9000\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3494 - acc: 0.9000 - val_loss: 0.3896 - val_acc: 0.9000\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3472 - acc: 0.9000 - val_loss: 0.3875 - val_acc: 0.9000\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3456 - acc: 0.9000 - val_loss: 0.3838 - val_acc: 0.9000\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3446 - acc: 0.9000 - val_loss: 0.3828 - val_acc: 0.9000\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3429 - acc: 0.9001 - val_loss: 0.3802 - val_acc: 0.9001\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3417 - acc: 0.9001 - val_loss: 0.3786 - val_acc: 0.9000\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3407 - acc: 0.9002 - val_loss: 0.3791 - val_acc: 0.9001\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3396 - acc: 0.9003 - val_loss: 0.3766 - val_acc: 0.9001\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3387 - acc: 0.9004 - val_loss: 0.3774 - val_acc: 0.9002\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3374 - acc: 0.9005 - val_loss: 0.3750 - val_acc: 0.9001\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3365 - acc: 0.9006 - val_loss: 0.3838 - val_acc: 0.9004\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3354 - acc: 0.9007 - val_loss: 0.3728 - val_acc: 0.9001\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3348 - acc: 0.9008 - val_loss: 0.3729 - val_acc: 0.9007\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3339 - acc: 0.9010 - val_loss: 0.3707 - val_acc: 0.9003\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3333 - acc: 0.9010 - val_loss: 0.3707 - val_acc: 0.9002\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3323 - acc: 0.9012 - val_loss: 0.3700 - val_acc: 0.9006\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3315 - acc: 0.9014 - val_loss: 0.3757 - val_acc: 0.9006\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3315 - acc: 0.9016 - val_loss: 0.3684 - val_acc: 0.9006\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3303 - acc: 0.9017 - val_loss: 0.3691 - val_acc: 0.9007\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3297 - acc: 0.9019 - val_loss: 0.3693 - val_acc: 0.9009\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3291 - acc: 0.9018 - val_loss: 0.3631 - val_acc: 0.9008\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3285 - acc: 0.9020 - val_loss: 0.3663 - val_acc: 0.9006\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3277 - acc: 0.9020 - val_loss: 0.3681 - val_acc: 0.9007\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3269 - acc: 0.9026 - val_loss: 0.3672 - val_acc: 0.9010\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3263 - acc: 0.9025 - val_loss: 0.3671 - val_acc: 0.9010\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3262 - acc: 0.9025 - val_loss: 0.3589 - val_acc: 0.9004\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3254 - acc: 0.9025 - val_loss: 0.3635 - val_acc: 0.9012\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3242 - acc: 0.9030 - val_loss: 0.3658 - val_acc: 0.9009\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3238 - acc: 0.9029 - val_loss: 0.3624 - val_acc: 0.9013\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3235 - acc: 0.9030 - val_loss: 0.3595 - val_acc: 0.9008\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3229 - acc: 0.9029 - val_loss: 0.3606 - val_acc: 0.9006\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3224 - acc: 0.9031 - val_loss: 0.3615 - val_acc: 0.9012\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3221 - acc: 0.9031 - val_loss: 0.3601 - val_acc: 0.9011\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3217 - acc: 0.9033 - val_loss: 0.3608 - val_acc: 0.9012\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3210 - acc: 0.9032 - val_loss: 0.3602 - val_acc: 0.9013\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3206 - acc: 0.9033 - val_loss: 0.3597 - val_acc: 0.9015\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3201 - acc: 0.9036 - val_loss: 0.3656 - val_acc: 0.9010\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3195 - acc: 0.9035 - val_loss: 0.3574 - val_acc: 0.9017\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3195 - acc: 0.9037 - val_loss: 0.3601 - val_acc: 0.9011\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3188 - acc: 0.9035 - val_loss: 0.3581 - val_acc: 0.9013\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3189 - acc: 0.9040 - val_loss: 0.3567 - val_acc: 0.9016\n",
      "Epoch 81/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3179 - acc: 0.9039 - val_loss: 0.3642 - val_acc: 0.8995\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3175 - acc: 0.9038 - val_loss: 0.3569 - val_acc: 0.9016\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3172 - acc: 0.9040 - val_loss: 0.3573 - val_acc: 0.9018\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3167 - acc: 0.9043 - val_loss: 0.3564 - val_acc: 0.9011\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3167 - acc: 0.9043 - val_loss: 0.3568 - val_acc: 0.9015\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3158 - acc: 0.9043 - val_loss: 0.3576 - val_acc: 0.9012\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3154 - acc: 0.9043 - val_loss: 0.3585 - val_acc: 0.9011\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3154 - acc: 0.9043 - val_loss: 0.3591 - val_acc: 0.9004\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3151 - acc: 0.9046 - val_loss: 0.3577 - val_acc: 0.9018\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3145 - acc: 0.9046 - val_loss: 0.3582 - val_acc: 0.9016\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3143 - acc: 0.9044 - val_loss: 0.3513 - val_acc: 0.9016\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3136 - acc: 0.9048 - val_loss: 0.3548 - val_acc: 0.9016\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3136 - acc: 0.9047 - val_loss: 0.3585 - val_acc: 0.9012\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3137 - acc: 0.9048 - val_loss: 0.3537 - val_acc: 0.9011\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3130 - acc: 0.9044 - val_loss: 0.3533 - val_acc: 0.9014\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3126 - acc: 0.9048 - val_loss: 0.3576 - val_acc: 0.9004\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 1s 99us/step - loss: 0.3119 - acc: 0.9050 - val_loss: 0.3564 - val_acc: 0.9012\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3121 - acc: 0.9048 - val_loss: 0.3612 - val_acc: 0.8997\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3117 - acc: 0.9053 - val_loss: 0.3472 - val_acc: 0.9025\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.3115 - acc: 0.9052 - val_loss: 0.3499 - val_acc: 0.9016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_17 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_15/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_16/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_19 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_17/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_18/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_21 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_19/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_20/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_23 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_21/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_22/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_25 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_23/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_24/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_27 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_25/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_26/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_29 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_27/concat:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'concatenate_28/concat:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_17 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_32:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_33:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_19 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_36:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_37:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_21 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_40:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_41:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_23 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_44:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_45:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_25 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_48:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_49:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_27 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_52:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_53:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_29 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_56:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'input_57:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9411856245994568\n",
      "1 0.5463625854253769\n",
      "2 0.46083818256855014\n",
      "3 0.44130665719509127\n",
      "4 0.4352327182888985\n",
      "5 0.4294524961709976\n",
      "6 0.4291132295131683\n",
      "7 0.437650151848793\n",
      "8 0.43039516657590865\n",
      "9 0.43416400253772736\n",
      "10 0.42976287692785264\n",
      "11 0.4326444581151009\n",
      "12 0.4274073937535286\n",
      "13 0.42853953927755356\n",
      "14 0.43008762151002883\n",
      "15 0.4346488571166992\n",
      "16 0.4302500328421593\n",
      "17 0.4381691437959671\n",
      "18 0.428895004093647\n",
      "19 0.433908648788929\n",
      "20 0.42909411251544954\n",
      "21 0.4266330599784851\n",
      "22 0.42376160323619844\n",
      "23 0.4230718034505844\n",
      "24 0.41899532794952393\n",
      "25 0.42134086787700653\n",
      "26 0.41452830225229265\n",
      "27 0.4106198936700821\n",
      "28 0.4017614659667015\n",
      "29 0.4120158639550209\n",
      "30 0.40518855810165405\n",
      "31 0.40252254635095597\n",
      "32 0.4014767900109291\n",
      "33 0.39542673259973526\n",
      "34 0.3960850512981415\n",
      "35 0.3986848142743111\n",
      "36 0.39540442943572995\n",
      "37 0.3878081229329109\n",
      "38 0.38786457031965255\n",
      "39 0.389577271938324\n",
      "40 0.3874805697798729\n",
      "41 0.3838341698050499\n",
      "42 0.38280821561813355\n",
      "43 0.3801615509390831\n",
      "44 0.3786107501387596\n",
      "45 0.3790872609615326\n",
      "46 0.3765789824724197\n",
      "47 0.3773843565583229\n",
      "48 0.37497080594301224\n",
      "49 0.3837737137079239\n",
      "50 0.3727964410185814\n",
      "51 0.37286721527576444\n",
      "52 0.3707432571053505\n",
      "53 0.370682812333107\n",
      "54 0.36997293055057523\n",
      "55 0.3756582084298134\n",
      "56 0.3684058219194412\n",
      "57 0.3690752238035202\n",
      "58 0.3693086203932762\n",
      "59 0.3631325915455818\n",
      "60 0.36631160527467727\n",
      "61 0.3681066244840622\n",
      "62 0.36716519355773924\n",
      "63 0.3670924469828606\n",
      "64 0.35891590803861617\n",
      "65 0.3634603714942932\n",
      "66 0.3657550236582756\n",
      "67 0.3624147868156433\n",
      "68 0.3594503858685493\n",
      "69 0.3605915099382401\n",
      "70 0.3615257641673088\n",
      "71 0.3601469251513481\n",
      "72 0.36082094848155977\n",
      "73 0.36015479564666747\n",
      "74 0.3596814581751823\n",
      "75 0.36555293440818787\n",
      "76 0.3573803836107254\n",
      "77 0.36007786482572557\n",
      "78 0.35813174813985826\n",
      "79 0.356740680038929\n",
      "80 0.3641567975282669\n",
      "81 0.3568794468045235\n",
      "82 0.35725108206272127\n",
      "83 0.3564366778731346\n",
      "84 0.35675296664237977\n",
      "85 0.35761563420295717\n",
      "86 0.35846679359674455\n",
      "87 0.3590648713707924\n",
      "88 0.35772746682167056\n",
      "89 0.35816316336393356\n",
      "90 0.3512976399064064\n",
      "91 0.35484130203723907\n",
      "92 0.35845184087753296\n",
      "93 0.35370325356721877\n",
      "94 0.35331465661525724\n",
      "95 0.3576454448699951\n",
      "96 0.3564418470859528\n",
      "97 0.3612242370843887\n",
      "98 0.34719243586063386\n",
      "99 0.3498776277899742\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 5s 442us/step - loss: 1.0965 - acc: 0.6257 - val_loss: 0.6575 - val_acc: 0.8885\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4963 - acc: 0.8986 - val_loss: 0.4596 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4169 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 1s 100us/step - loss: 0.4080 - acc: 0.9000 - val_loss: 0.4383 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4052 - acc: 0.9000 - val_loss: 0.4217 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 5s 455us/step - loss: 1.1164 - acc: 0.6383 - val_loss: 0.6786 - val_acc: 0.8809\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.5162 - acc: 0.8984 - val_loss: 0.4463 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4219 - acc: 0.9000 - val_loss: 0.4349 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4099 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4063 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4051 - acc: 0.9000 - val_loss: 0.4318 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4322 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.4013 - acc: 0.9000 - val_loss: 0.4356 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 101us/step - loss: 0.3994 - acc: 0.9000 - val_loss: 0.4378 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 9s 770us/step - loss: 1.0859 - acc: 0.6349 - val_loss: 0.6436 - val_acc: 0.8875\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4963 - acc: 0.8985 - val_loss: 0.4585 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4184 - acc: 0.9000 - val_loss: 0.4330 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4090 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4323 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4024 - acc: 0.9000 - val_loss: 0.4360 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4011 - acc: 0.9000 - val_loss: 0.4364 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3999 - acc: 0.9000 - val_loss: 0.4316 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3977 - acc: 0.9000 - val_loss: 0.4339 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3951 - acc: 0.9000 - val_loss: 0.4417 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3940 - acc: 0.9000 - val_loss: 0.4387 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3911 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3900 - acc: 0.9000 - val_loss: 0.4350 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3876 - acc: 0.9000 - val_loss: 0.4247 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3860 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3838 - acc: 0.9000 - val_loss: 0.4265 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3809 - acc: 0.9000 - val_loss: 0.4253 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3769 - acc: 0.9000 - val_loss: 0.4182 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3720 - acc: 0.9000 - val_loss: 0.4213 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 6s 477us/step - loss: 1.0507 - acc: 0.6690 - val_loss: 0.6147 - val_acc: 0.8930\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4829 - acc: 0.8993 - val_loss: 0.4438 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4160 - acc: 0.9000 - val_loss: 0.4355 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4086 - acc: 0.9000 - val_loss: 0.4313 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4281 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4269 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4031 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4008 - acc: 0.9000 - val_loss: 0.4348 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3994 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.4349 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3937 - acc: 0.9000 - val_loss: 0.4390 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3906 - acc: 0.9000 - val_loss: 0.4293 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3867 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3840 - acc: 0.9000 - val_loss: 0.4307 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3798 - acc: 0.9000 - val_loss: 0.4162 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3750 - acc: 0.9000 - val_loss: 0.4186 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3699 - acc: 0.9000 - val_loss: 0.4116 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3645 - acc: 0.9000 - val_loss: 0.4114 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3606 - acc: 0.9000 - val_loss: 0.4015 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3564 - acc: 0.9000 - val_loss: 0.4026 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3538 - acc: 0.9000 - val_loss: 0.3981 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3491 - acc: 0.9001 - val_loss: 0.3971 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3469 - acc: 0.9001 - val_loss: 0.3880 - val_acc: 0.8999\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3434 - acc: 0.9002 - val_loss: 0.3899 - val_acc: 0.9001\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3413 - acc: 0.9002 - val_loss: 0.3850 - val_acc: 0.9003\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3391 - acc: 0.9005 - val_loss: 0.3864 - val_acc: 0.9006\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.3373 - acc: 0.9006 - val_loss: 0.3831 - val_acc: 0.8999\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3360 - acc: 0.9008 - val_loss: 0.3770 - val_acc: 0.9001\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 6s 486us/step - loss: 1.0747 - acc: 0.6359 - val_loss: 0.6419 - val_acc: 0.8743\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4903 - acc: 0.8970 - val_loss: 0.4443 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4139 - acc: 0.9000 - val_loss: 0.4356 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4073 - acc: 0.9000 - val_loss: 0.4323 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4303 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4038 - acc: 0.9000 - val_loss: 0.4256 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4024 - acc: 0.9000 - val_loss: 0.4255 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.4017 - acc: 0.9000 - val_loss: 0.4207 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4004 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3990 - acc: 0.9000 - val_loss: 0.4349 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.4331 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3940 - acc: 0.9000 - val_loss: 0.4307 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3920 - acc: 0.9000 - val_loss: 0.4222 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3895 - acc: 0.9000 - val_loss: 0.4358 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3862 - acc: 0.9000 - val_loss: 0.4384 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3832 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3800 - acc: 0.9000 - val_loss: 0.4237 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3757 - acc: 0.9000 - val_loss: 0.4183 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3709 - acc: 0.9000 - val_loss: 0.4084 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3664 - acc: 0.9000 - val_loss: 0.4167 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3630 - acc: 0.9000 - val_loss: 0.3991 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3593 - acc: 0.9000 - val_loss: 0.4078 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3567 - acc: 0.9000 - val_loss: 0.4028 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3541 - acc: 0.9000 - val_loss: 0.4076 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3520 - acc: 0.9000 - val_loss: 0.3929 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3487 - acc: 0.9000 - val_loss: 0.3948 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3459 - acc: 0.9000 - val_loss: 0.3923 - val_acc: 0.9001\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3429 - acc: 0.9001 - val_loss: 0.3829 - val_acc: 0.9001\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3398 - acc: 0.9002 - val_loss: 0.3833 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3376 - acc: 0.9003 - val_loss: 0.3748 - val_acc: 0.8999\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3362 - acc: 0.9004 - val_loss: 0.3762 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3337 - acc: 0.9005 - val_loss: 0.3759 - val_acc: 0.9002\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3316 - acc: 0.9007 - val_loss: 0.3755 - val_acc: 0.9002\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3310 - acc: 0.9009 - val_loss: 0.3757 - val_acc: 0.9001\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3290 - acc: 0.9011 - val_loss: 0.3710 - val_acc: 0.9001\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3277 - acc: 0.9011 - val_loss: 0.3752 - val_acc: 0.9004\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3263 - acc: 0.9012 - val_loss: 0.3694 - val_acc: 0.9004\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3251 - acc: 0.9015 - val_loss: 0.3716 - val_acc: 0.9004\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3239 - acc: 0.9014 - val_loss: 0.3670 - val_acc: 0.9004\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3229 - acc: 0.9021 - val_loss: 0.3690 - val_acc: 0.9002\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3215 - acc: 0.9021 - val_loss: 0.3651 - val_acc: 0.9002\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3201 - acc: 0.9023 - val_loss: 0.3743 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3190 - acc: 0.9027 - val_loss: 0.3702 - val_acc: 0.8993\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3180 - acc: 0.9027 - val_loss: 0.3634 - val_acc: 0.9007\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3169 - acc: 0.9028 - val_loss: 0.3625 - val_acc: 0.8993\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3159 - acc: 0.9031 - val_loss: 0.3618 - val_acc: 0.9006\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3152 - acc: 0.9029 - val_loss: 0.3729 - val_acc: 0.9007\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3143 - acc: 0.9033 - val_loss: 0.3584 - val_acc: 0.9008\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3132 - acc: 0.9032 - val_loss: 0.3602 - val_acc: 0.9015\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3122 - acc: 0.9037 - val_loss: 0.3658 - val_acc: 0.9005\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 6s 493us/step - loss: 1.0623 - acc: 0.6642 - val_loss: 0.6158 - val_acc: 0.8948\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4849 - acc: 0.8992 - val_loss: 0.4543 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4165 - acc: 0.9000 - val_loss: 0.4362 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4088 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4305 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4039 - acc: 0.9000 - val_loss: 0.4245 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.4012 - acc: 0.9000 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3991 - acc: 0.9000 - val_loss: 0.4291 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3967 - acc: 0.9000 - val_loss: 0.4317 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3943 - acc: 0.9000 - val_loss: 0.4379 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3923 - acc: 0.9000 - val_loss: 0.4343 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3898 - acc: 0.9000 - val_loss: 0.4365 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3871 - acc: 0.9000 - val_loss: 0.4249 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3849 - acc: 0.9000 - val_loss: 0.4250 - val_acc: 0.9000\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3827 - acc: 0.9000 - val_loss: 0.4379 - val_acc: 0.9000\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3802 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3780 - acc: 0.9000 - val_loss: 0.4251 - val_acc: 0.9000\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3757 - acc: 0.9000 - val_loss: 0.4177 - val_acc: 0.9000\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3730 - acc: 0.9000 - val_loss: 0.4310 - val_acc: 0.9000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3681 - acc: 0.9000 - val_loss: 0.4093 - val_acc: 0.9000\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3632 - acc: 0.9000 - val_loss: 0.4033 - val_acc: 0.9000\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3604 - acc: 0.9000 - val_loss: 0.4014 - val_acc: 0.9000\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3568 - acc: 0.9000 - val_loss: 0.4034 - val_acc: 0.9000\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3545 - acc: 0.9000 - val_loss: 0.3935 - val_acc: 0.9000\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3518 - acc: 0.9000 - val_loss: 0.3936 - val_acc: 0.9000\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3495 - acc: 0.9000 - val_loss: 0.3966 - val_acc: 0.9000\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3475 - acc: 0.9000 - val_loss: 0.3840 - val_acc: 0.9000\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3459 - acc: 0.9000 - val_loss: 0.3908 - val_acc: 0.9000\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3436 - acc: 0.9000 - val_loss: 0.3886 - val_acc: 0.9000\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3414 - acc: 0.9000 - val_loss: 0.3868 - val_acc: 0.9001\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3400 - acc: 0.9000 - val_loss: 0.3901 - val_acc: 0.9003\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3382 - acc: 0.9001 - val_loss: 0.3726 - val_acc: 0.9000\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3367 - acc: 0.9002 - val_loss: 0.3815 - val_acc: 0.9000\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3351 - acc: 0.9002 - val_loss: 0.3780 - val_acc: 0.9001\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3336 - acc: 0.9006 - val_loss: 0.3767 - val_acc: 0.9002\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3325 - acc: 0.9006 - val_loss: 0.3770 - val_acc: 0.9006\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3318 - acc: 0.9005 - val_loss: 0.3747 - val_acc: 0.9004\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3301 - acc: 0.9007 - val_loss: 0.3673 - val_acc: 0.9001\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3291 - acc: 0.9008 - val_loss: 0.3780 - val_acc: 0.9006\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3281 - acc: 0.9009 - val_loss: 0.3702 - val_acc: 0.9000\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3276 - acc: 0.9012 - val_loss: 0.3635 - val_acc: 0.9003\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3266 - acc: 0.9015 - val_loss: 0.3686 - val_acc: 0.9007\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3255 - acc: 0.9013 - val_loss: 0.3720 - val_acc: 0.9005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3241 - acc: 0.9016 - val_loss: 0.3668 - val_acc: 0.9003\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3234 - acc: 0.9017 - val_loss: 0.3651 - val_acc: 0.9003\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3225 - acc: 0.9019 - val_loss: 0.3617 - val_acc: 0.9004\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3216 - acc: 0.9020 - val_loss: 0.3588 - val_acc: 0.9009\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3211 - acc: 0.9022 - val_loss: 0.3667 - val_acc: 0.9008\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3199 - acc: 0.9024 - val_loss: 0.3671 - val_acc: 0.9008\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3192 - acc: 0.9021 - val_loss: 0.3598 - val_acc: 0.9011\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3183 - acc: 0.9028 - val_loss: 0.3694 - val_acc: 0.9009\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3174 - acc: 0.9028 - val_loss: 0.3629 - val_acc: 0.9007\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3165 - acc: 0.9027 - val_loss: 0.3637 - val_acc: 0.9009\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3156 - acc: 0.9031 - val_loss: 0.3607 - val_acc: 0.9009\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3149 - acc: 0.9029 - val_loss: 0.3605 - val_acc: 0.9012\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3139 - acc: 0.9033 - val_loss: 0.3620 - val_acc: 0.9012\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3130 - acc: 0.9033 - val_loss: 0.3588 - val_acc: 0.9014\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3119 - acc: 0.9035 - val_loss: 0.3635 - val_acc: 0.9006\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3109 - acc: 0.9037 - val_loss: 0.3712 - val_acc: 0.9003\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3106 - acc: 0.9037 - val_loss: 0.3567 - val_acc: 0.9013\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3100 - acc: 0.9037 - val_loss: 0.3647 - val_acc: 0.9012\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3090 - acc: 0.9042 - val_loss: 0.3685 - val_acc: 0.9008\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3083 - acc: 0.9040 - val_loss: 0.3630 - val_acc: 0.9003\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3064 - acc: 0.9045 - val_loss: 0.3621 - val_acc: 0.9013\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.3063 - acc: 0.9044 - val_loss: 0.3672 - val_acc: 0.9010\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3048 - acc: 0.9045 - val_loss: 0.3657 - val_acc: 0.9008\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3045 - acc: 0.9046 - val_loss: 0.3627 - val_acc: 0.9015\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3034 - acc: 0.9045 - val_loss: 0.3600 - val_acc: 0.9014\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3028 - acc: 0.9049 - val_loss: 0.3689 - val_acc: 0.9003\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3014 - acc: 0.9049 - val_loss: 0.3720 - val_acc: 0.8995\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3006 - acc: 0.9052 - val_loss: 0.3661 - val_acc: 0.8999\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2998 - acc: 0.9052 - val_loss: 0.3640 - val_acc: 0.8987\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2988 - acc: 0.9055 - val_loss: 0.3622 - val_acc: 0.9008\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2974 - acc: 0.9058 - val_loss: 0.3669 - val_acc: 0.9004\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2969 - acc: 0.9058 - val_loss: 0.3675 - val_acc: 0.9001\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2959 - acc: 0.9060 - val_loss: 0.3687 - val_acc: 0.8993\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2952 - acc: 0.9062 - val_loss: 0.3602 - val_acc: 0.9009\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2935 - acc: 0.9065 - val_loss: 0.3644 - val_acc: 0.9018\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2928 - acc: 0.9063 - val_loss: 0.3661 - val_acc: 0.8998\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 6s 516us/step - loss: 1.0876 - acc: 0.6599 - val_loss: 0.6073 - val_acc: 0.8929\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4807 - acc: 0.8989 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4147 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4079 - acc: 0.9000 - val_loss: 0.4374 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4060 - acc: 0.9000 - val_loss: 0.4238 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4045 - acc: 0.9000 - val_loss: 0.4301 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4026 - acc: 0.9000 - val_loss: 0.4242 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4009 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3996 - acc: 0.9000 - val_loss: 0.4398 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3976 - acc: 0.9000 - val_loss: 0.4333 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3947 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3924 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3899 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3872 - acc: 0.9000 - val_loss: 0.4252 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3836 - acc: 0.9000 - val_loss: 0.4208 - val_acc: 0.9000\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3797 - acc: 0.9000 - val_loss: 0.4270 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3746 - acc: 0.9000 - val_loss: 0.4191 - val_acc: 0.9000\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3708 - acc: 0.9000 - val_loss: 0.4271 - val_acc: 0.9000\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3665 - acc: 0.9000 - val_loss: 0.4087 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3625 - acc: 0.9000 - val_loss: 0.4062 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3600 - acc: 0.9000 - val_loss: 0.4035 - val_acc: 0.9000\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3553 - acc: 0.9000 - val_loss: 0.3964 - val_acc: 0.9000\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3521 - acc: 0.9000 - val_loss: 0.3949 - val_acc: 0.9000\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3478 - acc: 0.9000 - val_loss: 0.3935 - val_acc: 0.8999\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3456 - acc: 0.9001 - val_loss: 0.3856 - val_acc: 0.8998\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3430 - acc: 0.9002 - val_loss: 0.3887 - val_acc: 0.9001\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3409 - acc: 0.9003 - val_loss: 0.3871 - val_acc: 0.9000\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3385 - acc: 0.9005 - val_loss: 0.3782 - val_acc: 0.8998\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3366 - acc: 0.9007 - val_loss: 0.3801 - val_acc: 0.8998\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3360 - acc: 0.9005 - val_loss: 0.3752 - val_acc: 0.8998\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3342 - acc: 0.9010 - val_loss: 0.3746 - val_acc: 0.8996\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3323 - acc: 0.9011 - val_loss: 0.3789 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3302 - acc: 0.9016 - val_loss: 0.3771 - val_acc: 0.9001\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3289 - acc: 0.9017 - val_loss: 0.3747 - val_acc: 0.8996\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3281 - acc: 0.9017 - val_loss: 0.3689 - val_acc: 0.9000\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3269 - acc: 0.9018 - val_loss: 0.3748 - val_acc: 0.9001\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3262 - acc: 0.9021 - val_loss: 0.3674 - val_acc: 0.9001\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3244 - acc: 0.9025 - val_loss: 0.3725 - val_acc: 0.9000\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3236 - acc: 0.9024 - val_loss: 0.3581 - val_acc: 0.9002\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3222 - acc: 0.9025 - val_loss: 0.3678 - val_acc: 0.9002\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3205 - acc: 0.9030 - val_loss: 0.3679 - val_acc: 0.9004\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.3203 - acc: 0.9030 - val_loss: 0.3655 - val_acc: 0.9005\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.3193 - acc: 0.9031 - val_loss: 0.3674 - val_acc: 0.9003\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.3179 - acc: 0.9035 - val_loss: 0.3631 - val_acc: 0.9011\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3168 - acc: 0.9036 - val_loss: 0.3608 - val_acc: 0.9012\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.3156 - acc: 0.9039 - val_loss: 0.3739 - val_acc: 0.9005\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3148 - acc: 0.9039 - val_loss: 0.3662 - val_acc: 0.9006\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3138 - acc: 0.9040 - val_loss: 0.3657 - val_acc: 0.8993\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3135 - acc: 0.9043 - val_loss: 0.3666 - val_acc: 0.9009\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3120 - acc: 0.9044 - val_loss: 0.3589 - val_acc: 0.9011\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3116 - acc: 0.9047 - val_loss: 0.3622 - val_acc: 0.9009\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3100 - acc: 0.9053 - val_loss: 0.3612 - val_acc: 0.9007\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3098 - acc: 0.9052 - val_loss: 0.3547 - val_acc: 0.9017\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3084 - acc: 0.9053 - val_loss: 0.3540 - val_acc: 0.9018\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3077 - acc: 0.9055 - val_loss: 0.3641 - val_acc: 0.9004\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3069 - acc: 0.9057 - val_loss: 0.3661 - val_acc: 0.9009\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3062 - acc: 0.9054 - val_loss: 0.3626 - val_acc: 0.9000\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3053 - acc: 0.9060 - val_loss: 0.3668 - val_acc: 0.8990\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3044 - acc: 0.9057 - val_loss: 0.3529 - val_acc: 0.9023\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3032 - acc: 0.9062 - val_loss: 0.3672 - val_acc: 0.9008\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3020 - acc: 0.9067 - val_loss: 0.3648 - val_acc: 0.9013\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3012 - acc: 0.9069 - val_loss: 0.3633 - val_acc: 0.9004\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.3006 - acc: 0.9067 - val_loss: 0.3634 - val_acc: 0.9006\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2998 - acc: 0.9072 - val_loss: 0.3638 - val_acc: 0.8998\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2986 - acc: 0.9072 - val_loss: 0.3660 - val_acc: 0.8992\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2982 - acc: 0.9074 - val_loss: 0.3612 - val_acc: 0.9012\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2967 - acc: 0.9074 - val_loss: 0.3618 - val_acc: 0.9008\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2959 - acc: 0.9075 - val_loss: 0.3629 - val_acc: 0.8999\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2945 - acc: 0.9079 - val_loss: 0.3634 - val_acc: 0.8996\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2939 - acc: 0.9083 - val_loss: 0.3656 - val_acc: 0.8998\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.2933 - acc: 0.9081 - val_loss: 0.3651 - val_acc: 0.8986\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2918 - acc: 0.9086 - val_loss: 0.3665 - val_acc: 0.8997\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.2904 - acc: 0.9090 - val_loss: 0.3663 - val_acc: 0.9005\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2898 - acc: 0.9091 - val_loss: 0.3667 - val_acc: 0.9000\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2881 - acc: 0.9091 - val_loss: 0.3700 - val_acc: 0.9002\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2878 - acc: 0.9093 - val_loss: 0.3769 - val_acc: 0.8970\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2867 - acc: 0.9094 - val_loss: 0.3891 - val_acc: 0.8948\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2850 - acc: 0.9097 - val_loss: 0.3764 - val_acc: 0.8977\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2845 - acc: 0.9100 - val_loss: 0.3771 - val_acc: 0.8991\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2838 - acc: 0.9099 - val_loss: 0.3836 - val_acc: 0.8943\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2825 - acc: 0.9106 - val_loss: 0.3786 - val_acc: 0.9002\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.2813 - acc: 0.9107 - val_loss: 0.3778 - val_acc: 0.8975\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.2803 - acc: 0.9107 - val_loss: 0.3707 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.2791 - acc: 0.9115 - val_loss: 0.3785 - val_acc: 0.8984\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.2781 - acc: 0.9115 - val_loss: 0.3815 - val_acc: 0.8974\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2769 - acc: 0.9120 - val_loss: 0.3789 - val_acc: 0.8992\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.2758 - acc: 0.9120 - val_loss: 0.3860 - val_acc: 0.8984\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.2745 - acc: 0.9128 - val_loss: 0.3921 - val_acc: 0.8995\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.2732 - acc: 0.9129 - val_loss: 0.3867 - val_acc: 0.8962\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.2720 - acc: 0.9133 - val_loss: 0.3866 - val_acc: 0.8959\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.2705 - acc: 0.9137 - val_loss: 0.3946 - val_acc: 0.8939\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.2703 - acc: 0.9137 - val_loss: 0.3854 - val_acc: 0.8974\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2683 - acc: 0.9139 - val_loss: 0.3849 - val_acc: 0.8952\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2676 - acc: 0.9141 - val_loss: 0.3916 - val_acc: 0.8925\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.2659 - acc: 0.9147 - val_loss: 0.3921 - val_acc: 0.8971\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.2647 - acc: 0.9149 - val_loss: 0.3999 - val_acc: 0.8965\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.2636 - acc: 0.9149 - val_loss: 0.3955 - val_acc: 0.8958\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2626 - acc: 0.9156 - val_loss: 0.3972 - val_acc: 0.8958\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 1s 103us/step - loss: 0.2617 - acc: 0.9159 - val_loss: 0.4049 - val_acc: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_31 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_29/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_30/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_33 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_31/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_32/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_35 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_33/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_34/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_37 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_35/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_36/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_39 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_37/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_38/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_41 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_39/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_40/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_43 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_41/concat:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'concatenate_42/concat:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_31 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_60:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_61:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_33 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_64:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_65:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_35 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_68:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_69:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_37 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_72:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_73:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_39 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_76:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_77:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_41 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_80:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_81:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_43 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_84:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_85:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6072930544614792\n",
      "1 0.43815588265657424\n",
      "2 0.4300875037908554\n",
      "3 0.4373712342977524\n",
      "4 0.4238239821791649\n",
      "5 0.43012290239334106\n",
      "6 0.43212163269519804\n",
      "7 0.42419715762138366\n",
      "8 0.4305790743231773\n",
      "9 0.4397983059287071\n",
      "10 0.43328088015317917\n",
      "11 0.43024771004915235\n",
      "12 0.43445889592170717\n",
      "13 0.4275917485356331\n",
      "14 0.42524794429540635\n",
      "15 0.4208295765519142\n",
      "16 0.42704706758260724\n",
      "17 0.4190763354301453\n",
      "18 0.42710468620061875\n",
      "19 0.40874321401119235\n",
      "20 0.40624092519283295\n",
      "21 0.4035113751888275\n",
      "22 0.39642272919416427\n",
      "23 0.3948979964852333\n",
      "24 0.39348260283470154\n",
      "25 0.38560139447450636\n",
      "26 0.38868398845195773\n",
      "27 0.38712220042943957\n",
      "28 0.37821640342473983\n",
      "29 0.38008973449468614\n",
      "30 0.37519191682338715\n",
      "31 0.3745666968822479\n",
      "32 0.37892107844352724\n",
      "33 0.37714368879795074\n",
      "34 0.3746849423646927\n",
      "35 0.368850177526474\n",
      "36 0.37484169989824295\n",
      "37 0.36744157344102857\n",
      "38 0.3724780488014221\n",
      "39 0.3580621594190598\n",
      "40 0.3677825778722763\n",
      "41 0.36792421728372576\n",
      "42 0.3654597392678261\n",
      "43 0.36739116996526716\n",
      "44 0.3630697017908096\n",
      "45 0.3608463853597641\n",
      "46 0.3739273178577423\n",
      "47 0.36620366215705874\n",
      "48 0.3656930351257324\n",
      "49 0.36660808205604556\n",
      "50 0.3589253857731819\n",
      "51 0.36217817664146423\n",
      "52 0.3611933824419975\n",
      "53 0.3546586537361145\n",
      "54 0.3540162545442581\n",
      "55 0.3640703332424164\n",
      "56 0.36610197633504865\n",
      "57 0.3626117169857025\n",
      "58 0.366759612262249\n",
      "59 0.3528693825006485\n",
      "60 0.36719990253448487\n",
      "61 0.36476663827896116\n",
      "62 0.36333172500133515\n",
      "63 0.3634223330020905\n",
      "64 0.3637661924958229\n",
      "65 0.3660497203469276\n",
      "66 0.3611978828907013\n",
      "67 0.3618119698762894\n",
      "68 0.36292906522750856\n",
      "69 0.36335295289754865\n",
      "70 0.3655796104669571\n",
      "71 0.3650871902704239\n",
      "72 0.3664861851930618\n",
      "73 0.3662719222903252\n",
      "74 0.36667669206857684\n",
      "75 0.37002370417118075\n",
      "76 0.3769102770090103\n",
      "77 0.3891157853603363\n",
      "78 0.37639850974082945\n",
      "79 0.37713460981845853\n",
      "80 0.3836334556341171\n",
      "81 0.37863622784614565\n",
      "82 0.37775288164615634\n",
      "83 0.3707178130745888\n",
      "84 0.37852379769086836\n",
      "85 0.3815369603037834\n",
      "86 0.3788999447226524\n",
      "87 0.38596726447343827\n",
      "88 0.39205218136310577\n",
      "89 0.386650986969471\n",
      "90 0.3865626621246338\n",
      "91 0.39464741706848144\n",
      "92 0.38538671225309373\n",
      "93 0.3849226298928261\n",
      "94 0.3916071608662605\n",
      "95 0.3921329379081726\n",
      "96 0.3998835277557373\n",
      "97 0.3955278313159943\n",
      "98 0.39723412960767746\n",
      "99 0.40487098425626755\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 6s 523us/step - loss: 0.9805 - acc: 0.6733 - val_loss: 0.5466 - val_acc: 0.8985\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4472 - acc: 0.8998 - val_loss: 0.4258 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 1s 106us/step - loss: 0.4100 - acc: 0.9000 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.4069 - acc: 0.9000 - val_loss: 0.4252 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 1s 105us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4454 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 6s 535us/step - loss: 0.9571 - acc: 0.6945 - val_loss: 0.5262 - val_acc: 0.8987\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.4472 - acc: 0.8998 - val_loss: 0.4443 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.4069 - acc: 0.9000 - val_loss: 0.4386 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4212 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.4041 - acc: 0.9000 - val_loss: 0.4362 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4353 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.4019 - acc: 0.9000 - val_loss: 0.4243 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4337 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.4004 - acc: 0.9000 - val_loss: 0.4250 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 7s 586us/step - loss: 0.9583 - acc: 0.6885 - val_loss: 0.5214 - val_acc: 0.8990\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4400 - acc: 0.8999 - val_loss: 0.4493 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.4098 - acc: 0.9000 - val_loss: 0.4273 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4063 - acc: 0.9000 - val_loss: 0.4325 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.4045 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.4036 - acc: 0.9000 - val_loss: 0.4263 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4381 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.4013 - acc: 0.9000 - val_loss: 0.4474 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3996 - acc: 0.9000 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3969 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3931 - acc: 0.9000 - val_loss: 0.4460 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3891 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3840 - acc: 0.9000 - val_loss: 0.4231 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3782 - acc: 0.9000 - val_loss: 0.4214 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3736 - acc: 0.9000 - val_loss: 0.4202 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3682 - acc: 0.9000 - val_loss: 0.4391 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3636 - acc: 0.9000 - val_loss: 0.4115 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3594 - acc: 0.9000 - val_loss: 0.4089 - val_acc: 0.8998\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3545 - acc: 0.9000 - val_loss: 0.4096 - val_acc: 0.8999\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 1s 107us/step - loss: 0.3502 - acc: 0.9000 - val_loss: 0.4040 - val_acc: 0.8996\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 7s 565us/step - loss: 0.9335 - acc: 0.7043 - val_loss: 0.5178 - val_acc: 0.8989\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.4415 - acc: 0.8999 - val_loss: 0.4362 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4429 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4063 - acc: 0.9000 - val_loss: 0.4399 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4051 - acc: 0.9000 - val_loss: 0.4316 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4027 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4310 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3996 - acc: 0.9000 - val_loss: 0.4445 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.4336 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3931 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3896 - acc: 0.9000 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3861 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3817 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3759 - acc: 0.9000 - val_loss: 0.4169 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3705 - acc: 0.9000 - val_loss: 0.4229 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3658 - acc: 0.9000 - val_loss: 0.4198 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3613 - acc: 0.9000 - val_loss: 0.4056 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3569 - acc: 0.9000 - val_loss: 0.4086 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3527 - acc: 0.9000 - val_loss: 0.4041 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3478 - acc: 0.9000 - val_loss: 0.3967 - val_acc: 0.8999\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3449 - acc: 0.9002 - val_loss: 0.3957 - val_acc: 0.9001\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3409 - acc: 0.9005 - val_loss: 0.3863 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3374 - acc: 0.9008 - val_loss: 0.3890 - val_acc: 0.9001\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3341 - acc: 0.9011 - val_loss: 0.3913 - val_acc: 0.8997\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3305 - acc: 0.9013 - val_loss: 0.3829 - val_acc: 0.8994\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3286 - acc: 0.9016 - val_loss: 0.3747 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3257 - acc: 0.9021 - val_loss: 0.3814 - val_acc: 0.8996\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3234 - acc: 0.9022 - val_loss: 0.3686 - val_acc: 0.9003\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3213 - acc: 0.9027 - val_loss: 0.3666 - val_acc: 0.9002\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 7s 564us/step - loss: 0.9807 - acc: 0.6769 - val_loss: 0.5322 - val_acc: 0.8991\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4481 - acc: 0.8998 - val_loss: 0.4370 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4122 - acc: 0.9000 - val_loss: 0.4258 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.4077 - acc: 0.9000 - val_loss: 0.4423 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.4037 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4024 - acc: 0.9000 - val_loss: 0.4249 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4340 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3993 - acc: 0.9000 - val_loss: 0.4350 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s 108us/step - loss: 0.3969 - acc: 0.9000 - val_loss: 0.4394 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3932 - acc: 0.9000 - val_loss: 0.4360 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3893 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3855 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3803 - acc: 0.9000 - val_loss: 0.4168 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3747 - acc: 0.9000 - val_loss: 0.4272 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3689 - acc: 0.9000 - val_loss: 0.4210 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3641 - acc: 0.9000 - val_loss: 0.4077 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3590 - acc: 0.9000 - val_loss: 0.4086 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3532 - acc: 0.9000 - val_loss: 0.3971 - val_acc: 0.8999\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3483 - acc: 0.9001 - val_loss: 0.3983 - val_acc: 0.8999\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3433 - acc: 0.9001 - val_loss: 0.3956 - val_acc: 0.9001\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3404 - acc: 0.9005 - val_loss: 0.3920 - val_acc: 0.8995\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3367 - acc: 0.9006 - val_loss: 0.3807 - val_acc: 0.8992\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3326 - acc: 0.9011 - val_loss: 0.3826 - val_acc: 0.8995\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3301 - acc: 0.9013 - val_loss: 0.3741 - val_acc: 0.8999\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3279 - acc: 0.9019 - val_loss: 0.3890 - val_acc: 0.9003\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3255 - acc: 0.9019 - val_loss: 0.3800 - val_acc: 0.9003\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3219 - acc: 0.9023 - val_loss: 0.3750 - val_acc: 0.9002\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3183 - acc: 0.9032 - val_loss: 0.3807 - val_acc: 0.8996\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3171 - acc: 0.9036 - val_loss: 0.3714 - val_acc: 0.9004\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3142 - acc: 0.9040 - val_loss: 0.3750 - val_acc: 0.9003\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3117 - acc: 0.9043 - val_loss: 0.3752 - val_acc: 0.8997\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3100 - acc: 0.9046 - val_loss: 0.3801 - val_acc: 0.9001\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3066 - acc: 0.9053 - val_loss: 0.3764 - val_acc: 0.8984\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3042 - acc: 0.9057 - val_loss: 0.3718 - val_acc: 0.9003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3020 - acc: 0.9065 - val_loss: 0.3759 - val_acc: 0.8998\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2988 - acc: 0.9070 - val_loss: 0.3774 - val_acc: 0.8981\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2961 - acc: 0.9074 - val_loss: 0.3694 - val_acc: 0.9004\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2934 - acc: 0.9075 - val_loss: 0.3778 - val_acc: 0.8986\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2904 - acc: 0.9082 - val_loss: 0.3814 - val_acc: 0.8997\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2879 - acc: 0.9092 - val_loss: 0.3837 - val_acc: 0.8982\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2846 - acc: 0.9097 - val_loss: 0.3814 - val_acc: 0.8989\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2809 - acc: 0.9107 - val_loss: 0.3827 - val_acc: 0.8982\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2776 - acc: 0.9116 - val_loss: 0.3886 - val_acc: 0.8971\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2733 - acc: 0.9127 - val_loss: 0.3946 - val_acc: 0.8966\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2704 - acc: 0.9137 - val_loss: 0.3995 - val_acc: 0.8954\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2661 - acc: 0.9143 - val_loss: 0.3984 - val_acc: 0.8945\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2617 - acc: 0.9157 - val_loss: 0.4000 - val_acc: 0.8933\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2575 - acc: 0.9172 - val_loss: 0.4009 - val_acc: 0.8975\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2535 - acc: 0.9181 - val_loss: 0.4151 - val_acc: 0.8925\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 8s 635us/step - loss: 0.9288 - acc: 0.7029 - val_loss: 0.5188 - val_acc: 0.8993\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.4429 - acc: 0.8999 - val_loss: 0.4343 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4479 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4074 - acc: 0.9000 - val_loss: 0.4226 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4325 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.4043 - acc: 0.9000 - val_loss: 0.4462 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.4030 - acc: 0.9000 - val_loss: 0.4376 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.4008 - acc: 0.9000 - val_loss: 0.4332 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3988 - acc: 0.9000 - val_loss: 0.4359 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3958 - acc: 0.9000 - val_loss: 0.4368 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3919 - acc: 0.9000 - val_loss: 0.4259 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3884 - acc: 0.9000 - val_loss: 0.4313 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3837 - acc: 0.9000 - val_loss: 0.4334 - val_acc: 0.9000\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3780 - acc: 0.9000 - val_loss: 0.4320 - val_acc: 0.9000\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3727 - acc: 0.9000 - val_loss: 0.4126 - val_acc: 0.9000\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3664 - acc: 0.9000 - val_loss: 0.4107 - val_acc: 0.9000\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.3623 - acc: 0.9000 - val_loss: 0.4115 - val_acc: 0.9000\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3576 - acc: 0.9000 - val_loss: 0.4047 - val_acc: 0.8999\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3537 - acc: 0.8999 - val_loss: 0.4065 - val_acc: 0.9000\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3489 - acc: 0.9000 - val_loss: 0.4010 - val_acc: 0.8999\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3440 - acc: 0.9002 - val_loss: 0.3906 - val_acc: 0.8998\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3399 - acc: 0.9003 - val_loss: 0.3842 - val_acc: 0.8995\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3364 - acc: 0.9005 - val_loss: 0.3846 - val_acc: 0.8993\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.3327 - acc: 0.9009 - val_loss: 0.3801 - val_acc: 0.8993\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3302 - acc: 0.9012 - val_loss: 0.3797 - val_acc: 0.9001\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3278 - acc: 0.9016 - val_loss: 0.3770 - val_acc: 0.8995\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3250 - acc: 0.9021 - val_loss: 0.3766 - val_acc: 0.9000\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3234 - acc: 0.9021 - val_loss: 0.3865 - val_acc: 0.8989\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.3202 - acc: 0.9026 - val_loss: 0.3847 - val_acc: 0.8965\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3182 - acc: 0.9029 - val_loss: 0.3707 - val_acc: 0.9004\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3159 - acc: 0.9037 - val_loss: 0.3716 - val_acc: 0.9001\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3135 - acc: 0.9039 - val_loss: 0.3764 - val_acc: 0.8989\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3113 - acc: 0.9041 - val_loss: 0.3782 - val_acc: 0.9003\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3090 - acc: 0.9047 - val_loss: 0.3769 - val_acc: 0.8999\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3070 - acc: 0.9049 - val_loss: 0.3747 - val_acc: 0.8982\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3048 - acc: 0.9057 - val_loss: 0.3728 - val_acc: 0.8991\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3017 - acc: 0.9061 - val_loss: 0.3728 - val_acc: 0.8988\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2989 - acc: 0.9066 - val_loss: 0.3767 - val_acc: 0.8993\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2963 - acc: 0.9072 - val_loss: 0.3833 - val_acc: 0.8988\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2946 - acc: 0.9075 - val_loss: 0.3804 - val_acc: 0.8987\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2912 - acc: 0.9084 - val_loss: 0.3840 - val_acc: 0.8964\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.2880 - acc: 0.9089 - val_loss: 0.3818 - val_acc: 0.8970\n",
      "Epoch 44/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.2852 - acc: 0.9097 - val_loss: 0.3811 - val_acc: 0.8979\n",
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.2821 - acc: 0.9105 - val_loss: 0.3888 - val_acc: 0.8974\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.2783 - acc: 0.9117 - val_loss: 0.3980 - val_acc: 0.8959\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2754 - acc: 0.9122 - val_loss: 0.3970 - val_acc: 0.8956\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2715 - acc: 0.9134 - val_loss: 0.3970 - val_acc: 0.8938\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.2678 - acc: 0.9145 - val_loss: 0.3980 - val_acc: 0.8950\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2632 - acc: 0.9160 - val_loss: 0.4121 - val_acc: 0.8942\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.2593 - acc: 0.9168 - val_loss: 0.4187 - val_acc: 0.8900\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.2555 - acc: 0.9181 - val_loss: 0.4236 - val_acc: 0.8917\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2510 - acc: 0.9190 - val_loss: 0.4339 - val_acc: 0.8887\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2466 - acc: 0.9208 - val_loss: 0.4283 - val_acc: 0.8913\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2423 - acc: 0.9218 - val_loss: 0.4331 - val_acc: 0.8910\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2384 - acc: 0.9236 - val_loss: 0.4467 - val_acc: 0.8910\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2339 - acc: 0.9245 - val_loss: 0.4486 - val_acc: 0.8857\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2294 - acc: 0.9260 - val_loss: 0.4539 - val_acc: 0.8840\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2246 - acc: 0.9283 - val_loss: 0.4695 - val_acc: 0.8841\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2208 - acc: 0.9290 - val_loss: 0.4712 - val_acc: 0.8820\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2148 - acc: 0.9317 - val_loss: 0.4790 - val_acc: 0.8820\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.2105 - acc: 0.9324 - val_loss: 0.4882 - val_acc: 0.8785\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.2063 - acc: 0.9340 - val_loss: 0.4824 - val_acc: 0.8835\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.2016 - acc: 0.9353 - val_loss: 0.5007 - val_acc: 0.8791\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.1982 - acc: 0.9360 - val_loss: 0.5029 - val_acc: 0.8794\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1921 - acc: 0.9387 - val_loss: 0.5217 - val_acc: 0.8800\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.1879 - acc: 0.9404 - val_loss: 0.5255 - val_acc: 0.8773\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.1849 - acc: 0.9407 - val_loss: 0.5368 - val_acc: 0.8774\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.1800 - acc: 0.9428 - val_loss: 0.5339 - val_acc: 0.8796\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.1751 - acc: 0.9444 - val_loss: 0.5568 - val_acc: 0.8761\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.1714 - acc: 0.9454 - val_loss: 0.5604 - val_acc: 0.8760\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.1666 - acc: 0.9472 - val_loss: 0.5803 - val_acc: 0.8744\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1638 - acc: 0.9475 - val_loss: 0.5825 - val_acc: 0.8745\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.1575 - acc: 0.9504 - val_loss: 0.5896 - val_acc: 0.8745\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.1543 - acc: 0.9513 - val_loss: 0.5960 - val_acc: 0.8686\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.1501 - acc: 0.9531 - val_loss: 0.6021 - val_acc: 0.8696\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1480 - acc: 0.9533 - val_loss: 0.6121 - val_acc: 0.8703\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.1431 - acc: 0.9550 - val_loss: 0.6314 - val_acc: 0.8704\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.1386 - acc: 0.9564 - val_loss: 0.6313 - val_acc: 0.8697\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.1354 - acc: 0.9572 - val_loss: 0.6528 - val_acc: 0.8689\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 8s 649us/step - loss: 0.9784 - acc: 0.6746 - val_loss: 0.5364 - val_acc: 0.8989\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.4497 - acc: 0.8999 - val_loss: 0.4414 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.4117 - acc: 0.9000 - val_loss: 0.4422 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.4078 - acc: 0.9000 - val_loss: 0.4273 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.4051 - acc: 0.9000 - val_loss: 0.4394 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4209 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.4026 - acc: 0.9000 - val_loss: 0.4342 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.4007 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3980 - acc: 0.9000 - val_loss: 0.4353 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3959 - acc: 0.9000 - val_loss: 0.4381 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3916 - acc: 0.9000 - val_loss: 0.4321 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3886 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3850 - acc: 0.9000 - val_loss: 0.4227 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3820 - acc: 0.9000 - val_loss: 0.4172 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.3784 - acc: 0.9000 - val_loss: 0.4195 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3745 - acc: 0.9000 - val_loss: 0.4224 - val_acc: 0.9000\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3699 - acc: 0.9000 - val_loss: 0.4182 - val_acc: 0.9000\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.3644 - acc: 0.9000 - val_loss: 0.4098 - val_acc: 0.9000\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3597 - acc: 0.9000 - val_loss: 0.3974 - val_acc: 0.9000\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3563 - acc: 0.9000 - val_loss: 0.4031 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.3520 - acc: 0.9000 - val_loss: 0.4001 - val_acc: 0.9000\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3470 - acc: 0.9001 - val_loss: 0.3919 - val_acc: 0.9000\n",
      "Epoch 23/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3427 - acc: 0.9002 - val_loss: 0.3919 - val_acc: 0.8999\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3401 - acc: 0.9004 - val_loss: 0.3886 - val_acc: 0.8997\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3373 - acc: 0.9004 - val_loss: 0.3935 - val_acc: 0.9001\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.3333 - acc: 0.9008 - val_loss: 0.3904 - val_acc: 0.9001\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3309 - acc: 0.9013 - val_loss: 0.3785 - val_acc: 0.8995\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.3283 - acc: 0.9014 - val_loss: 0.3822 - val_acc: 0.8998\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3261 - acc: 0.9018 - val_loss: 0.3801 - val_acc: 0.9003\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.3230 - acc: 0.9023 - val_loss: 0.3776 - val_acc: 0.9002\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3206 - acc: 0.9029 - val_loss: 0.3769 - val_acc: 0.8999\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.3176 - acc: 0.9033 - val_loss: 0.3881 - val_acc: 0.8997\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3150 - acc: 0.9035 - val_loss: 0.3736 - val_acc: 0.9001\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.3130 - acc: 0.9040 - val_loss: 0.3755 - val_acc: 0.8996\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.3103 - acc: 0.9041 - val_loss: 0.3810 - val_acc: 0.8993\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3068 - acc: 0.9050 - val_loss: 0.3750 - val_acc: 0.8993\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.3042 - acc: 0.9053 - val_loss: 0.3784 - val_acc: 0.8991\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.3017 - acc: 0.9058 - val_loss: 0.3780 - val_acc: 0.8988\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.2981 - acc: 0.9066 - val_loss: 0.3788 - val_acc: 0.8978\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.2949 - acc: 0.9071 - val_loss: 0.3866 - val_acc: 0.8988\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.2919 - acc: 0.9077 - val_loss: 0.3768 - val_acc: 0.8999\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2890 - acc: 0.9084 - val_loss: 0.3834 - val_acc: 0.8983\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.2850 - acc: 0.9095 - val_loss: 0.3937 - val_acc: 0.8956\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2808 - acc: 0.9105 - val_loss: 0.3886 - val_acc: 0.8981\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2771 - acc: 0.9114 - val_loss: 0.3997 - val_acc: 0.8979\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2744 - acc: 0.9117 - val_loss: 0.3987 - val_acc: 0.8952\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.2694 - acc: 0.9133 - val_loss: 0.3998 - val_acc: 0.8951\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2658 - acc: 0.9141 - val_loss: 0.3981 - val_acc: 0.8930\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2612 - acc: 0.9154 - val_loss: 0.4093 - val_acc: 0.8902\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.2574 - acc: 0.9161 - val_loss: 0.4168 - val_acc: 0.8901\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.2516 - acc: 0.9183 - val_loss: 0.4177 - val_acc: 0.8912\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.2476 - acc: 0.9192 - val_loss: 0.4285 - val_acc: 0.8915\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2431 - acc: 0.9210 - val_loss: 0.4328 - val_acc: 0.8916\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.2385 - acc: 0.9218 - val_loss: 0.4206 - val_acc: 0.8908\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.2334 - acc: 0.9233 - val_loss: 0.4449 - val_acc: 0.8889\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2282 - acc: 0.9248 - val_loss: 0.4372 - val_acc: 0.8880\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.2227 - acc: 0.9272 - val_loss: 0.4559 - val_acc: 0.8866\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.2173 - acc: 0.9284 - val_loss: 0.4498 - val_acc: 0.8860\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.2124 - acc: 0.9305 - val_loss: 0.4649 - val_acc: 0.8838\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.2090 - acc: 0.9310 - val_loss: 0.4699 - val_acc: 0.8835\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.2033 - acc: 0.9333 - val_loss: 0.4721 - val_acc: 0.8805\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.1984 - acc: 0.9348 - val_loss: 0.4828 - val_acc: 0.8823\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.1933 - acc: 0.9365 - val_loss: 0.4977 - val_acc: 0.8805\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.1891 - acc: 0.9378 - val_loss: 0.5046 - val_acc: 0.8853\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.1858 - acc: 0.9386 - val_loss: 0.4969 - val_acc: 0.8802\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.1800 - acc: 0.9410 - val_loss: 0.5219 - val_acc: 0.8760\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.1741 - acc: 0.9431 - val_loss: 0.5264 - val_acc: 0.8812\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.1686 - acc: 0.9443 - val_loss: 0.5326 - val_acc: 0.8761\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.1654 - acc: 0.9459 - val_loss: 0.5491 - val_acc: 0.8798\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 1s 109us/step - loss: 0.1614 - acc: 0.9468 - val_loss: 0.5652 - val_acc: 0.8727\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.1580 - acc: 0.9481 - val_loss: 0.5614 - val_acc: 0.8790\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 1s 110us/step - loss: 0.1525 - acc: 0.9502 - val_loss: 0.5692 - val_acc: 0.8755\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 1s 112us/step - loss: 0.1476 - acc: 0.9521 - val_loss: 0.5876 - val_acc: 0.8726\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.1436 - acc: 0.9532 - val_loss: 0.5898 - val_acc: 0.8744\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.1404 - acc: 0.9540 - val_loss: 0.5977 - val_acc: 0.8665\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.1351 - acc: 0.9561 - val_loss: 0.6117 - val_acc: 0.8722\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1315 - acc: 0.9577 - val_loss: 0.6240 - val_acc: 0.8688\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.1282 - acc: 0.9584 - val_loss: 0.6324 - val_acc: 0.8723\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1238 - acc: 0.9599 - val_loss: 0.6481 - val_acc: 0.8706\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.1199 - acc: 0.9619 - val_loss: 0.6691 - val_acc: 0.8689\n",
      "Epoch 81/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1185 - acc: 0.9617 - val_loss: 0.6771 - val_acc: 0.8655\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.1149 - acc: 0.9627 - val_loss: 0.6757 - val_acc: 0.8649\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.1117 - acc: 0.9640 - val_loss: 0.6892 - val_acc: 0.8638\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.1073 - acc: 0.9654 - val_loss: 0.6978 - val_acc: 0.8668\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.1028 - acc: 0.9674 - val_loss: 0.7092 - val_acc: 0.8684\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.1003 - acc: 0.9681 - val_loss: 0.7221 - val_acc: 0.8641\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.0966 - acc: 0.9692 - val_loss: 0.7302 - val_acc: 0.8619\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.0955 - acc: 0.9695 - val_loss: 0.7359 - val_acc: 0.8629\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.0907 - acc: 0.9713 - val_loss: 0.7567 - val_acc: 0.8645\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.0895 - acc: 0.9714 - val_loss: 0.7657 - val_acc: 0.8626\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.0861 - acc: 0.9729 - val_loss: 0.7802 - val_acc: 0.8552\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.0864 - acc: 0.9719 - val_loss: 0.7939 - val_acc: 0.8615\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 1s 111us/step - loss: 0.0828 - acc: 0.9737 - val_loss: 0.7908 - val_acc: 0.8602\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.0853 - acc: 0.9724 - val_loss: 0.7940 - val_acc: 0.8617\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.0765 - acc: 0.9758 - val_loss: 0.8167 - val_acc: 0.8556\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.0745 - acc: 0.9766 - val_loss: 0.8271 - val_acc: 0.8613\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 1s 115us/step - loss: 0.0696 - acc: 0.9784 - val_loss: 0.8428 - val_acc: 0.8602\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 1s 114us/step - loss: 0.0687 - acc: 0.9785 - val_loss: 0.8491 - val_acc: 0.8565\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.0678 - acc: 0.9791 - val_loss: 0.8639 - val_acc: 0.8572\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.0687 - acc: 0.9779 - val_loss: 0.8669 - val_acc: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_45 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_43/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_44/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_47 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_45/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_46/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_49 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_47/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_48/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_51 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_49/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_50/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_53 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_51/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_52/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_55 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_53/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_54/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_57 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_55/concat:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'concatenate_56/concat:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_45 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_88:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_89:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_47 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_92:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_93:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_49 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_96:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_97:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_51 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_100:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_101:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_53 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_104:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_105:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_55 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_108:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_109:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_57 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_112:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'input_113:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5364366692304611\n",
      "1 0.44142422676086424\n",
      "2 0.44216631561517716\n",
      "3 0.4272893032431602\n",
      "4 0.4394484931230545\n",
      "5 0.4208856555819511\n",
      "6 0.4341859656572342\n",
      "7 0.4375089657306671\n",
      "8 0.4352851954102516\n",
      "9 0.43810607433319093\n",
      "10 0.43214015394449234\n",
      "11 0.42830985844135283\n",
      "12 0.42265938252210616\n",
      "13 0.4172358176112175\n",
      "14 0.4195031800866127\n",
      "15 0.42235361605882643\n",
      "16 0.4181597617268562\n",
      "17 0.4097548416256905\n",
      "18 0.3973571529984474\n",
      "19 0.40310065060853956\n",
      "20 0.400077218413353\n",
      "21 0.3919474920630455\n",
      "22 0.39189963787794113\n",
      "23 0.3885879161953926\n",
      "24 0.3934602737426758\n",
      "25 0.3903750976920128\n",
      "26 0.3784806814789772\n",
      "27 0.3822320455312729\n",
      "28 0.38006810784339906\n",
      "29 0.37764702588319776\n",
      "30 0.3768865188956261\n",
      "31 0.38807852298021317\n",
      "32 0.3735904657840729\n",
      "33 0.3755290421843529\n",
      "34 0.38101269423961637\n",
      "35 0.37501992225646974\n",
      "36 0.3783922478556633\n",
      "37 0.37798679262399676\n",
      "38 0.3787801879644394\n",
      "39 0.38657175153493883\n",
      "40 0.37676590800285337\n",
      "41 0.38342922627925874\n",
      "42 0.3937042814493179\n",
      "43 0.38859531939029696\n",
      "44 0.39967798948287964\n",
      "45 0.39866041213274\n",
      "46 0.3997788792848587\n",
      "47 0.39811563372612\n",
      "48 0.40931043863296507\n",
      "49 0.4167522445321083\n",
      "50 0.4177252921462059\n",
      "51 0.4284990531206131\n",
      "52 0.4328319874405861\n",
      "53 0.4206218194961548\n",
      "54 0.4448695394396782\n",
      "55 0.43718385726213455\n",
      "56 0.45591965079307556\n",
      "57 0.4498434716463089\n",
      "58 0.46492941230535506\n",
      "59 0.4699398511648178\n",
      "60 0.47207306504249574\n",
      "61 0.4828049600124359\n",
      "62 0.4976706811785698\n",
      "63 0.5045898473262787\n",
      "64 0.4968745401501656\n",
      "65 0.5218743079900742\n",
      "66 0.5264384204149246\n",
      "67 0.5325635719299316\n",
      "68 0.549094777405262\n",
      "69 0.5652248793840409\n",
      "70 0.5613902267813683\n",
      "71 0.5691937586665153\n",
      "72 0.5876348671317101\n",
      "73 0.5897909265756607\n",
      "74 0.597704926431179\n",
      "75 0.6117389971017837\n",
      "76 0.6240314173698426\n",
      "77 0.6323538586497307\n",
      "78 0.6481323283910752\n",
      "79 0.6690586125850677\n",
      "80 0.67710753262043\n",
      "81 0.675685031414032\n",
      "82 0.6892368745803833\n",
      "83 0.6978288382291794\n",
      "84 0.7092099642753601\n",
      "85 0.7221149182319642\n",
      "86 0.7301712721586228\n",
      "87 0.7359216499328614\n",
      "88 0.756703354716301\n",
      "89 0.7657479572296143\n",
      "90 0.7802214634418487\n",
      "91 0.7939113348722457\n",
      "92 0.7907937574386597\n",
      "93 0.7939594584703445\n",
      "94 0.8166847723722458\n",
      "95 0.8271446591615677\n",
      "96 0.8427869498729705\n",
      "97 0.8490913397073746\n",
      "98 0.8638877755403519\n",
      "99 0.8669064164161682\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 7s 620us/step - loss: 0.8610 - acc: 0.7257 - val_loss: 0.4806 - val_acc: 0.8999\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.4225 - acc: 0.9000 - val_loss: 0.4269 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.4091 - acc: 0.9000 - val_loss: 0.4322 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 8s 625us/step - loss: 0.8624 - acc: 0.7235 - val_loss: 0.4867 - val_acc: 0.8999\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.4238 - acc: 0.9000 - val_loss: 0.4363 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.4091 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.4062 - acc: 0.9000 - val_loss: 0.4216 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4026 - acc: 0.9000 - val_loss: 0.4238 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.4008 - acc: 0.9000 - val_loss: 0.4403 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3986 - acc: 0.9000 - val_loss: 0.4237 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3946 - acc: 0.9000 - val_loss: 0.4293 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3891 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 8s 635us/step - loss: 0.8809 - acc: 0.7182 - val_loss: 0.4736 - val_acc: 0.8999\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.4246 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4095 - acc: 0.9000 - val_loss: 0.4314 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4055 - acc: 0.9000 - val_loss: 0.4192 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.4032 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.4011 - acc: 0.9000 - val_loss: 0.4259 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3999 - acc: 0.9000 - val_loss: 0.4468 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3957 - acc: 0.9000 - val_loss: 0.4268 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.3906 - acc: 0.9000 - val_loss: 0.4361 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3839 - acc: 0.9000 - val_loss: 0.4200 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3774 - acc: 0.9000 - val_loss: 0.4193 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 1s 116us/step - loss: 0.3714 - acc: 0.9000 - val_loss: 0.4202 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3650 - acc: 0.9000 - val_loss: 0.4101 - val_acc: 0.8998\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3577 - acc: 0.9000 - val_loss: 0.4146 - val_acc: 0.8999\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.3533 - acc: 0.9000 - val_loss: 0.4024 - val_acc: 0.8999\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3482 - acc: 0.8999 - val_loss: 0.4059 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.3416 - acc: 0.9002 - val_loss: 0.4104 - val_acc: 0.8993\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.3351 - acc: 0.9007 - val_loss: 0.3854 - val_acc: 0.8990\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 1s 117us/step - loss: 0.3296 - acc: 0.9014 - val_loss: 0.4017 - val_acc: 0.8989\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 8s 697us/step - loss: 0.8463 - acc: 0.7312 - val_loss: 0.4608 - val_acc: 0.9000\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.4217 - acc: 0.9000 - val_loss: 0.4275 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.4094 - acc: 0.9000 - val_loss: 0.4348 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4358 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4257 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.4033 - acc: 0.9000 - val_loss: 0.4265 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.4028 - acc: 0.9000 - val_loss: 0.4295 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.4011 - acc: 0.9000 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3968 - acc: 0.9000 - val_loss: 0.4244 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.3930 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3868 - acc: 0.9000 - val_loss: 0.4410 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 2s 146us/step - loss: 0.3815 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3742 - acc: 0.9000 - val_loss: 0.4332 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.3686 - acc: 0.9000 - val_loss: 0.4187 - val_acc: 0.9000\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3601 - acc: 0.8999 - val_loss: 0.4132 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3542 - acc: 0.9000 - val_loss: 0.4097 - val_acc: 0.8993\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.3493 - acc: 0.9000 - val_loss: 0.4009 - val_acc: 0.8996\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3440 - acc: 0.9002 - val_loss: 0.3967 - val_acc: 0.8990\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.3391 - acc: 0.9005 - val_loss: 0.4022 - val_acc: 0.8975\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3348 - acc: 0.9007 - val_loss: 0.3908 - val_acc: 0.8990\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3287 - acc: 0.9012 - val_loss: 0.3859 - val_acc: 0.8989\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 2s 148us/step - loss: 0.3234 - acc: 0.9021 - val_loss: 0.3906 - val_acc: 0.8989\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.3192 - acc: 0.9030 - val_loss: 0.3902 - val_acc: 0.8986\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.3133 - acc: 0.9035 - val_loss: 0.3967 - val_acc: 0.8983\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3098 - acc: 0.9044 - val_loss: 0.3956 - val_acc: 0.8951\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.3033 - acc: 0.9057 - val_loss: 0.4045 - val_acc: 0.8969\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.2980 - acc: 0.9071 - val_loss: 0.4056 - val_acc: 0.8967\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.2920 - acc: 0.9085 - val_loss: 0.4025 - val_acc: 0.8935\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.2861 - acc: 0.9097 - val_loss: 0.4120 - val_acc: 0.8966\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.2784 - acc: 0.9119 - val_loss: 0.4178 - val_acc: 0.8938\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 9s 733us/step - loss: 0.8504 - acc: 0.7336 - val_loss: 0.4811 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.4235 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.4085 - acc: 0.9000 - val_loss: 0.4232 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.4070 - acc: 0.9000 - val_loss: 0.4152 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.4035 - acc: 0.9000 - val_loss: 0.4328 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3995 - acc: 0.9000 - val_loss: 0.4252 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3957 - acc: 0.9000 - val_loss: 0.4570 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.3911 - acc: 0.9000 - val_loss: 0.4469 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3851 - acc: 0.9000 - val_loss: 0.4300 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.3767 - acc: 0.9000 - val_loss: 0.4311 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3706 - acc: 0.9000 - val_loss: 0.4165 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.3648 - acc: 0.9000 - val_loss: 0.4018 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.3594 - acc: 0.9000 - val_loss: 0.4080 - val_acc: 0.8998\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3532 - acc: 0.9000 - val_loss: 0.4026 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3477 - acc: 0.9000 - val_loss: 0.4026 - val_acc: 0.8997\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3426 - acc: 0.9001 - val_loss: 0.3946 - val_acc: 0.8997\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.3372 - acc: 0.9007 - val_loss: 0.4010 - val_acc: 0.8981\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3331 - acc: 0.9010 - val_loss: 0.3877 - val_acc: 0.8993\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.3285 - acc: 0.9020 - val_loss: 0.3886 - val_acc: 0.8984\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.3242 - acc: 0.9023 - val_loss: 0.3863 - val_acc: 0.8994\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.3202 - acc: 0.9027 - val_loss: 0.3853 - val_acc: 0.8991\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3153 - acc: 0.9038 - val_loss: 0.3812 - val_acc: 0.8992\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3124 - acc: 0.9041 - val_loss: 0.3931 - val_acc: 0.8960\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.3075 - acc: 0.9049 - val_loss: 0.3912 - val_acc: 0.8951\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3021 - acc: 0.9061 - val_loss: 0.3902 - val_acc: 0.8990\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.2977 - acc: 0.9074 - val_loss: 0.3920 - val_acc: 0.8951\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.2915 - acc: 0.9091 - val_loss: 0.3964 - val_acc: 0.8954\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.2851 - acc: 0.9105 - val_loss: 0.4057 - val_acc: 0.8947\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.2782 - acc: 0.9124 - val_loss: 0.4072 - val_acc: 0.8932\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.2703 - acc: 0.9148 - val_loss: 0.4208 - val_acc: 0.8921\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.2632 - acc: 0.9171 - val_loss: 0.4171 - val_acc: 0.8933\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.2541 - acc: 0.9201 - val_loss: 0.4286 - val_acc: 0.8909\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.2447 - acc: 0.9227 - val_loss: 0.4423 - val_acc: 0.8918\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.2368 - acc: 0.9255 - val_loss: 0.4463 - val_acc: 0.8889\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.2272 - acc: 0.9290 - val_loss: 0.4595 - val_acc: 0.8865\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.2165 - acc: 0.9325 - val_loss: 0.4767 - val_acc: 0.8867\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.2074 - acc: 0.9357 - val_loss: 0.4862 - val_acc: 0.8832\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.1974 - acc: 0.9386 - val_loss: 0.4992 - val_acc: 0.8806\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.1886 - acc: 0.9413 - val_loss: 0.5152 - val_acc: 0.8816\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1782 - acc: 0.9455 - val_loss: 0.5213 - val_acc: 0.8808\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.1684 - acc: 0.9481 - val_loss: 0.5480 - val_acc: 0.8768\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.1598 - acc: 0.9505 - val_loss: 0.5668 - val_acc: 0.8774\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s 119us/step - loss: 0.1516 - acc: 0.9530 - val_loss: 0.5775 - val_acc: 0.8749\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s 118us/step - loss: 0.1453 - acc: 0.9546 - val_loss: 0.6094 - val_acc: 0.8715\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1361 - acc: 0.9578 - val_loss: 0.6190 - val_acc: 0.8737\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.1270 - acc: 0.9602 - val_loss: 0.6306 - val_acc: 0.8714\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1208 - acc: 0.9626 - val_loss: 0.6443 - val_acc: 0.8761\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.1137 - acc: 0.9643 - val_loss: 0.6767 - val_acc: 0.8702\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 8s 690us/step - loss: 0.8623 - acc: 0.7200 - val_loss: 0.4566 - val_acc: 0.9000\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4205 - acc: 0.9000 - val_loss: 0.4180 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.4080 - acc: 0.9000 - val_loss: 0.4338 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4060 - acc: 0.9000 - val_loss: 0.4374 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4405 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.4034 - acc: 0.9000 - val_loss: 0.4306 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4039 - acc: 0.9000 - val_loss: 0.4387 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.4002 - acc: 0.9000 - val_loss: 0.4285 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.4361 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.3920 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3871 - acc: 0.9000 - val_loss: 0.4147 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3818 - acc: 0.9000 - val_loss: 0.4271 - val_acc: 0.9000\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3738 - acc: 0.9000 - val_loss: 0.4115 - val_acc: 0.9000\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3675 - acc: 0.9000 - val_loss: 0.4083 - val_acc: 0.8998\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.3585 - acc: 0.9000 - val_loss: 0.4074 - val_acc: 0.8999\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3524 - acc: 0.9000 - val_loss: 0.4169 - val_acc: 0.8995\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.3467 - acc: 0.9000 - val_loss: 0.3961 - val_acc: 0.8995\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3410 - acc: 0.9003 - val_loss: 0.3976 - val_acc: 0.8988\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3369 - acc: 0.9007 - val_loss: 0.3991 - val_acc: 0.8990\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3321 - acc: 0.9009 - val_loss: 0.3962 - val_acc: 0.8992\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.3276 - acc: 0.9014 - val_loss: 0.3918 - val_acc: 0.8982\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3236 - acc: 0.9020 - val_loss: 0.3938 - val_acc: 0.8977\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3192 - acc: 0.9027 - val_loss: 0.3943 - val_acc: 0.8971\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3149 - acc: 0.9036 - val_loss: 0.3958 - val_acc: 0.8982\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3102 - acc: 0.9045 - val_loss: 0.4002 - val_acc: 0.8937\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.3044 - acc: 0.9060 - val_loss: 0.4048 - val_acc: 0.8957\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.3010 - acc: 0.9064 - val_loss: 0.4048 - val_acc: 0.8958\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.2943 - acc: 0.9080 - val_loss: 0.4035 - val_acc: 0.8934\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.2877 - acc: 0.9094 - val_loss: 0.4066 - val_acc: 0.8920\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.2811 - acc: 0.9117 - val_loss: 0.4224 - val_acc: 0.8933\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.2725 - acc: 0.9139 - val_loss: 0.4105 - val_acc: 0.8933\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.2634 - acc: 0.9167 - val_loss: 0.4329 - val_acc: 0.8877\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.2562 - acc: 0.9193 - val_loss: 0.4337 - val_acc: 0.8901\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.2471 - acc: 0.9220 - val_loss: 0.4494 - val_acc: 0.8893\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.2370 - acc: 0.9258 - val_loss: 0.4510 - val_acc: 0.8860\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.2269 - acc: 0.9292 - val_loss: 0.4589 - val_acc: 0.8820\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.2171 - acc: 0.9320 - val_loss: 0.4774 - val_acc: 0.8844\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.2067 - acc: 0.9356 - val_loss: 0.4950 - val_acc: 0.8823\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.1969 - acc: 0.9389 - val_loss: 0.5041 - val_acc: 0.8826\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.1882 - acc: 0.9414 - val_loss: 0.5234 - val_acc: 0.8810\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.1781 - acc: 0.9448 - val_loss: 0.5337 - val_acc: 0.8801\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.1689 - acc: 0.9480 - val_loss: 0.5501 - val_acc: 0.8756\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.1602 - acc: 0.9504 - val_loss: 0.5783 - val_acc: 0.8804\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.1504 - acc: 0.9535 - val_loss: 0.5986 - val_acc: 0.8729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.1433 - acc: 0.9554 - val_loss: 0.5978 - val_acc: 0.8712\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.1351 - acc: 0.9581 - val_loss: 0.6278 - val_acc: 0.8721\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.1262 - acc: 0.9612 - val_loss: 0.6398 - val_acc: 0.8737\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.1191 - acc: 0.9633 - val_loss: 0.6692 - val_acc: 0.8708\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.1124 - acc: 0.9647 - val_loss: 0.6947 - val_acc: 0.8688\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.1085 - acc: 0.9652 - val_loss: 0.6935 - val_acc: 0.8699\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.1033 - acc: 0.9669 - val_loss: 0.7380 - val_acc: 0.8655\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0937 - acc: 0.9707 - val_loss: 0.7332 - val_acc: 0.8682\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.0994 - acc: 0.9672 - val_loss: 0.7394 - val_acc: 0.8713\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.0860 - acc: 0.9723 - val_loss: 0.7707 - val_acc: 0.8613\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0767 - acc: 0.9751 - val_loss: 0.7797 - val_acc: 0.8654\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.0704 - acc: 0.9778 - val_loss: 0.8099 - val_acc: 0.8667\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0674 - acc: 0.9786 - val_loss: 0.8232 - val_acc: 0.8661\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0627 - acc: 0.9800 - val_loss: 0.8539 - val_acc: 0.8620\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.0572 - acc: 0.9816 - val_loss: 0.8751 - val_acc: 0.8586\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.0589 - acc: 0.9811 - val_loss: 0.8939 - val_acc: 0.8651\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0638 - acc: 0.9784 - val_loss: 0.8929 - val_acc: 0.8619\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.0536 - acc: 0.9823 - val_loss: 0.9035 - val_acc: 0.8571\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0443 - acc: 0.9862 - val_loss: 0.9304 - val_acc: 0.8641\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0369 - acc: 0.9892 - val_loss: 0.9473 - val_acc: 0.8607\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0339 - acc: 0.9898 - val_loss: 0.9707 - val_acc: 0.8593\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.0322 - acc: 0.9908 - val_loss: 1.0017 - val_acc: 0.8621\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0345 - acc: 0.9898 - val_loss: 1.0020 - val_acc: 0.8574\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.0341 - acc: 0.9900 - val_loss: 1.0223 - val_acc: 0.8618\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0360 - acc: 0.9890 - val_loss: 0.9981 - val_acc: 0.8639\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0378 - acc: 0.9882 - val_loss: 1.0095 - val_acc: 0.8598\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0317 - acc: 0.9909 - val_loss: 1.0225 - val_acc: 0.8609\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0234 - acc: 0.9942 - val_loss: 1.0492 - val_acc: 0.8576\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0222 - acc: 0.9942 - val_loss: 1.0663 - val_acc: 0.8588\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0189 - acc: 0.9954 - val_loss: 1.0814 - val_acc: 0.8632\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 1.0850 - val_acc: 0.8612\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0162 - acc: 0.9962 - val_loss: 1.0992 - val_acc: 0.8642\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0156 - acc: 0.9963 - val_loss: 1.1123 - val_acc: 0.8622\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0163 - acc: 0.9962 - val_loss: 1.1202 - val_acc: 0.8594\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.0371 - acc: 0.9882 - val_loss: 1.0856 - val_acc: 0.8532\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.0443 - acc: 0.9849 - val_loss: 1.0733 - val_acc: 0.8597\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 9s 733us/step - loss: 0.8721 - acc: 0.7163 - val_loss: 0.4731 - val_acc: 0.8998\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.4263 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.4076 - acc: 0.9000 - val_loss: 0.4441 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.4067 - acc: 0.9000 - val_loss: 0.4412 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.4045 - acc: 0.9000 - val_loss: 0.4276 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.4040 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4318 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.4000 - acc: 0.9000 - val_loss: 0.4278 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3953 - acc: 0.9000 - val_loss: 0.4407 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.3918 - acc: 0.9000 - val_loss: 0.4412 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3867 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3811 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.3755 - acc: 0.9000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.3668 - acc: 0.9000 - val_loss: 0.4200 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.3599 - acc: 0.8999 - val_loss: 0.4111 - val_acc: 0.8998\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3544 - acc: 0.8999 - val_loss: 0.4040 - val_acc: 0.8996\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.3496 - acc: 0.9000 - val_loss: 0.4050 - val_acc: 0.8996\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.3430 - acc: 0.9001 - val_loss: 0.3981 - val_acc: 0.8993\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.3389 - acc: 0.9003 - val_loss: 0.4073 - val_acc: 0.8995\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.3343 - acc: 0.9006 - val_loss: 0.3945 - val_acc: 0.8990\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.3292 - acc: 0.9011 - val_loss: 0.3932 - val_acc: 0.8990\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3250 - acc: 0.9016 - val_loss: 0.4000 - val_acc: 0.8976\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3214 - acc: 0.9020 - val_loss: 0.3942 - val_acc: 0.8981\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.3159 - acc: 0.9029 - val_loss: 0.3867 - val_acc: 0.8969\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3115 - acc: 0.9035 - val_loss: 0.3988 - val_acc: 0.8967\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.3068 - acc: 0.9045 - val_loss: 0.3870 - val_acc: 0.8984\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.3020 - acc: 0.9056 - val_loss: 0.4057 - val_acc: 0.8964\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.2955 - acc: 0.9070 - val_loss: 0.3995 - val_acc: 0.8921\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.2891 - acc: 0.9086 - val_loss: 0.4073 - val_acc: 0.8952\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.2814 - acc: 0.9105 - val_loss: 0.4103 - val_acc: 0.8952\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.2754 - acc: 0.9120 - val_loss: 0.4285 - val_acc: 0.8880\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.2666 - acc: 0.9142 - val_loss: 0.4209 - val_acc: 0.8939\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.2567 - acc: 0.9169 - val_loss: 0.4490 - val_acc: 0.8853\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.2485 - acc: 0.9197 - val_loss: 0.4556 - val_acc: 0.8872\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.2369 - acc: 0.9235 - val_loss: 0.4585 - val_acc: 0.8848\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.2267 - acc: 0.9275 - val_loss: 0.4589 - val_acc: 0.8857\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.2165 - acc: 0.9302 - val_loss: 0.4922 - val_acc: 0.8851\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.2067 - acc: 0.9337 - val_loss: 0.4824 - val_acc: 0.8793\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.1963 - acc: 0.9373 - val_loss: 0.5021 - val_acc: 0.8769\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.1834 - acc: 0.9421 - val_loss: 0.5254 - val_acc: 0.8774\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.1742 - acc: 0.9448 - val_loss: 0.5407 - val_acc: 0.8771\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.1618 - acc: 0.9491 - val_loss: 0.5734 - val_acc: 0.8794\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.1545 - acc: 0.9508 - val_loss: 0.5831 - val_acc: 0.8744\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.1422 - acc: 0.9555 - val_loss: 0.6093 - val_acc: 0.8755\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.1357 - acc: 0.9568 - val_loss: 0.6276 - val_acc: 0.8722\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.1273 - acc: 0.9592 - val_loss: 0.6606 - val_acc: 0.8750\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.1206 - acc: 0.9618 - val_loss: 0.6581 - val_acc: 0.8707\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.1110 - acc: 0.9647 - val_loss: 0.6751 - val_acc: 0.8675\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0999 - acc: 0.9685 - val_loss: 0.7111 - val_acc: 0.8679\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0941 - acc: 0.9705 - val_loss: 0.7308 - val_acc: 0.8666\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0872 - acc: 0.9726 - val_loss: 0.7439 - val_acc: 0.8685\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.0816 - acc: 0.9739 - val_loss: 0.7693 - val_acc: 0.8636\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0786 - acc: 0.9746 - val_loss: 0.7948 - val_acc: 0.8706\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.0845 - acc: 0.9724 - val_loss: 0.7960 - val_acc: 0.8623\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.0721 - acc: 0.9768 - val_loss: 0.8077 - val_acc: 0.8624\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.0644 - acc: 0.9795 - val_loss: 0.8385 - val_acc: 0.8614\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.0560 - acc: 0.9824 - val_loss: 0.8653 - val_acc: 0.8626\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.0548 - acc: 0.9826 - val_loss: 0.8792 - val_acc: 0.8626\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0578 - acc: 0.9811 - val_loss: 0.8826 - val_acc: 0.8634\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.0455 - acc: 0.9865 - val_loss: 0.9111 - val_acc: 0.8578\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.0426 - acc: 0.9870 - val_loss: 0.9389 - val_acc: 0.8613\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.9404 - val_acc: 0.8585\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 1s 121us/step - loss: 0.0358 - acc: 0.9894 - val_loss: 0.9658 - val_acc: 0.8592\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0321 - acc: 0.9907 - val_loss: 0.9868 - val_acc: 0.8605\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0609 - acc: 0.9793 - val_loss: 0.9622 - val_acc: 0.8592\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0429 - acc: 0.9861 - val_loss: 0.9964 - val_acc: 0.8593\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 1s 122us/step - loss: 0.0337 - acc: 0.9902 - val_loss: 0.9901 - val_acc: 0.8609\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.0256 - acc: 0.9930 - val_loss: 1.0291 - val_acc: 0.8556\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0226 - acc: 0.9943 - val_loss: 1.0453 - val_acc: 0.8617\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 1.0539 - val_acc: 0.8613\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 1.0624 - val_acc: 0.8601\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0183 - acc: 0.9956 - val_loss: 1.0751 - val_acc: 0.8612\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0177 - acc: 0.9958 - val_loss: 1.0904 - val_acc: 0.8589\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0182 - acc: 0.9955 - val_loss: 1.1134 - val_acc: 0.8557\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0364 - acc: 0.9885 - val_loss: 1.0649 - val_acc: 0.8449\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.0416 - acc: 0.9863 - val_loss: 1.0652 - val_acc: 0.8512\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.0333 - acc: 0.9896 - val_loss: 1.0772 - val_acc: 0.8571\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 1.0946 - val_acc: 0.8570\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0153 - acc: 0.9967 - val_loss: 1.0946 - val_acc: 0.8604\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 1.1137 - val_acc: 0.8604\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0120 - acc: 0.9974 - val_loss: 1.1349 - val_acc: 0.8624\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0129 - acc: 0.9972 - val_loss: 1.1403 - val_acc: 0.8588\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0119 - acc: 0.9975 - val_loss: 1.1517 - val_acc: 0.8598\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.0117 - acc: 0.9974 - val_loss: 1.1523 - val_acc: 0.8599\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 1.1672 - val_acc: 0.8573\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 1.1838 - val_acc: 0.8581\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 1.1682 - val_acc: 0.8537\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0252 - acc: 0.9926 - val_loss: 1.1644 - val_acc: 0.8562\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0411 - acc: 0.9865 - val_loss: 1.1292 - val_acc: 0.8549\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0313 - acc: 0.9899 - val_loss: 1.1490 - val_acc: 0.8531\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0178 - acc: 0.9954 - val_loss: 1.1578 - val_acc: 0.8590\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.0114 - acc: 0.9973 - val_loss: 1.1717 - val_acc: 0.8581\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 0.0092 - acc: 0.9978 - val_loss: 1.1785 - val_acc: 0.8639\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0086 - acc: 0.9978 - val_loss: 1.1827 - val_acc: 0.8602\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0133 - acc: 0.9966 - val_loss: 1.1846 - val_acc: 0.8573\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 1.1864 - val_acc: 0.8601\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0096 - acc: 0.9977 - val_loss: 1.1920 - val_acc: 0.8632\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0085 - acc: 0.9980 - val_loss: 1.2018 - val_acc: 0.8598\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.0089 - acc: 0.9978 - val_loss: 1.1994 - val_acc: 0.8596\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0106 - acc: 0.9973 - val_loss: 1.2125 - val_acc: 0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_59 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_57/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_58/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_61 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_59/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_60/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_63 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_61/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_62/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_65 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_63/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_64/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_67 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_65/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_66/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_69 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_67/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_68/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_71 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_69/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_70/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_59 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_116:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_117:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_61 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_120:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_121:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_63 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_124:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_125:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_65 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_128:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_129:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_67 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_132:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_133:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_69 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_136:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_137:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_71 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_140:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_141:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4731178405880928\n",
      "1 0.4275969812273979\n",
      "2 0.4440618458390236\n",
      "3 0.4411573839187622\n",
      "4 0.4275842180848122\n",
      "5 0.4265648400783539\n",
      "6 0.4318290987610817\n",
      "7 0.4277964490652084\n",
      "8 0.44070088416337966\n",
      "9 0.44116441190242767\n",
      "10 0.42918916553258896\n",
      "11 0.4292360070347786\n",
      "12 0.42621719509363176\n",
      "13 0.41999289840459825\n",
      "14 0.41111778110265734\n",
      "15 0.4039900907874107\n",
      "16 0.4049717196822166\n",
      "17 0.39811230808496473\n",
      "18 0.40733631044626234\n",
      "19 0.39452834337949755\n",
      "20 0.39321081578731537\n",
      "21 0.4000061413645744\n",
      "22 0.3942167767882347\n",
      "23 0.3866689819097519\n",
      "24 0.3988491755723953\n",
      "25 0.38699358850717547\n",
      "26 0.40569977074861524\n",
      "27 0.3995423302054405\n",
      "28 0.40730577528476714\n",
      "29 0.41033606499433517\n",
      "30 0.4285425943136215\n",
      "31 0.42093664437532424\n",
      "32 0.4489751309156418\n",
      "33 0.455598766207695\n",
      "34 0.45849161356687546\n",
      "35 0.45891284227371215\n",
      "36 0.49224611490964887\n",
      "37 0.4823663219809532\n",
      "38 0.5020746463537216\n",
      "39 0.5254346367716789\n",
      "40 0.5406600472331047\n",
      "41 0.5733646655082703\n",
      "42 0.5831157904863358\n",
      "43 0.6092907825112343\n",
      "44 0.6276207572221756\n",
      "45 0.6606450748443603\n",
      "46 0.6580551138520241\n",
      "47 0.6750517874956131\n",
      "48 0.7111295330524444\n",
      "49 0.7308318889141083\n",
      "50 0.7439472043514251\n",
      "51 0.7692790251970291\n",
      "52 0.794753623008728\n",
      "53 0.7959684664011002\n",
      "54 0.8077172780036926\n",
      "55 0.8385486304759979\n",
      "56 0.8652685594558716\n",
      "57 0.8791917389631272\n",
      "58 0.8826097416877746\n",
      "59 0.9111143785715103\n",
      "60 0.9389272528886795\n",
      "61 0.9404128444194794\n",
      "62 0.9658154660463333\n",
      "63 0.9868335765600205\n",
      "64 0.9622238820791245\n",
      "65 0.9964051133394242\n",
      "66 0.990059654712677\n",
      "67 1.0290608090162277\n",
      "68 1.0452762407064438\n",
      "69 1.053876496553421\n",
      "70 1.0624376237392426\n",
      "71 1.0750755620002748\n",
      "72 1.0904374945163726\n",
      "73 1.1134421414136886\n",
      "74 1.0648813241720199\n",
      "75 1.0652469617128373\n",
      "76 1.077180022597313\n",
      "77 1.0945586395263671\n",
      "78 1.094592809677124\n",
      "79 1.1137023240327835\n",
      "80 1.134889930486679\n",
      "81 1.1403418505191802\n",
      "82 1.1517042189836502\n",
      "83 1.1522696703672408\n",
      "84 1.1671576011180878\n",
      "85 1.1838459038734437\n",
      "86 1.1682420349121094\n",
      "87 1.1643939304351807\n",
      "88 1.1292066496610642\n",
      "89 1.1490014338493346\n",
      "90 1.1577591985464095\n",
      "91 1.1716868877410889\n",
      "92 1.1784693223237992\n",
      "93 1.1827325600385665\n",
      "94 1.1845523416996002\n",
      "95 1.1864107131958008\n",
      "96 1.1920306879281997\n",
      "97 1.2018430948257446\n",
      "98 1.1994284921884537\n",
      "99 1.2124728244543075\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 9s 789us/step - loss: 0.7928 - acc: 0.7472 - val_loss: 0.4431 - val_acc: 0.9000\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.4137 - acc: 0.9000 - val_loss: 0.4380 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4078 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4079 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.4045 - acc: 0.9000 - val_loss: 0.4250 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 9s 763us/step - loss: 0.8096 - acc: 0.7335 - val_loss: 0.4704 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.4166 - acc: 0.9000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.4084 - acc: 0.9000 - val_loss: 0.4252 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.4060 - acc: 0.9000 - val_loss: 0.4445 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 2s 144us/step - loss: 0.4053 - acc: 0.9000 - val_loss: 0.4340 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.4029 - acc: 0.9000 - val_loss: 0.4347 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.3992 - acc: 0.9000 - val_loss: 0.4346 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.3938 - acc: 0.9000 - val_loss: 0.4409 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.3851 - acc: 0.9000 - val_loss: 0.4277 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3758 - acc: 0.9000 - val_loss: 0.4183 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 10s 805us/step - loss: 0.8118 - acc: 0.7323 - val_loss: 0.4456 - val_acc: 0.9000\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.4158 - acc: 0.9000 - val_loss: 0.4291 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.4088 - acc: 0.9000 - val_loss: 0.4427 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 2s 149us/step - loss: 0.4068 - acc: 0.9000 - val_loss: 0.4307 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.4050 - acc: 0.9000 - val_loss: 0.4355 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.4027 - acc: 0.9000 - val_loss: 0.4248 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.4006 - acc: 0.9000 - val_loss: 0.4282 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.3954 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3884 - acc: 0.9000 - val_loss: 0.4226 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 2s 144us/step - loss: 0.3805 - acc: 0.9000 - val_loss: 0.4234 - val_acc: 0.9000\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.3707 - acc: 0.9000 - val_loss: 0.4196 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.3614 - acc: 0.9000 - val_loss: 0.4170 - val_acc: 0.8999\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 2s 146us/step - loss: 0.3529 - acc: 0.9000 - val_loss: 0.4076 - val_acc: 0.8998\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3457 - acc: 0.9001 - val_loss: 0.4076 - val_acc: 0.8990\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3397 - acc: 0.9004 - val_loss: 0.3959 - val_acc: 0.8996\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3322 - acc: 0.9012 - val_loss: 0.4032 - val_acc: 0.8993\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.3255 - acc: 0.9016 - val_loss: 0.3986 - val_acc: 0.8975\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3185 - acc: 0.9025 - val_loss: 0.4082 - val_acc: 0.8964\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.3123 - acc: 0.9037 - val_loss: 0.4011 - val_acc: 0.8963\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3039 - acc: 0.9060 - val_loss: 0.4139 - val_acc: 0.8928\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 9s 762us/step - loss: 0.7924 - acc: 0.7442 - val_loss: 0.4551 - val_acc: 0.9000\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.4135 - acc: 0.9000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.4088 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.4058 - acc: 0.9000 - val_loss: 0.4420 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.4044 - acc: 0.9000 - val_loss: 0.4395 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.4018 - acc: 0.9000 - val_loss: 0.4343 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3979 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3917 - acc: 0.9000 - val_loss: 0.4226 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.3820 - acc: 0.9000 - val_loss: 0.4185 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3713 - acc: 0.9000 - val_loss: 0.4168 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3632 - acc: 0.9000 - val_loss: 0.4057 - val_acc: 0.8998\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3561 - acc: 0.9000 - val_loss: 0.4136 - val_acc: 0.8995\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3498 - acc: 0.9000 - val_loss: 0.4042 - val_acc: 0.8996\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.3416 - acc: 0.9003 - val_loss: 0.4059 - val_acc: 0.8990\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.3349 - acc: 0.9007 - val_loss: 0.4094 - val_acc: 0.8975\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3285 - acc: 0.9012 - val_loss: 0.4045 - val_acc: 0.8958\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.3228 - acc: 0.9023 - val_loss: 0.4057 - val_acc: 0.8975\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 1s 123us/step - loss: 0.3150 - acc: 0.9035 - val_loss: 0.4045 - val_acc: 0.8970\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.3077 - acc: 0.9050 - val_loss: 0.4051 - val_acc: 0.8938\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.2989 - acc: 0.9069 - val_loss: 0.4117 - val_acc: 0.8963\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.2901 - acc: 0.9094 - val_loss: 0.4184 - val_acc: 0.8906\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.2773 - acc: 0.9128 - val_loss: 0.4254 - val_acc: 0.8920\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.2649 - acc: 0.9167 - val_loss: 0.4395 - val_acc: 0.8896\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.2515 - acc: 0.9213 - val_loss: 0.4674 - val_acc: 0.8874\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.2371 - acc: 0.9258 - val_loss: 0.4744 - val_acc: 0.8845\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.2216 - acc: 0.9315 - val_loss: 0.4833 - val_acc: 0.8853\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.2044 - acc: 0.9368 - val_loss: 0.5218 - val_acc: 0.8806\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.1904 - acc: 0.9415 - val_loss: 0.5370 - val_acc: 0.8780\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.1767 - acc: 0.9459 - val_loss: 0.5516 - val_acc: 0.8765\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.1628 - acc: 0.9500 - val_loss: 0.5857 - val_acc: 0.8746\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 9s 767us/step - loss: 0.8392 - acc: 0.7225 - val_loss: 0.4380 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.4152 - acc: 0.9000 - val_loss: 0.4246 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.4097 - acc: 0.9000 - val_loss: 0.4352 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.4065 - acc: 0.9000 - val_loss: 0.4278 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.4048 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.4033 - acc: 0.9000 - val_loss: 0.4323 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4286 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.3998 - acc: 0.9000 - val_loss: 0.4353 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 2s 146us/step - loss: 0.3932 - acc: 0.9000 - val_loss: 0.4351 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.3857 - acc: 0.9000 - val_loss: 0.4379 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.3766 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.3658 - acc: 0.9000 - val_loss: 0.4078 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 2s 148us/step - loss: 0.3574 - acc: 0.9000 - val_loss: 0.4116 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.3509 - acc: 0.9001 - val_loss: 0.4028 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 2s 151us/step - loss: 0.3447 - acc: 0.9001 - val_loss: 0.4086 - val_acc: 0.8996\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.3369 - acc: 0.9004 - val_loss: 0.4040 - val_acc: 0.8992\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.3308 - acc: 0.9011 - val_loss: 0.4051 - val_acc: 0.8979\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.3233 - acc: 0.9020 - val_loss: 0.4075 - val_acc: 0.8961\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.3160 - acc: 0.9034 - val_loss: 0.3908 - val_acc: 0.8977\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3099 - acc: 0.9049 - val_loss: 0.4056 - val_acc: 0.8973\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 2s 144us/step - loss: 0.3018 - acc: 0.9066 - val_loss: 0.4046 - val_acc: 0.8951\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.2910 - acc: 0.9094 - val_loss: 0.4175 - val_acc: 0.8922\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.2816 - acc: 0.9124 - val_loss: 0.4208 - val_acc: 0.8921\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.2687 - acc: 0.9163 - val_loss: 0.4372 - val_acc: 0.8897\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.2560 - acc: 0.9196 - val_loss: 0.4495 - val_acc: 0.8899\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.2391 - acc: 0.9255 - val_loss: 0.4758 - val_acc: 0.8839\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.2251 - acc: 0.9301 - val_loss: 0.4867 - val_acc: 0.8841\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.2100 - acc: 0.9347 - val_loss: 0.5063 - val_acc: 0.8820\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.1930 - acc: 0.9404 - val_loss: 0.5193 - val_acc: 0.8821\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.1800 - acc: 0.9441 - val_loss: 0.5502 - val_acc: 0.8762\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 2s 146us/step - loss: 0.1651 - acc: 0.9490 - val_loss: 0.5755 - val_acc: 0.8785\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.1536 - acc: 0.9525 - val_loss: 0.6033 - val_acc: 0.8704\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.1421 - acc: 0.9557 - val_loss: 0.6233 - val_acc: 0.8752\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.1288 - acc: 0.9596 - val_loss: 0.6594 - val_acc: 0.8715\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.1165 - acc: 0.9635 - val_loss: 0.6918 - val_acc: 0.8678\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.1089 - acc: 0.9652 - val_loss: 0.6947 - val_acc: 0.8711\n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0967 - acc: 0.9692 - val_loss: 0.7500 - val_acc: 0.8695\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.0887 - acc: 0.9715 - val_loss: 0.7504 - val_acc: 0.8708\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.0873 - acc: 0.9714 - val_loss: 0.7780 - val_acc: 0.8641\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0755 - acc: 0.9753 - val_loss: 0.8112 - val_acc: 0.8683\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0659 - acc: 0.9784 - val_loss: 0.8375 - val_acc: 0.8690\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0571 - acc: 0.9817 - val_loss: 0.8703 - val_acc: 0.8638\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0474 - acc: 0.9850 - val_loss: 0.8968 - val_acc: 0.8669\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0441 - acc: 0.9861 - val_loss: 0.9336 - val_acc: 0.8702\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0476 - acc: 0.9847 - val_loss: 0.9186 - val_acc: 0.8657\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0465 - acc: 0.9849 - val_loss: 0.9366 - val_acc: 0.8654\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0364 - acc: 0.9887 - val_loss: 0.9784 - val_acc: 0.8616\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0290 - acc: 0.9915 - val_loss: 0.9964 - val_acc: 0.8636\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.0275 - acc: 0.9927 - val_loss: 1.0155 - val_acc: 0.8640\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0251 - acc: 0.9933 - val_loss: 1.0314 - val_acc: 0.8637\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 9s 784us/step - loss: 0.7857 - acc: 0.7472 - val_loss: 0.4589 - val_acc: 0.9000\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4152 - acc: 0.9000 - val_loss: 0.4323 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.4079 - acc: 0.9000 - val_loss: 0.4384 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4227 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4230 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.4021 - acc: 0.9000 - val_loss: 0.4250 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3983 - acc: 0.9000 - val_loss: 0.4369 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3915 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3808 - acc: 0.9000 - val_loss: 0.4139 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3686 - acc: 0.9000 - val_loss: 0.4140 - val_acc: 0.9000\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3594 - acc: 0.9000 - val_loss: 0.4086 - val_acc: 0.9000\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3526 - acc: 0.9000 - val_loss: 0.4020 - val_acc: 0.8996\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3451 - acc: 0.9001 - val_loss: 0.3992 - val_acc: 0.8996\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3385 - acc: 0.9006 - val_loss: 0.3990 - val_acc: 0.8993\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3317 - acc: 0.9009 - val_loss: 0.3883 - val_acc: 0.8983\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3267 - acc: 0.9018 - val_loss: 0.3957 - val_acc: 0.8970\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3210 - acc: 0.9027 - val_loss: 0.3968 - val_acc: 0.8975\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3139 - acc: 0.9038 - val_loss: 0.3948 - val_acc: 0.8960\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3060 - acc: 0.9058 - val_loss: 0.3932 - val_acc: 0.8955\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3012 - acc: 0.9068 - val_loss: 0.3989 - val_acc: 0.8939\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.2897 - acc: 0.9098 - val_loss: 0.4262 - val_acc: 0.8922\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.2788 - acc: 0.9127 - val_loss: 0.4168 - val_acc: 0.8933\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.2684 - acc: 0.9167 - val_loss: 0.4168 - val_acc: 0.8912\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.2557 - acc: 0.9202 - val_loss: 0.4468 - val_acc: 0.8861\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.2415 - acc: 0.9251 - val_loss: 0.4570 - val_acc: 0.8842\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.2257 - acc: 0.9307 - val_loss: 0.4762 - val_acc: 0.8794\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.2128 - acc: 0.9346 - val_loss: 0.4947 - val_acc: 0.8812\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1976 - acc: 0.9399 - val_loss: 0.5318 - val_acc: 0.8775\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.1858 - acc: 0.9439 - val_loss: 0.5383 - val_acc: 0.8784\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1713 - acc: 0.9483 - val_loss: 0.5654 - val_acc: 0.8802\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.1618 - acc: 0.9506 - val_loss: 0.5931 - val_acc: 0.8792\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.1493 - acc: 0.9541 - val_loss: 0.6200 - val_acc: 0.8744\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.1372 - acc: 0.9578 - val_loss: 0.6443 - val_acc: 0.8775\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.1286 - acc: 0.9600 - val_loss: 0.6580 - val_acc: 0.8699\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1298 - acc: 0.9588 - val_loss: 0.6724 - val_acc: 0.8709\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1111 - acc: 0.9650 - val_loss: 0.7051 - val_acc: 0.8704\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.1022 - acc: 0.9677 - val_loss: 0.7178 - val_acc: 0.8713\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0967 - acc: 0.9689 - val_loss: 0.7352 - val_acc: 0.8726\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0834 - acc: 0.9730 - val_loss: 0.7760 - val_acc: 0.8683\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0732 - acc: 0.9769 - val_loss: 0.8078 - val_acc: 0.8664\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0666 - acc: 0.9782 - val_loss: 0.8367 - val_acc: 0.8703\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0624 - acc: 0.9795 - val_loss: 0.8537 - val_acc: 0.8652\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.0632 - acc: 0.9790 - val_loss: 0.8606 - val_acc: 0.8611\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.0562 - acc: 0.9819 - val_loss: 0.8764 - val_acc: 0.8665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0443 - acc: 0.9861 - val_loss: 0.9067 - val_acc: 0.8677\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0383 - acc: 0.9881 - val_loss: 0.9411 - val_acc: 0.8671\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.0338 - acc: 0.9898 - val_loss: 0.9782 - val_acc: 0.8615\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0319 - acc: 0.9908 - val_loss: 0.9836 - val_acc: 0.8587\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.0316 - acc: 0.9908 - val_loss: 0.9731 - val_acc: 0.8627\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 0.9838 - val_acc: 0.8620\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.0310 - acc: 0.9907 - val_loss: 1.0134 - val_acc: 0.8620\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0256 - acc: 0.9928 - val_loss: 1.0248 - val_acc: 0.8620\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.0224 - acc: 0.9942 - val_loss: 1.0428 - val_acc: 0.8657\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0165 - acc: 0.9960 - val_loss: 1.0596 - val_acc: 0.8676\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 1.0716 - val_acc: 0.8645\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0146 - acc: 0.9966 - val_loss: 1.0868 - val_acc: 0.8656\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 1.0926 - val_acc: 0.8650\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.0111 - acc: 0.9975 - val_loss: 1.1034 - val_acc: 0.8662\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0106 - acc: 0.9974 - val_loss: 1.1179 - val_acc: 0.8689\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.0102 - acc: 0.9975 - val_loss: 1.1328 - val_acc: 0.8664\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0092 - acc: 0.9977 - val_loss: 1.1320 - val_acc: 0.8645\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 2s 149us/step - loss: 0.0093 - acc: 0.9977 - val_loss: 1.1365 - val_acc: 0.8688\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 1.1402 - val_acc: 0.8676\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.0124 - acc: 0.9968 - val_loss: 1.1260 - val_acc: 0.8617\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 1.1207 - val_acc: 0.8637\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 2s 149us/step - loss: 0.0491 - acc: 0.9838 - val_loss: 1.0640 - val_acc: 0.8595\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0345 - acc: 0.9890 - val_loss: 1.0599 - val_acc: 0.8572\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 2s 144us/step - loss: 0.0207 - acc: 0.9941 - val_loss: 1.0904 - val_acc: 0.8600\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.0123 - acc: 0.9968 - val_loss: 1.1142 - val_acc: 0.8649\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 2s 144us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 1.1201 - val_acc: 0.8666\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 2s 153us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 1.1374 - val_acc: 0.8683\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.0080 - acc: 0.9979 - val_loss: 1.1485 - val_acc: 0.8680\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.0076 - acc: 0.9979 - val_loss: 1.1550 - val_acc: 0.8663\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0076 - acc: 0.9979 - val_loss: 1.1609 - val_acc: 0.8684\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 1.1641 - val_acc: 0.8691\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 1.1654 - val_acc: 0.8692\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.0070 - acc: 0.9979 - val_loss: 1.1719 - val_acc: 0.8697\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 1.1693 - val_acc: 0.8662\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 2s 141us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 1.1742 - val_acc: 0.8694\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 1.1708 - val_acc: 0.8683\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 10s 805us/step - loss: 0.8073 - acc: 0.7361 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.4149 - acc: 0.9000 - val_loss: 0.4254 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.4081 - acc: 0.9000 - val_loss: 0.4302 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.4066 - acc: 0.9000 - val_loss: 0.4436 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.4037 - acc: 0.9000 - val_loss: 0.4386 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.4023 - acc: 0.9000 - val_loss: 0.4335 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3988 - acc: 0.9000 - val_loss: 0.4275 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3915 - acc: 0.9000 - val_loss: 0.4322 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.3812 - acc: 0.9000 - val_loss: 0.4145 - val_acc: 0.9000\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3685 - acc: 0.9000 - val_loss: 0.4163 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.3577 - acc: 0.9000 - val_loss: 0.4017 - val_acc: 0.8995\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3512 - acc: 0.9000 - val_loss: 0.4022 - val_acc: 0.8999\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3451 - acc: 0.9002 - val_loss: 0.4083 - val_acc: 0.8994\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.3384 - acc: 0.9005 - val_loss: 0.3984 - val_acc: 0.8983\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3319 - acc: 0.9011 - val_loss: 0.4020 - val_acc: 0.8983\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.3258 - acc: 0.9019 - val_loss: 0.3958 - val_acc: 0.8975\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3208 - acc: 0.9027 - val_loss: 0.4016 - val_acc: 0.8972\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.3143 - acc: 0.9040 - val_loss: 0.3982 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.3065 - acc: 0.9056 - val_loss: 0.4018 - val_acc: 0.8960\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.2995 - acc: 0.9067 - val_loss: 0.4092 - val_acc: 0.8916\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.2888 - acc: 0.9097 - val_loss: 0.4171 - val_acc: 0.8926\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 2s 125us/step - loss: 0.2787 - acc: 0.9127 - val_loss: 0.4149 - val_acc: 0.8913\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.2659 - acc: 0.9165 - val_loss: 0.4452 - val_acc: 0.8880\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 1s 125us/step - loss: 0.2525 - acc: 0.9212 - val_loss: 0.4546 - val_acc: 0.8880\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.2399 - acc: 0.9258 - val_loss: 0.4582 - val_acc: 0.8843\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.2241 - acc: 0.9306 - val_loss: 0.4831 - val_acc: 0.8862\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.2078 - acc: 0.9363 - val_loss: 0.5108 - val_acc: 0.8822\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.1960 - acc: 0.9398 - val_loss: 0.5290 - val_acc: 0.8764\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.1818 - acc: 0.9440 - val_loss: 0.5479 - val_acc: 0.8799\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.1671 - acc: 0.9492 - val_loss: 0.5769 - val_acc: 0.8747\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.1557 - acc: 0.9521 - val_loss: 0.5983 - val_acc: 0.8741\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.1449 - acc: 0.9554 - val_loss: 0.6223 - val_acc: 0.8727\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.1300 - acc: 0.9601 - val_loss: 0.6598 - val_acc: 0.8741\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.1234 - acc: 0.9617 - val_loss: 0.6649 - val_acc: 0.8751\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.1142 - acc: 0.9639 - val_loss: 0.6910 - val_acc: 0.8693\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.1037 - acc: 0.9674 - val_loss: 0.7281 - val_acc: 0.8721\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0930 - acc: 0.9705 - val_loss: 0.7673 - val_acc: 0.8700\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0853 - acc: 0.9723 - val_loss: 0.7760 - val_acc: 0.8695\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.0765 - acc: 0.9757 - val_loss: 0.8071 - val_acc: 0.8709\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 2s 136us/step - loss: 0.0728 - acc: 0.9763 - val_loss: 0.8274 - val_acc: 0.8590\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0645 - acc: 0.9787 - val_loss: 0.8503 - val_acc: 0.8627\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 2s 149us/step - loss: 0.0555 - acc: 0.9819 - val_loss: 0.8814 - val_acc: 0.8649\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0533 - acc: 0.9828 - val_loss: 0.9031 - val_acc: 0.8678\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 2s 140us/step - loss: 0.0443 - acc: 0.9859 - val_loss: 0.9164 - val_acc: 0.8647\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 2s 138us/step - loss: 0.0389 - acc: 0.9880 - val_loss: 0.9394 - val_acc: 0.8662\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0346 - acc: 0.9896 - val_loss: 0.9709 - val_acc: 0.8626\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 2s 146us/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.9879 - val_acc: 0.8627\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 2s 135us/step - loss: 0.0247 - acc: 0.9931 - val_loss: 1.0240 - val_acc: 0.8661\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.0246 - acc: 0.9932 - val_loss: 1.0167 - val_acc: 0.8634\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.0226 - acc: 0.9939 - val_loss: 1.0486 - val_acc: 0.8592\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0211 - acc: 0.9945 - val_loss: 1.0621 - val_acc: 0.8651\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 2s 145us/step - loss: 0.0194 - acc: 0.9951 - val_loss: 1.0573 - val_acc: 0.8592\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0280 - acc: 0.9920 - val_loss: 1.0520 - val_acc: 0.8625\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.0306 - acc: 0.9910 - val_loss: 1.0498 - val_acc: 0.8661\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 2s 139us/step - loss: 0.0315 - acc: 0.9909 - val_loss: 1.0480 - val_acc: 0.8628\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 1s 124us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 1.0710 - val_acc: 0.8620\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0158 - acc: 0.9962 - val_loss: 1.0894 - val_acc: 0.8656\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 2s 147us/step - loss: 0.0129 - acc: 0.9969 - val_loss: 1.1016 - val_acc: 0.8637\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0118 - acc: 0.9972 - val_loss: 1.1162 - val_acc: 0.8650\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 2s 146us/step - loss: 0.0101 - acc: 0.9975 - val_loss: 1.1231 - val_acc: 0.8672\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0096 - acc: 0.9977 - val_loss: 1.1165 - val_acc: 0.8666\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 2s 134us/step - loss: 0.0100 - acc: 0.9975 - val_loss: 1.1352 - val_acc: 0.8679\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 2s 142us/step - loss: 0.0090 - acc: 0.9977 - val_loss: 1.1423 - val_acc: 0.8666\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.0090 - acc: 0.9977 - val_loss: 1.1517 - val_acc: 0.8687\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0085 - acc: 0.9977 - val_loss: 1.1578 - val_acc: 0.8677\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0083 - acc: 0.9977 - val_loss: 1.1605 - val_acc: 0.8703\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0076 - acc: 0.9980 - val_loss: 1.1635 - val_acc: 0.8676\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.0081 - acc: 0.9978 - val_loss: 1.1708 - val_acc: 0.8692\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0082 - acc: 0.9978 - val_loss: 1.1769 - val_acc: 0.8683\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0297 - acc: 0.9901 - val_loss: 1.0923 - val_acc: 0.8604\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0600 - acc: 0.9795 - val_loss: 1.0616 - val_acc: 0.8539\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0219 - acc: 0.9935 - val_loss: 1.0815 - val_acc: 0.8652\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0101 - acc: 0.9975 - val_loss: 1.1222 - val_acc: 0.8665\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0083 - acc: 0.9977 - val_loss: 1.1359 - val_acc: 0.8666\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 1.1364 - val_acc: 0.8656\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 1.1564 - val_acc: 0.8688\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 1.1631 - val_acc: 0.8693\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0059 - acc: 0.9981 - val_loss: 1.1641 - val_acc: 0.8698\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0058 - acc: 0.9981 - val_loss: 1.1723 - val_acc: 0.8705\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 1.1712 - val_acc: 0.8701\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0059 - acc: 0.9981 - val_loss: 1.1818 - val_acc: 0.8694\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0059 - acc: 0.9981 - val_loss: 1.1873 - val_acc: 0.8701\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0065 - acc: 0.9979 - val_loss: 1.1851 - val_acc: 0.8662\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0062 - acc: 0.9981 - val_loss: 1.1925 - val_acc: 0.8687\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 1.1895 - val_acc: 0.8711\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 2s 129us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 1.1906 - val_acc: 0.8659\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 1.1990 - val_acc: 0.8662\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 1.1965 - val_acc: 0.8670\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0422 - acc: 0.9858 - val_loss: 1.0736 - val_acc: 0.8547\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0436 - acc: 0.9851 - val_loss: 1.0926 - val_acc: 0.8612\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0167 - acc: 0.9951 - val_loss: 1.1031 - val_acc: 0.8661\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0090 - acc: 0.9974 - val_loss: 1.1301 - val_acc: 0.8657\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 1.1546 - val_acc: 0.8651\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 2s 126us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 1.1689 - val_acc: 0.8692\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 1.1729 - val_acc: 0.8694\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 2s 132us/step - loss: 0.0053 - acc: 0.9980 - val_loss: 1.1814 - val_acc: 0.8706\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 2s 128us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 1.1889 - val_acc: 0.8697\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 2s 133us/step - loss: 0.0051 - acc: 0.9980 - val_loss: 1.1899 - val_acc: 0.8694\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 2s 130us/step - loss: 0.0050 - acc: 0.9980 - val_loss: 1.1990 - val_acc: 0.8686\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 2s 131us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 1.1927 - val_acc: 0.8691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_73 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_71/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_72/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_75 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_73/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_74/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_77 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_75/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_76/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_79 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_77/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_78/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_81 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_79/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_80/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_83 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_81/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_82/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_85 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_83/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_84/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_73 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_144:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_145:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_75 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_148:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_149:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_77 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_152:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_153:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_79 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_156:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_157:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_81 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_160:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_161:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_83 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_164:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_165:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_85 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_168:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_169:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4411045375466347\n",
      "1 0.42543904572725294\n",
      "2 0.43022155404090884\n",
      "3 0.4436313459277153\n",
      "4 0.43855359971523283\n",
      "5 0.43350892663002016\n",
      "6 0.4274733790755272\n",
      "7 0.4322101885080338\n",
      "8 0.4145178559422493\n",
      "9 0.4163499513268471\n",
      "10 0.4017495822906494\n",
      "11 0.40216461807489395\n",
      "12 0.40826385229825973\n",
      "13 0.39843378514051436\n",
      "14 0.4020489400625229\n",
      "15 0.3958102747797966\n",
      "16 0.4016014763712883\n",
      "17 0.3981529098749161\n",
      "18 0.401758109331131\n",
      "19 0.40920518934726713\n",
      "20 0.4171276423335075\n",
      "21 0.4148639789223671\n",
      "22 0.4452424618601799\n",
      "23 0.45461991757154463\n",
      "24 0.45823596000671385\n",
      "25 0.48308271944522857\n",
      "26 0.5108322420716286\n",
      "27 0.5290171441435814\n",
      "28 0.5479166522622109\n",
      "29 0.5768823054432869\n",
      "30 0.5982920676469803\n",
      "31 0.6222731390595436\n",
      "32 0.6597595238685607\n",
      "33 0.6649079683423043\n",
      "34 0.6910386914014817\n",
      "35 0.7280827432870864\n",
      "36 0.7672632163763047\n",
      "37 0.7760462039709091\n",
      "38 0.8071038544178009\n",
      "39 0.827384026646614\n",
      "40 0.8502910828590393\n",
      "41 0.88140988945961\n",
      "42 0.9030774182081223\n",
      "43 0.916374083161354\n",
      "44 0.9393868285417557\n",
      "45 0.970902972817421\n",
      "46 0.9878889948129654\n",
      "47 1.0239635491371155\n",
      "48 1.0166913568973541\n",
      "49 1.0485944491624832\n",
      "50 1.0621178287267685\n",
      "51 1.057324337363243\n",
      "52 1.0519931197166443\n",
      "53 1.0497920835018157\n",
      "54 1.0479558557271957\n",
      "55 1.0710187834501266\n",
      "56 1.0894212132692338\n",
      "57 1.1015967953205108\n",
      "58 1.1161882495880127\n",
      "59 1.1231468349695206\n",
      "60 1.1164847856760025\n",
      "61 1.1351668751239776\n",
      "62 1.142261824607849\n",
      "63 1.1516540956497192\n",
      "64 1.157808683514595\n",
      "65 1.1605330091714858\n",
      "66 1.1634945100545884\n",
      "67 1.1708496248722076\n",
      "68 1.1768808978796006\n",
      "69 1.0923207116127014\n",
      "70 1.0615917134284973\n",
      "71 1.0814947032928466\n",
      "72 1.12215038895607\n",
      "73 1.135904735326767\n",
      "74 1.1363776433467865\n",
      "75 1.156382148861885\n",
      "76 1.1631352865695954\n",
      "77 1.1641392117738725\n",
      "78 1.1722935712337494\n",
      "79 1.171236545443535\n",
      "80 1.1818046587705613\n",
      "81 1.1872966408729553\n",
      "82 1.1850527614355086\n",
      "83 1.1924590456485749\n",
      "84 1.1895390665531158\n",
      "85 1.1905658239126204\n",
      "86 1.1989522880315782\n",
      "87 1.1964737886190415\n",
      "88 1.0736137485504151\n",
      "89 1.092570338845253\n",
      "90 1.1030518579483033\n",
      "91 1.130147521495819\n",
      "92 1.1545699852705003\n",
      "93 1.1689044398069381\n",
      "94 1.172853006720543\n",
      "95 1.1813961386680603\n",
      "96 1.188938021659851\n",
      "97 1.1899343812465668\n",
      "98 1.1989529967308044\n",
      "99 1.1926925241947175\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "12000/12000 [==============================] - 11s 946us/step - loss: 0.7560 - acc: 0.7553 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 2/5\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4371 - val_acc: 0.9000\n",
      "Epoch 3/5\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.4073 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.4051 - acc: 0.9000 - val_loss: 0.4167 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.4024 - acc: 0.9000 - val_loss: 0.4254 - val_acc: 0.9000\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 12s 966us/step - loss: 0.7414 - acc: 0.7556 - val_loss: 0.4318 - val_acc: 0.9000\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.4073 - acc: 0.9000 - val_loss: 0.4296 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4398 - val_acc: 0.9000\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.4014 - acc: 0.9000 - val_loss: 0.4345 - val_acc: 0.9000\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.3941 - acc: 0.9000 - val_loss: 0.4325 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.3796 - acc: 0.9000 - val_loss: 0.4128 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.3639 - acc: 0.9000 - val_loss: 0.3986 - val_acc: 0.8999\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.3567 - acc: 0.9000 - val_loss: 0.4030 - val_acc: 0.9000\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.3471 - acc: 0.9001 - val_loss: 0.3910 - val_acc: 0.8997\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 12s 1ms/step - loss: 0.7662 - acc: 0.7501 - val_loss: 0.4376 - val_acc: 0.9000\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.4126 - acc: 0.9000 - val_loss: 0.4650 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.4078 - acc: 0.9000 - val_loss: 0.4334 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.4058 - acc: 0.9000 - val_loss: 0.4362 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.3966 - acc: 0.9000 - val_loss: 0.4357 - val_acc: 0.9000\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3824 - acc: 0.9000 - val_loss: 0.4319 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.3687 - acc: 0.9000 - val_loss: 0.4029 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.3590 - acc: 0.9000 - val_loss: 0.4097 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.3511 - acc: 0.9000 - val_loss: 0.3890 - val_acc: 0.8996\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.3440 - acc: 0.9002 - val_loss: 0.3994 - val_acc: 0.8999\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3370 - acc: 0.9005 - val_loss: 0.4000 - val_acc: 0.8990\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.3307 - acc: 0.9014 - val_loss: 0.3994 - val_acc: 0.8965\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.3241 - acc: 0.9018 - val_loss: 0.3924 - val_acc: 0.8973\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.3172 - acc: 0.9035 - val_loss: 0.3972 - val_acc: 0.8956\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.3098 - acc: 0.9052 - val_loss: 0.4074 - val_acc: 0.8959\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.3008 - acc: 0.9074 - val_loss: 0.4105 - val_acc: 0.8939\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.2902 - acc: 0.9099 - val_loss: 0.4221 - val_acc: 0.8914\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.2774 - acc: 0.9143 - val_loss: 0.4251 - val_acc: 0.8919\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.2622 - acc: 0.9186 - val_loss: 0.4468 - val_acc: 0.8859\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "12000/12000 [==============================] - 12s 1ms/step - loss: 0.7929 - acc: 0.7407 - val_loss: 0.4377 - val_acc: 0.9000\n",
      "Epoch 2/30\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.4111 - acc: 0.9000 - val_loss: 0.4247 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.4085 - acc: 0.9000 - val_loss: 0.4283 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.4059 - acc: 0.9000 - val_loss: 0.4330 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.4032 - acc: 0.9000 - val_loss: 0.4354 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.3969 - acc: 0.9000 - val_loss: 0.4327 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.3847 - acc: 0.9000 - val_loss: 0.4144 - val_acc: 0.9000\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.3702 - acc: 0.9000 - val_loss: 0.4085 - val_acc: 0.9000\n",
      "Epoch 9/30\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.3602 - acc: 0.9000 - val_loss: 0.4094 - val_acc: 0.8999\n",
      "Epoch 10/30\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.3537 - acc: 0.9000 - val_loss: 0.4051 - val_acc: 0.8999\n",
      "Epoch 11/30\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.3444 - acc: 0.9001 - val_loss: 0.3934 - val_acc: 0.8996\n",
      "Epoch 12/30\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.3393 - acc: 0.9003 - val_loss: 0.3930 - val_acc: 0.8997\n",
      "Epoch 13/30\n",
      "12000/12000 [==============================] - 2s 183us/step - loss: 0.3329 - acc: 0.9012 - val_loss: 0.3939 - val_acc: 0.8989\n",
      "Epoch 14/30\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.3279 - acc: 0.9018 - val_loss: 0.4002 - val_acc: 0.8964\n",
      "Epoch 15/30\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.3231 - acc: 0.9025 - val_loss: 0.3924 - val_acc: 0.8975\n",
      "Epoch 16/30\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.3151 - acc: 0.9042 - val_loss: 0.3929 - val_acc: 0.8968\n",
      "Epoch 17/30\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.3076 - acc: 0.9059 - val_loss: 0.4018 - val_acc: 0.8960\n",
      "Epoch 18/30\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.2973 - acc: 0.9083 - val_loss: 0.4072 - val_acc: 0.8956\n",
      "Epoch 19/30\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.2866 - acc: 0.9112 - val_loss: 0.4186 - val_acc: 0.8928\n",
      "Epoch 20/30\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.2717 - acc: 0.9164 - val_loss: 0.4420 - val_acc: 0.8893\n",
      "Epoch 21/30\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.2561 - acc: 0.9211 - val_loss: 0.4498 - val_acc: 0.8841\n",
      "Epoch 22/30\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.2399 - acc: 0.9260 - val_loss: 0.4746 - val_acc: 0.8856\n",
      "Epoch 23/30\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.2223 - acc: 0.9322 - val_loss: 0.5003 - val_acc: 0.8840\n",
      "Epoch 24/30\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.2059 - acc: 0.9375 - val_loss: 0.5111 - val_acc: 0.8777\n",
      "Epoch 25/30\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.1899 - acc: 0.9419 - val_loss: 0.5444 - val_acc: 0.8814\n",
      "Epoch 26/30\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.1755 - acc: 0.9467 - val_loss: 0.5832 - val_acc: 0.8716\n",
      "Epoch 27/30\n",
      "12000/12000 [==============================] - 2s 204us/step - loss: 0.1600 - acc: 0.9509 - val_loss: 0.6106 - val_acc: 0.8753\n",
      "Epoch 28/30\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.1482 - acc: 0.9537 - val_loss: 0.6313 - val_acc: 0.8726\n",
      "Epoch 29/30\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.1369 - acc: 0.9573 - val_loss: 0.6577 - val_acc: 0.8717\n",
      "Epoch 30/30\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.1295 - acc: 0.9591 - val_loss: 0.6797 - val_acc: 0.8756\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.7980 - acc: 0.7382 - val_loss: 0.4361 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.4120 - acc: 0.9000 - val_loss: 0.4266 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.4077 - acc: 0.9000 - val_loss: 0.4292 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.4047 - acc: 0.9000 - val_loss: 0.4382 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.4018 - acc: 0.9000 - val_loss: 0.4591 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 2s 207us/step - loss: 0.3933 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.3790 - acc: 0.9000 - val_loss: 0.4188 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.3658 - acc: 0.9000 - val_loss: 0.4133 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.3584 - acc: 0.9000 - val_loss: 0.4032 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.3509 - acc: 0.9000 - val_loss: 0.3947 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 2s 182us/step - loss: 0.3440 - acc: 0.9000 - val_loss: 0.3931 - val_acc: 0.8996\n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 2s 182us/step - loss: 0.3356 - acc: 0.9007 - val_loss: 0.3899 - val_acc: 0.8986\n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.3290 - acc: 0.9013 - val_loss: 0.3950 - val_acc: 0.8980\n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.3229 - acc: 0.9025 - val_loss: 0.3975 - val_acc: 0.8977\n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 2s 204us/step - loss: 0.3153 - acc: 0.9037 - val_loss: 0.3957 - val_acc: 0.8978\n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3076 - acc: 0.9053 - val_loss: 0.3954 - val_acc: 0.8956\n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.2982 - acc: 0.9082 - val_loss: 0.4032 - val_acc: 0.8942\n",
      "Epoch 18/50\n",
      "12000/12000 [==============================] - 2s 208us/step - loss: 0.2869 - acc: 0.9115 - val_loss: 0.4221 - val_acc: 0.8899\n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.2726 - acc: 0.9151 - val_loss: 0.4340 - val_acc: 0.8897\n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.2580 - acc: 0.9198 - val_loss: 0.4501 - val_acc: 0.8864\n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.2405 - acc: 0.9256 - val_loss: 0.4745 - val_acc: 0.8831\n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.2230 - acc: 0.9318 - val_loss: 0.4954 - val_acc: 0.8826\n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 2s 179us/step - loss: 0.2097 - acc: 0.9361 - val_loss: 0.5165 - val_acc: 0.8823\n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 2s 183us/step - loss: 0.1900 - acc: 0.9424 - val_loss: 0.5579 - val_acc: 0.8766\n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 2s 182us/step - loss: 0.1807 - acc: 0.9451 - val_loss: 0.5633 - val_acc: 0.8804\n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.1650 - acc: 0.9497 - val_loss: 0.5951 - val_acc: 0.8775\n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.1493 - acc: 0.9540 - val_loss: 0.6243 - val_acc: 0.8757\n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.1372 - acc: 0.9577 - val_loss: 0.6481 - val_acc: 0.8734\n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.1273 - acc: 0.9599 - val_loss: 0.6844 - val_acc: 0.8726\n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.1124 - acc: 0.9642 - val_loss: 0.7151 - val_acc: 0.8699\n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.1082 - acc: 0.9654 - val_loss: 0.7442 - val_acc: 0.8739\n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0923 - acc: 0.9699 - val_loss: 0.7648 - val_acc: 0.8698\n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 2s 202us/step - loss: 0.0846 - acc: 0.9719 - val_loss: 0.7887 - val_acc: 0.8643\n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.0731 - acc: 0.9760 - val_loss: 0.8364 - val_acc: 0.8680\n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.0596 - acc: 0.9808 - val_loss: 0.8598 - val_acc: 0.8653\n",
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.0548 - acc: 0.9817 - val_loss: 0.8714 - val_acc: 0.8672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.0484 - acc: 0.9844 - val_loss: 0.9181 - val_acc: 0.8602\n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0411 - acc: 0.9871 - val_loss: 0.9461 - val_acc: 0.8626\n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.0374 - acc: 0.9884 - val_loss: 0.9340 - val_acc: 0.8597\n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0395 - acc: 0.9877 - val_loss: 0.9448 - val_acc: 0.8631\n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0318 - acc: 0.9906 - val_loss: 0.9796 - val_acc: 0.8639\n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.0204 - acc: 0.9947 - val_loss: 1.0147 - val_acc: 0.8669\n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 2s 183us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 1.0271 - val_acc: 0.8676\n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0127 - acc: 0.9969 - val_loss: 1.0658 - val_acc: 0.8670\n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0115 - acc: 0.9971 - val_loss: 1.0745 - val_acc: 0.8696\n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0103 - acc: 0.9973 - val_loss: 1.0842 - val_acc: 0.8663\n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.0098 - acc: 0.9975 - val_loss: 1.0981 - val_acc: 0.8698\n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.0097 - acc: 0.9974 - val_loss: 1.0986 - val_acc: 0.8699\n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 1.1149 - val_acc: 0.8677\n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 1.1143 - val_acc: 0.8619\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/80\n",
      "12000/12000 [==============================] - 12s 1ms/step - loss: 0.7590 - acc: 0.7545 - val_loss: 0.4240 - val_acc: 0.9000\n",
      "Epoch 2/80\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.4110 - acc: 0.9000 - val_loss: 0.4465 - val_acc: 0.9000\n",
      "Epoch 3/80\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.4087 - acc: 0.9000 - val_loss: 0.4322 - val_acc: 0.9000\n",
      "Epoch 4/80\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.4054 - acc: 0.9000 - val_loss: 0.4308 - val_acc: 0.9000\n",
      "Epoch 5/80\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4352 - val_acc: 0.9000\n",
      "Epoch 6/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.3938 - acc: 0.9000 - val_loss: 0.4205 - val_acc: 0.9000\n",
      "Epoch 7/80\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.3782 - acc: 0.9000 - val_loss: 0.4210 - val_acc: 0.9000\n",
      "Epoch 8/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.3674 - acc: 0.9000 - val_loss: 0.4017 - val_acc: 0.9000\n",
      "Epoch 9/80\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.3579 - acc: 0.9000 - val_loss: 0.4004 - val_acc: 0.9000\n",
      "Epoch 10/80\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3501 - acc: 0.9001 - val_loss: 0.4021 - val_acc: 0.8999\n",
      "Epoch 11/80\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3423 - acc: 0.9004 - val_loss: 0.3907 - val_acc: 0.8996\n",
      "Epoch 12/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.3347 - acc: 0.9009 - val_loss: 0.3905 - val_acc: 0.8992\n",
      "Epoch 13/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.3286 - acc: 0.9015 - val_loss: 0.3906 - val_acc: 0.8981\n",
      "Epoch 14/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.3226 - acc: 0.9023 - val_loss: 0.4048 - val_acc: 0.8960\n",
      "Epoch 15/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.3149 - acc: 0.9042 - val_loss: 0.3998 - val_acc: 0.8948\n",
      "Epoch 16/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.3055 - acc: 0.9058 - val_loss: 0.4078 - val_acc: 0.8925\n",
      "Epoch 17/80\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.2959 - acc: 0.9087 - val_loss: 0.4078 - val_acc: 0.8933\n",
      "Epoch 18/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.2828 - acc: 0.9122 - val_loss: 0.4210 - val_acc: 0.8929\n",
      "Epoch 19/80\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.2681 - acc: 0.9174 - val_loss: 0.4455 - val_acc: 0.8875\n",
      "Epoch 20/80\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.2516 - acc: 0.9230 - val_loss: 0.4498 - val_acc: 0.8873\n",
      "Epoch 21/80\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.2341 - acc: 0.9285 - val_loss: 0.4929 - val_acc: 0.8831\n",
      "Epoch 22/80\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.2172 - acc: 0.9338 - val_loss: 0.5037 - val_acc: 0.8818\n",
      "Epoch 23/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.2003 - acc: 0.9390 - val_loss: 0.5312 - val_acc: 0.8800\n",
      "Epoch 24/80\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.1887 - acc: 0.9421 - val_loss: 0.5652 - val_acc: 0.8747\n",
      "Epoch 25/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.1706 - acc: 0.9480 - val_loss: 0.5965 - val_acc: 0.8771\n",
      "Epoch 26/80\n",
      "12000/12000 [==============================] - 2s 206us/step - loss: 0.1562 - acc: 0.9524 - val_loss: 0.6113 - val_acc: 0.8754\n",
      "Epoch 27/80\n",
      "12000/12000 [==============================] - 2s 181us/step - loss: 0.1452 - acc: 0.9549 - val_loss: 0.6428 - val_acc: 0.8746\n",
      "Epoch 28/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.1304 - acc: 0.9595 - val_loss: 0.6700 - val_acc: 0.8732\n",
      "Epoch 29/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.1209 - acc: 0.9619 - val_loss: 0.6917 - val_acc: 0.8691\n",
      "Epoch 30/80\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.1164 - acc: 0.9629 - val_loss: 0.7104 - val_acc: 0.8693\n",
      "Epoch 31/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0987 - acc: 0.9685 - val_loss: 0.7548 - val_acc: 0.8705\n",
      "Epoch 32/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0901 - acc: 0.9706 - val_loss: 0.7974 - val_acc: 0.8638\n",
      "Epoch 33/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0827 - acc: 0.9731 - val_loss: 0.8142 - val_acc: 0.8685\n",
      "Epoch 34/80\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0693 - acc: 0.9771 - val_loss: 0.8359 - val_acc: 0.8631\n",
      "Epoch 35/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.0595 - acc: 0.9804 - val_loss: 0.8721 - val_acc: 0.8703\n",
      "Epoch 36/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0491 - acc: 0.9838 - val_loss: 0.9056 - val_acc: 0.8636\n",
      "Epoch 37/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.0422 - acc: 0.9862 - val_loss: 0.9429 - val_acc: 0.8640\n",
      "Epoch 38/80\n",
      "12000/12000 [==============================] - 3s 213us/step - loss: 0.0349 - acc: 0.9894 - val_loss: 0.9580 - val_acc: 0.8696\n",
      "Epoch 39/80\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0393 - acc: 0.9878 - val_loss: 0.9614 - val_acc: 0.8682\n",
      "Epoch 40/80\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.0293 - acc: 0.9913 - val_loss: 0.9777 - val_acc: 0.8666\n",
      "Epoch 41/80\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0212 - acc: 0.9942 - val_loss: 1.0278 - val_acc: 0.8655\n",
      "Epoch 42/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.0189 - acc: 0.9952 - val_loss: 1.0431 - val_acc: 0.8669\n",
      "Epoch 43/80\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0153 - acc: 0.9961 - val_loss: 1.0756 - val_acc: 0.8672\n",
      "Epoch 44/80\n",
      "12000/12000 [==============================] - 2s 180us/step - loss: 0.0145 - acc: 0.9962 - val_loss: 1.0757 - val_acc: 0.8712\n",
      "Epoch 45/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 179us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 1.0654 - val_acc: 0.8601\n",
      "Epoch 46/80\n",
      "12000/12000 [==============================] - 2s 182us/step - loss: 0.0249 - acc: 0.9927 - val_loss: 1.0300 - val_acc: 0.8668\n",
      "Epoch 47/80\n",
      "12000/12000 [==============================] - 2s 183us/step - loss: 0.0255 - acc: 0.9927 - val_loss: 1.0492 - val_acc: 0.8661\n",
      "Epoch 48/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0169 - acc: 0.9956 - val_loss: 1.0687 - val_acc: 0.8638\n",
      "Epoch 49/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0132 - acc: 0.9967 - val_loss: 1.0945 - val_acc: 0.8681\n",
      "Epoch 50/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0112 - acc: 0.9973 - val_loss: 1.1130 - val_acc: 0.8655\n",
      "Epoch 51/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.0194 - acc: 0.9943 - val_loss: 1.0587 - val_acc: 0.8636\n",
      "Epoch 52/80\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 1.0817 - val_acc: 0.8634\n",
      "Epoch 53/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0140 - acc: 0.9962 - val_loss: 1.0913 - val_acc: 0.8653\n",
      "Epoch 54/80\n",
      "12000/12000 [==============================] - 2s 200us/step - loss: 0.0129 - acc: 0.9966 - val_loss: 1.1085 - val_acc: 0.8642\n",
      "Epoch 55/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 1.1225 - val_acc: 0.8668\n",
      "Epoch 56/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0073 - acc: 0.9980 - val_loss: 1.1302 - val_acc: 0.8659\n",
      "Epoch 57/80\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.0072 - acc: 0.9978 - val_loss: 1.1504 - val_acc: 0.8701\n",
      "Epoch 58/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 1.1434 - val_acc: 0.8669\n",
      "Epoch 59/80\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 1.1542 - val_acc: 0.8688\n",
      "Epoch 60/80\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 1.1667 - val_acc: 0.8705\n",
      "Epoch 61/80\n",
      "12000/12000 [==============================] - 2s 183us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 1.1661 - val_acc: 0.8710\n",
      "Epoch 62/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 1.1677 - val_acc: 0.8690\n",
      "Epoch 63/80\n",
      "12000/12000 [==============================] - 2s 204us/step - loss: 0.0062 - acc: 0.9979 - val_loss: 1.1762 - val_acc: 0.8712\n",
      "Epoch 64/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 1.1723 - val_acc: 0.8722\n",
      "Epoch 65/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0054 - acc: 0.9980 - val_loss: 1.1826 - val_acc: 0.8710\n",
      "Epoch 66/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 1.1870 - val_acc: 0.8680\n",
      "Epoch 67/80\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.0053 - acc: 0.9981 - val_loss: 1.1951 - val_acc: 0.8715\n",
      "Epoch 68/80\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.0153 - acc: 0.9949 - val_loss: 1.0904 - val_acc: 0.8552\n",
      "Epoch 69/80\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0523 - acc: 0.9818 - val_loss: 1.0431 - val_acc: 0.8555\n",
      "Epoch 70/80\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.0255 - acc: 0.9919 - val_loss: 1.0869 - val_acc: 0.8628\n",
      "Epoch 71/80\n",
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.0109 - acc: 0.9968 - val_loss: 1.1256 - val_acc: 0.8666\n",
      "Epoch 72/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0070 - acc: 0.9979 - val_loss: 1.1375 - val_acc: 0.8675\n",
      "Epoch 73/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0054 - acc: 0.9980 - val_loss: 1.1574 - val_acc: 0.8702\n",
      "Epoch 74/80\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.0049 - acc: 0.9980 - val_loss: 1.1631 - val_acc: 0.8706\n",
      "Epoch 75/80\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 1.1756 - val_acc: 0.8710\n",
      "Epoch 76/80\n",
      "12000/12000 [==============================] - 2s 203us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 1.1682 - val_acc: 0.8718\n",
      "Epoch 77/80\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.0043 - acc: 0.9981 - val_loss: 1.1906 - val_acc: 0.8722\n",
      "Epoch 78/80\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0043 - acc: 0.9981 - val_loss: 1.1882 - val_acc: 0.8712\n",
      "Epoch 79/80\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0043 - acc: 0.9981 - val_loss: 1.1928 - val_acc: 0.8717\n",
      "Epoch 80/80\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0043 - acc: 0.9980 - val_loss: 1.1937 - val_acc: 0.8721\n",
      "Train on 12000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "12000/12000 [==============================] - 13s 1ms/step - loss: 0.7982 - acc: 0.7366 - val_loss: 0.4408 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.4138 - acc: 0.9000 - val_loss: 0.4361 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.4083 - acc: 0.9000 - val_loss: 0.4267 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.4054 - acc: 0.9000 - val_loss: 0.4304 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.4035 - acc: 0.9000 - val_loss: 0.4298 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.3979 - acc: 0.9000 - val_loss: 0.4356 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.3842 - acc: 0.9000 - val_loss: 0.4221 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.3703 - acc: 0.9000 - val_loss: 0.4231 - val_acc: 0.9000\n",
      "Epoch 9/100\n",
      "12000/12000 [==============================] - 2s 179us/step - loss: 0.3602 - acc: 0.9000 - val_loss: 0.4179 - val_acc: 0.8996\n",
      "Epoch 10/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.3518 - acc: 0.9000 - val_loss: 0.4012 - val_acc: 0.9000\n",
      "Epoch 11/100\n",
      "12000/12000 [==============================] - 2s 178us/step - loss: 0.3435 - acc: 0.9002 - val_loss: 0.3966 - val_acc: 0.8999\n",
      "Epoch 12/100\n",
      "12000/12000 [==============================] - 2s 180us/step - loss: 0.3381 - acc: 0.9005 - val_loss: 0.4069 - val_acc: 0.8997\n",
      "Epoch 13/100\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.3324 - acc: 0.9011 - val_loss: 0.3896 - val_acc: 0.8986\n",
      "Epoch 14/100\n",
      "12000/12000 [==============================] - 2s 181us/step - loss: 0.3256 - acc: 0.9017 - val_loss: 0.3925 - val_acc: 0.8983\n",
      "Epoch 15/100\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.3201 - acc: 0.9027 - val_loss: 0.3914 - val_acc: 0.8972\n",
      "Epoch 16/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3125 - acc: 0.9041 - val_loss: 0.4037 - val_acc: 0.8948\n",
      "Epoch 17/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.3041 - acc: 0.9061 - val_loss: 0.4071 - val_acc: 0.8945\n",
      "Epoch 18/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.2929 - acc: 0.9094 - val_loss: 0.4159 - val_acc: 0.8928\n",
      "Epoch 19/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.2798 - acc: 0.9128 - val_loss: 0.4153 - val_acc: 0.8879\n",
      "Epoch 20/100\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.2654 - acc: 0.9175 - val_loss: 0.4342 - val_acc: 0.8885\n",
      "Epoch 21/100\n",
      "12000/12000 [==============================] - 2s 196us/step - loss: 0.2472 - acc: 0.9231 - val_loss: 0.4702 - val_acc: 0.8852\n",
      "Epoch 22/100\n",
      "12000/12000 [==============================] - 2s 201us/step - loss: 0.2300 - acc: 0.9290 - val_loss: 0.4812 - val_acc: 0.8845\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 199us/step - loss: 0.2113 - acc: 0.9345 - val_loss: 0.5123 - val_acc: 0.8833\n",
      "Epoch 24/100\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.1945 - acc: 0.9400 - val_loss: 0.5355 - val_acc: 0.8759\n",
      "Epoch 25/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.1776 - acc: 0.9449 - val_loss: 0.5674 - val_acc: 0.8794\n",
      "Epoch 26/100\n",
      "12000/12000 [==============================] - 2s 198us/step - loss: 0.1629 - acc: 0.9493 - val_loss: 0.5993 - val_acc: 0.8794\n",
      "Epoch 27/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.1478 - acc: 0.9540 - val_loss: 0.6124 - val_acc: 0.8760\n",
      "Epoch 28/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.1374 - acc: 0.9564 - val_loss: 0.6507 - val_acc: 0.8706\n",
      "Epoch 29/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.1250 - acc: 0.9602 - val_loss: 0.6979 - val_acc: 0.8712\n",
      "Epoch 30/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.1078 - acc: 0.9659 - val_loss: 0.7358 - val_acc: 0.8716\n",
      "Epoch 31/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.1002 - acc: 0.9675 - val_loss: 0.7417 - val_acc: 0.8678\n",
      "Epoch 32/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0887 - acc: 0.9709 - val_loss: 0.7631 - val_acc: 0.8715\n",
      "Epoch 33/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0778 - acc: 0.9745 - val_loss: 0.8041 - val_acc: 0.8673\n",
      "Epoch 34/100\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0672 - acc: 0.9780 - val_loss: 0.8476 - val_acc: 0.8653\n",
      "Epoch 35/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0551 - acc: 0.9820 - val_loss: 0.8808 - val_acc: 0.8697\n",
      "Epoch 36/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0526 - acc: 0.9828 - val_loss: 0.8984 - val_acc: 0.8657\n",
      "Epoch 37/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0411 - acc: 0.9869 - val_loss: 0.9373 - val_acc: 0.8634\n",
      "Epoch 38/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0324 - acc: 0.9903 - val_loss: 0.9711 - val_acc: 0.8666\n",
      "Epoch 39/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0321 - acc: 0.9907 - val_loss: 0.9826 - val_acc: 0.8613\n",
      "Epoch 40/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0354 - acc: 0.9895 - val_loss: 0.9933 - val_acc: 0.8628\n",
      "Epoch 41/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0290 - acc: 0.9915 - val_loss: 1.0036 - val_acc: 0.8627\n",
      "Epoch 42/100\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0217 - acc: 0.9942 - val_loss: 1.0274 - val_acc: 0.8661\n",
      "Epoch 43/100\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0188 - acc: 0.9950 - val_loss: 1.0595 - val_acc: 0.8650\n",
      "Epoch 44/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0165 - acc: 0.9960 - val_loss: 1.0690 - val_acc: 0.8659\n",
      "Epoch 45/100\n",
      "12000/12000 [==============================] - 2s 192us/step - loss: 0.0128 - acc: 0.9968 - val_loss: 1.0875 - val_acc: 0.8668\n",
      "Epoch 46/100\n",
      "12000/12000 [==============================] - 2s 183us/step - loss: 0.0147 - acc: 0.9962 - val_loss: 1.0802 - val_acc: 0.8664\n",
      "Epoch 47/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 1.0871 - val_acc: 0.8628\n",
      "Epoch 48/100\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.0141 - acc: 0.9964 - val_loss: 1.0954 - val_acc: 0.8658\n",
      "Epoch 49/100\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0124 - acc: 0.9967 - val_loss: 1.1081 - val_acc: 0.8642\n",
      "Epoch 50/100\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0134 - acc: 0.9966 - val_loss: 1.1224 - val_acc: 0.8683\n",
      "Epoch 51/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 1.1074 - val_acc: 0.8647\n",
      "Epoch 52/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0177 - acc: 0.9951 - val_loss: 1.0990 - val_acc: 0.8658\n",
      "Epoch 53/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0154 - acc: 0.9958 - val_loss: 1.0973 - val_acc: 0.8658\n",
      "Epoch 54/100\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.0128 - acc: 0.9966 - val_loss: 1.1129 - val_acc: 0.8646\n",
      "Epoch 55/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0103 - acc: 0.9972 - val_loss: 1.1311 - val_acc: 0.8651\n",
      "Epoch 56/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 1.1511 - val_acc: 0.8647\n",
      "Epoch 57/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0076 - acc: 0.9978 - val_loss: 1.1681 - val_acc: 0.8660\n",
      "Epoch 58/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 1.1630 - val_acc: 0.8658\n",
      "Epoch 59/100\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 1.1738 - val_acc: 0.8691\n",
      "Epoch 60/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 1.1731 - val_acc: 0.8684\n",
      "Epoch 61/100\n",
      "12000/12000 [==============================] - 2s 197us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 1.1881 - val_acc: 0.8679\n",
      "Epoch 62/100\n",
      "12000/12000 [==============================] - 2s 193us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 1.1929 - val_acc: 0.8660\n",
      "Epoch 63/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 1.1647 - val_acc: 0.8644\n",
      "Epoch 64/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0196 - acc: 0.9938 - val_loss: 1.0953 - val_acc: 0.8530\n",
      "Epoch 65/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0285 - acc: 0.9903 - val_loss: 1.0967 - val_acc: 0.8602\n",
      "Epoch 66/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0267 - acc: 0.9914 - val_loss: 1.1034 - val_acc: 0.8615\n",
      "Epoch 67/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0180 - acc: 0.9946 - val_loss: 1.1297 - val_acc: 0.8657\n",
      "Epoch 68/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0138 - acc: 0.9962 - val_loss: 1.1488 - val_acc: 0.8643\n",
      "Epoch 69/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 1.1626 - val_acc: 0.8643\n",
      "Epoch 70/100\n",
      "12000/12000 [==============================] - 2s 195us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 1.1750 - val_acc: 0.8681\n",
      "Epoch 71/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0058 - acc: 0.9979 - val_loss: 1.1747 - val_acc: 0.8686\n",
      "Epoch 72/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0053 - acc: 0.9980 - val_loss: 1.1900 - val_acc: 0.8696\n",
      "Epoch 73/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0050 - acc: 0.9980 - val_loss: 1.1923 - val_acc: 0.8691\n",
      "Epoch 74/100\n",
      "12000/12000 [==============================] - 2s 190us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 1.1949 - val_acc: 0.8717\n",
      "Epoch 75/100\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 1.2053 - val_acc: 0.8696\n",
      "Epoch 76/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 1.1951 - val_acc: 0.8727\n",
      "Epoch 77/100\n",
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0045 - acc: 0.9980 - val_loss: 1.2074 - val_acc: 0.8681\n",
      "Epoch 78/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0042 - acc: 0.9981 - val_loss: 1.2087 - val_acc: 0.8739\n",
      "Epoch 79/100\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0043 - acc: 0.9980 - val_loss: 1.2109 - val_acc: 0.8705\n",
      "Epoch 80/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0042 - acc: 0.9981 - val_loss: 1.2204 - val_acc: 0.8712\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 2s 188us/step - loss: 0.0042 - acc: 0.9981 - val_loss: 1.2165 - val_acc: 0.8738\n",
      "Epoch 82/100\n",
      "12000/12000 [==============================] - 2s 184us/step - loss: 0.0041 - acc: 0.9981 - val_loss: 1.2272 - val_acc: 0.8707\n",
      "Epoch 83/100\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0042 - acc: 0.9981 - val_loss: 1.2205 - val_acc: 0.8724\n",
      "Epoch 84/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0041 - acc: 0.9981 - val_loss: 1.2273 - val_acc: 0.8715\n",
      "Epoch 85/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0042 - acc: 0.9980 - val_loss: 1.2279 - val_acc: 0.8718\n",
      "Epoch 86/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0042 - acc: 0.9980 - val_loss: 1.2325 - val_acc: 0.8726\n",
      "Epoch 87/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0041 - acc: 0.9981 - val_loss: 1.2362 - val_acc: 0.8691\n",
      "Epoch 88/100\n",
      "12000/12000 [==============================] - 2s 191us/step - loss: 0.0150 - acc: 0.9948 - val_loss: 1.1217 - val_acc: 0.8578\n",
      "Epoch 89/100\n",
      "12000/12000 [==============================] - 2s 194us/step - loss: 0.0662 - acc: 0.9765 - val_loss: 1.0358 - val_acc: 0.8614\n",
      "Epoch 90/100\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0231 - acc: 0.9924 - val_loss: 1.0962 - val_acc: 0.8605\n",
      "Epoch 91/100\n",
      "12000/12000 [==============================] - 2s 176us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 1.1124 - val_acc: 0.8652\n",
      "Epoch 92/100\n",
      "12000/12000 [==============================] - 2s 177us/step - loss: 0.0062 - acc: 0.9978 - val_loss: 1.1420 - val_acc: 0.8671\n",
      "Epoch 93/100\n",
      "12000/12000 [==============================] - 2s 178us/step - loss: 0.0052 - acc: 0.9979 - val_loss: 1.1664 - val_acc: 0.8682\n",
      "Epoch 94/100\n",
      "12000/12000 [==============================] - 2s 181us/step - loss: 0.0042 - acc: 0.9980 - val_loss: 1.1738 - val_acc: 0.8710\n",
      "Epoch 95/100\n",
      "12000/12000 [==============================] - 2s 181us/step - loss: 0.0038 - acc: 0.9982 - val_loss: 1.1926 - val_acc: 0.8697\n",
      "Epoch 96/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0038 - acc: 0.9981 - val_loss: 1.1942 - val_acc: 0.8697\n",
      "Epoch 97/100\n",
      "12000/12000 [==============================] - 2s 186us/step - loss: 0.0037 - acc: 0.9981 - val_loss: 1.2002 - val_acc: 0.8706\n",
      "Epoch 98/100\n",
      "12000/12000 [==============================] - 2s 185us/step - loss: 0.0037 - acc: 0.9981 - val_loss: 1.2068 - val_acc: 0.8708\n",
      "Epoch 99/100\n",
      "12000/12000 [==============================] - 2s 189us/step - loss: 0.0036 - acc: 0.9981 - val_loss: 1.2105 - val_acc: 0.8705\n",
      "Epoch 100/100\n",
      "12000/12000 [==============================] - 2s 187us/step - loss: 0.0037 - acc: 0.9980 - val_loss: 1.2176 - val_acc: 0.8711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_87 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_85/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_86/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_89 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_87/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_88/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_91 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_89/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_90/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_93 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_91/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_92/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_95 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_93/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_94/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_97 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_95/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_96/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_99 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_97/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_98/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_87 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_172:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_173:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_89 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_176:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_177:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_91 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_180:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_181:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_93 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_184:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_185:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_95 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_188:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_189:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_97 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_192:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_193:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/dongjoon/jupyter_py3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_99 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_196:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'input_197:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.44077548176050185\n",
      "1 0.4361479222774506\n",
      "2 0.4266989657282829\n",
      "3 0.43035037487745287\n",
      "4 0.4298167034983635\n",
      "5 0.4355990773439407\n",
      "6 0.42206063330173493\n",
      "7 0.4231310987472534\n",
      "8 0.41793341010808943\n",
      "9 0.4011542868614197\n",
      "10 0.3965775138139725\n",
      "11 0.40693413347005847\n",
      "12 0.38964962244033813\n",
      "13 0.39251017600297927\n",
      "14 0.3914281949400902\n",
      "15 0.40368761390447616\n",
      "16 0.40707884430885316\n",
      "17 0.41585217863321305\n",
      "18 0.41525435030460356\n",
      "19 0.4341977205872536\n",
      "20 0.4702144554257393\n",
      "21 0.48117958158254626\n",
      "22 0.5122600716352462\n",
      "23 0.5354883879423141\n",
      "24 0.5673702180385589\n",
      "25 0.5993331575393677\n",
      "26 0.61243204921484\n",
      "27 0.6506737554073334\n",
      "28 0.6979088518023491\n",
      "29 0.7358159470558167\n",
      "30 0.7416766029596329\n",
      "31 0.7630865013599396\n",
      "32 0.8041192257404327\n",
      "33 0.8475710779428482\n",
      "34 0.88080206990242\n",
      "35 0.8983974355459213\n",
      "36 0.9373041707277298\n",
      "37 0.9710758846998214\n",
      "38 0.9825815004110336\n",
      "39 0.9933003598451614\n",
      "40 1.0036000549793243\n",
      "41 1.0273619449138642\n",
      "42 1.0594734972715378\n",
      "43 1.069038646221161\n",
      "44 1.0875226145982742\n",
      "45 1.0802114236354827\n",
      "46 1.0871447217464447\n",
      "47 1.0953793615102767\n",
      "48 1.1080825746059417\n",
      "49 1.1224112445116043\n",
      "50 1.1074084240198134\n",
      "51 1.0990015697479247\n",
      "52 1.0972800713777542\n",
      "53 1.112893837094307\n",
      "54 1.1310731709003448\n",
      "55 1.1511417937278747\n",
      "56 1.16812420129776\n",
      "57 1.1630365872383117\n",
      "58 1.1738030052185058\n",
      "59 1.1730991792678833\n",
      "60 1.188142653107643\n",
      "61 1.192903176546097\n",
      "62 1.1647476410865785\n",
      "63 1.0953410369157792\n",
      "64 1.096683599948883\n",
      "65 1.1033732157945633\n",
      "66 1.1296787589788437\n",
      "67 1.1488361996412277\n",
      "68 1.1625818282365799\n",
      "69 1.1750068336725235\n",
      "70 1.1747086369991302\n",
      "71 1.189965271949768\n",
      "72 1.1923193806409835\n",
      "73 1.1948705393075942\n",
      "74 1.2053202497959137\n",
      "75 1.195071233510971\n",
      "76 1.207369499206543\n",
      "77 1.2086797922849655\n",
      "78 1.2108941453695297\n",
      "79 1.2204350566864013\n",
      "80 1.2165227955579758\n",
      "81 1.2272079873085022\n",
      "82 1.22050563454628\n",
      "83 1.2272757613658904\n",
      "84 1.227875217795372\n",
      "85 1.232477287054062\n",
      "86 1.236194509267807\n",
      "87 1.121699555516243\n",
      "88 1.035829473733902\n",
      "89 1.0961814600229263\n",
      "90 1.112396034002304\n",
      "91 1.1419744116067887\n",
      "92 1.1664090961217881\n",
      "93 1.173766828775406\n",
      "94 1.192572745680809\n",
      "95 1.1942218834161757\n",
      "96 1.2002406615018844\n",
      "97 1.2068410569429397\n",
      "98 1.2104616504907608\n",
      "99 1.2175766134262085\n"
     ]
    }
   ],
   "source": [
    "def lstm_model(latent_dim, half):\n",
    "    batch_size = 1000  # Batch size for training.\n",
    "    epochs = 45  # Number of epochs to train for.\n",
    "#     latent_dim = 128  # Latent dimensionality of the encoding space.\n",
    "#     half = 64\n",
    "    num_samples = 10000  # Number of samples to train on.\n",
    "    encoder_inputs = Input(shape=(None, 6))\n",
    "    \n",
    "    encoder = Bidirectional(CuDNNLSTM(half, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = concatenate([forward_h, backward_h])\n",
    "    state_c = concatenate([forward_c, backward_c])\n",
    "    \n",
    "    \n",
    "    # only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder\n",
    "    decoder_inputs = Input(shape=(None, 12))\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(6, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Run training\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def modelFit(epoch, batchSize, latent_dim, half, X_train, y_train, y_train1):\n",
    "    model1, encoder_model1, decoder_model1 = lstm_model(latent_dim, half)\n",
    "    hist1 = model1.fit([X_train, y_train1], y_train,\n",
    "          batch_size=batchSize,\n",
    "          epochs=epoch,\n",
    "          validation_data=([X_val,y_val1], y_val),\n",
    "          verbose = 1\n",
    "         )\n",
    "    return hist1, model1, encoder_model1, decoder_model1\n",
    "\n",
    "def grid_search(latent, half,train_size, X_train, y_train, y_train1):\n",
    "    hist1, model1, encoder_model1, decoder_model1 = modelFit(5, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist2 ,model2, encoder_model2, decoder_model2 = modelFit(10, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist3 ,model3, encoder_model3, decoder_model3 = modelFit(20, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist4 ,model4, encoder_model4, decoder_model4 = modelFit(30, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist5 ,model5, encoder_model5, decoder_model5 = modelFit(50, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist6 ,model6, encoder_model6, decoder_model6 = modelFit(80, 100, latent, half, X_train, y_train, y_train1)\n",
    "    hist7 ,model7, encoder_model7, decoder_model7 = modelFit(100, 100, latent, half, X_train, y_train, y_train1)\n",
    "    #hist8 ,model8, encoder_model8, decoder_model8 = modelFit(500, 100, latent, half)\n",
    "\n",
    "    model1.save(\"models/{}_{}_5.h5\".format(train_size,half))\n",
    "    model2.save(\"models/{}_{}_10.h5\".format(train_size,half))\n",
    "    model3.save(\"models/{}_{}_20.h5\".format(train_size,half))\n",
    "    model4.save(\"models/{}_{}_30.h5\".format(train_size,half))\n",
    "    model5.save(\"models/{}_{}_50.h5\".format(train_size,half))\n",
    "    model6.save(\"models/{}_{}_80.h5\".format(train_size,half))\n",
    "    model7.save(\"models/{}_{}_100.h5\".format(train_size,half))\n",
    "    #model8.save(\"{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    encoder_model1.save(\"models/E{}_{}_5.h5\".format(train_size,half))\n",
    "    encoder_model2.save(\"models/E{}_{}_10.h5\".format(train_size,half))\n",
    "    encoder_model3.save(\"models/E{}_{}_20.h5\".format(train_size,half))\n",
    "    encoder_model4.save(\"models/E{}_{}_30.h5\".format(train_size,half))\n",
    "    encoder_model5.save(\"models/E{}_{}_50.h5\".format(train_size,half))\n",
    "    encoder_model6.save(\"models/E{}_{}_80.h5\".format(train_size,half))\n",
    "    encoder_model7.save(\"models/E{}_{}_100.h5\".format(train_size,half))\n",
    "    #encoder_model8.save(\"E{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    decoder_model1.save(\"models/D{}_{}_5.h5\".format(train_size,half))\n",
    "    decoder_model2.save(\"models/D{}_{}_10.h5\".format(train_size,half))\n",
    "    decoder_model3.save(\"models/D{}_{}_20.h5\".format(train_size,half))\n",
    "    decoder_model4.save(\"models/D{}_{}_30.h5\".format(train_size,half))\n",
    "    decoder_model5.save(\"models/D{}_{}_50.h5\".format(train_size,half))\n",
    "    decoder_model6.save(\"models/D{}_{}_80.h5\".format(train_size,half))\n",
    "    decoder_model7.save(\"models/D{}_{}_100.h5\".format(train_size,half))\n",
    "    #decoder_model8.save(\"D{}_{}_500.h5\".format(train_size,half))\n",
    "    \n",
    "    count = [i for i in range(len(hist7.history['val_loss']))]\n",
    "    for i, value in zip(count, hist7.history['val_loss']):\n",
    "        print(i, value)\n",
    "\n",
    "grid_search(4, 2, 12000, X_train, y_train, y_train1)\n",
    "grid_search(32, 16, 12000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 12000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 12000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 12000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 12000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 12000, X_train, y_train, y_train1)\n",
    "grid_search(8192, 4096, 12000, X_train, y_train, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:24000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:24000]\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:24000]\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "\n",
    "grid_search(32, 16, 24000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 24000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 24000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 24000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 24000, X_train, y_train, y_train1)\n",
    "grid_search(1024, 512, 24000, X_train, y_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:48000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:48000]\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:48000]\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "\n",
    "\n",
    "grid_search(32, 16, 48000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 48000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 48000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 48000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 48000, X_train, y_train, y_train1)\n",
    "grid_search(1048, 512, 48000, X_train, y_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('prepData/X_train_camFam3_1mutOnly_v3_chr2.npy')[:96000]\n",
    "y_train=np.load('prepData/y_train_camFam3_1mutOnly_v3_chr2.npy')[:96000]\n",
    "y_train1 = np.load('prepData/y_train1_camFam3_1mutOnly_v3_chr2.npy')[:96000]\n",
    "y_train1 = concat(X_train, y_train1)\n",
    "\n",
    "\n",
    "grid_search(32, 16, 96000, X_train, y_train, y_train1)\n",
    "grid_search(64, 32, 96000, X_train, y_train, y_train1)\n",
    "grid_search(128, 64, 96000, X_train, y_train, y_train1)\n",
    "grid_search(256, 128, 96000, X_train, y_train, y_train1)\n",
    "grid_search(512, 256, 96000, X_train, y_train, y_train1)\n",
    "grid_search(1096, 512, 96000, X_train, y_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model, encoder_model, decoder_model):\n",
    "    nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "    # Encode the input as state vectors.\n",
    "    #print(input_seq[0,0])\n",
    "    index = 0\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    #print(len(states_value))\n",
    "    #print(states_value)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 12))\n",
    "    target_seq[0][0]= np.hstack((input_seq[0,index], np.array([1,0,0,0,0,0])))\n",
    "    #print(target_seq)\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    probability = 1\n",
    "    \n",
    "    while not stop_condition:\n",
    "        index = index +1\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        #sampled_token_index = np.random.choice(6, 1, p=output_tokens[0, -1, :])[0]\n",
    "        \n",
    "        #print(output_tokens[0, -1, :])\n",
    "        sampled_char = nucleotide[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        #print(decoded_sentence)\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == 10):\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 12))\n",
    "        temp = np.zeros((6))\n",
    "        temp[sampled_token_index] = 1\n",
    "        target_seq[0][0]= np.hstack((input_seq[0, index], temp))\n",
    "        # target_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def get_prob(input_seq, target, model, encoder_model, decoder_model):\n",
    "    nucleotide = ['0', 'A', 'C', 'G', 'T', '-']\n",
    "    # Encode the input as state vectors.\n",
    "    index = 0\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, 12))\n",
    "    target_seq[0][0]= np.hstack((input_seq[0,index], np.array([1,0,0,0,0,0])))\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    probability = 1.0\n",
    "    \n",
    "    while not stop_condition:\n",
    "        index = index +1\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Sample a token\n",
    "        #print(output_tokens[0, -1, :])\n",
    "        sampled_token_index = np.argmax(target[index-1])\n",
    "        #sampled_token_index = np.random.choice(6, 1, p=output_tokens[0, -1, :])[0]\n",
    "        probability = probability * output_tokens[0, -1, :][sampled_token_index]\n",
    "        #print(output_tokens[0, -1, :])\n",
    "        sampled_char = nucleotide[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        #print(decoded_sentence)\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == 10):\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 12))\n",
    "        temp = np.zeros((6))\n",
    "        temp[sampled_token_index] = 1\n",
    "        target_seq[0][0]= np.hstack((input_seq[0, index], temp))\n",
    "        # target_seq[0, 0, sampled_token_index] = 1\n",
    "        \n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence, probability\n",
    "\n",
    "def decode_gru(input_seq, model, encoder_model, decoder_model):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, 6))\n",
    "    target_seq[0][0]= np.array([1,0,0,0,0,0])\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = nucleotide[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == 10):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 6))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def diffList(a, b):\n",
    "    count = 0\n",
    "    length = len(a)\n",
    "    for i in range(length):\n",
    "        if a[i] != b[i]:\n",
    "            count = count+1\n",
    "    return count\n",
    "def predict2(X_test, y_test, model, encoder_model, decoder_model, gru=False):\n",
    "    x_true =[]\n",
    "    y_hat =[]\n",
    "    y_true =[]\n",
    "    probList=[]\n",
    "    productProb = 0\n",
    "    for seq_index in range(len(X_test)):\n",
    "        input_seq = X_test[seq_index: seq_index + 1]\n",
    "        #print(input_seq[0])\n",
    "        if gru:\n",
    "            decoded_sentence = decode_gru(input_seq, model, encoder_model, decoder_model)\n",
    "        else :\n",
    "            decoded_sentence = decode_sequence(input_seq, model, encoder_model, decoder_model)\n",
    "        _, prob = get_prob(input_seq, y_test[seq_index], model, encoder_model, decoder_model)\n",
    "        probList.append(prob)\n",
    "        productProb = productProb+ math.log(prob)\n",
    "        input_sen = decoder(input_seq[0])\n",
    "        print(input_sen, ' -> ',\n",
    "              decoded_sentence, 'True:', decoder(y_test[seq_index]), \n",
    "              printHitMiss(decoded_sentence, decoder(y_test[seq_index])), \n",
    "              diffList(input_sen, decoded_sentence)\n",
    "             )\n",
    "        print(input_sen, ' -> ',\n",
    "              decoder(y_test[seq_index]), 'True:', decoder(y_test[seq_index]), \n",
    "              prob,\n",
    "              printHitMiss(decoded_sentence, decoder(y_test[seq_index])), \n",
    "              diffList(input_sen, decoded_sentence)\n",
    "             )\n",
    "        print()\n",
    "        x_true.append(input_sen)\n",
    "        y_hat.append(decoded_sentence)\n",
    "        y_true.append(decoder(y_test[seq_index]))\n",
    "    print(\"Mean and std of probabilities : {} , {}  \".format(np.mean(probList), np.std(probList)))\n",
    "    print(\"Sum of log probabilities : {}\".format(productProb))\n",
    "    print(\"Percentage of target and prediction being identical: {}\".format(accuracy(y_hat, y_true)))\n",
    "    print(\"Percentage of training and prediction being identical: {}\".format(accuracy(y_hat, x_true)))\n",
    "    print(\"Accuracy given mutation happened : {}\".format(accuracy2(x_true, y_hat, y_true)))\n",
    "    #print(\"Test loss : {}\".format(keras.losses.categorical_crossentropy(y_true, y_hat)))\n",
    "    return x_true, y_hat, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.summary()\n",
    "#model2.summary()\n",
    "def grid_predict(train_size, half, X_test, y_test):\n",
    "    model1 = load_model(\"{}_{}_5.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_10.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_20.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_30.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_50.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_80.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_100.h5\".format(train_size,half))\n",
    "#     model1 =load_model(\"{}_{}_500.h5\".format(train_size,half))\n",
    "\n",
    "    encoder_model1 = load_model(\"E{}_{}_5.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_10.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_20.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_30.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_50.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_80.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_100.h5\".format(train_size,half))\n",
    "#     encoder_model1 =load_model(\"E{}_{}_500.h5\".format(train_size,half))\n",
    "\n",
    "    decoder_model1 =load_model(\"D{}_{}_5.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_10.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_20.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_30.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_50.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_80.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_100.h5\".format(train_size,half))\n",
    "#     decoder_model1 =load_model(\"D{}_{}_500.h5\".format(train_size,half))\n",
    "    #return model1, encoder_model1, decoder_model1\n",
    "    x_true, y_hat, y_true = predict2(X_test, y_test, model1, encoder_model1, decoder_model1, gru=False)\n",
    "model1, encoder_model1, decoder_model1 = grid_predict(12000, 128, X_test, y_test)\n",
    "\n",
    "encoder_model1.summary()\n",
    "decoder_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [i for i in range(len(hist7.history['val_loss']))]\n",
    "for i, value in zip(count, hist7.history['val_loss']):\n",
    "    print(i, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotgraph(hist1):\n",
    "    plt.plot(hist1.history['val_acc'])\n",
    "    plt.plot(hist1.history['acc'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['validation', 'train'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(hist1.history['val_loss'])\n",
    "    plt.plot(hist1.history['loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['validation', 'train'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "x_true, y_hat, y_true = predict2(X_test, y_test, model1, encoder_model1, decoder_model1, gru=False)\n",
    "#plotgraph(hist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true2, y_hat2, y_true2 = predict2(X_test, y_test, model2, encoder_model2, decoder_model2)\n",
    "plotgraph(hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true3, y_hat3, y_true3 = predict2(X_test, y_test, model3, encoder_model3, decoder_model3)\n",
    "plotgraph(hist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true4, y_hat4, y_true4 = predict2(X_test, y_test, model4, encoder_model4, decoder_model4)\n",
    "plotgraph(hist4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true5, y_hat5, y_true5 = predict2(X_test, y_test, model5, encoder_model5, decoder_model5)\n",
    "plotgraph(hist5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true6, y_hat6, y_true6 = predict2(X_test, y_test, model6, encoder_model6, decoder_model6)\n",
    "plotgraph(hist6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true7, y_hat7, y_true7 = predict2(X_test, y_test, model7, encoder_model7, decoder_model7)\n",
    "plotgraph(hist7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true8, y_hat8, y_true8 = predict2(X_test, y_test, model8, encoder_model8, decoder_model8)\n",
    "plotgraph(hist8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding ratio of mutation on predicted sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countMutation(ancestor, target, realDes): \n",
    "    AtoG = 0\n",
    "    AtoC = 0\n",
    "    AtoT= 0\n",
    "    GtoA= 0\n",
    "    GtoC= 0\n",
    "    GtoT= 0\n",
    "    CtoA= 0\n",
    "    CtoG= 0\n",
    "    CtoT= 0\n",
    "    TtoA= 0\n",
    "    TtoG= 0\n",
    "    TtoC= 0\n",
    "    CG = 0\n",
    "    CA = 0\n",
    "    CAGT = 0\n",
    "    CAAT = 0\n",
    "    transitionCorrect = 0\n",
    "    transversionCorrect = 0\n",
    "    for j in range(len(ancestor)):\n",
    "        for i in range(10):\n",
    "\n",
    "                if ancestor[j][i] == 'A' and target[j][i] == 'G':\n",
    "                    AtoG = AtoG+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect+=1\n",
    "                elif ancestor[j][i] == 'A' and target[j][i] == 'C':\n",
    "                    AtoC = AtoC+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'A' and target[j][i] == 'T':\n",
    "                    AtoT = AtoT+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'G' and target[j][i] == 'A':\n",
    "                    GtoA = GtoA+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect +=1\n",
    "                elif ancestor[j][i] == 'G' and target[j][i] == 'C':\n",
    "                    GtoC = GtoC+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'G' and target[j][i] == 'T':\n",
    "                    GtoT = GtoT+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'C' and target[j][i] == 'A':\n",
    "                    CtoA = CtoA+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'C' and target[j][i] == 'G':\n",
    "                    CtoG = CtoG+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'C' and target[j][i] == 'T':\n",
    "                    CtoT = CtoT+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect +=1\n",
    "                elif ancestor[j][i] == 'T' and target[j][i] == 'A':\n",
    "                    TtoA = TtoA+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'T' and target[j][i] == 'G':\n",
    "                    TtoG = TtoG+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transversionCorrect +=1\n",
    "                elif ancestor[j][i] == 'T' and target[j][i] == 'C':\n",
    "                    TtoC = TtoC+1\n",
    "                    if target[j][i] == realDes[j][i]:\n",
    "                        transitionCorrect +=1\n",
    "                else :\n",
    "                    transitionCorrect = transitionCorrect\n",
    "                if i<9 and ancestor[j][i] == 'C' and ancestor[j][i+1] == 'G' :\n",
    "                    CG = CG+1\n",
    "                    if target[j][i] == 'C' and target[j][i+1] == 'A' :\n",
    "                        CA = CA+1\n",
    "                if i<6 and ancestor[j][i] == 'C' and ancestor[j][i+1] == 'A' and ancestor[j][i+2] == 'A' and ancestor[j][i+3] == 'T':\n",
    "                    CAAT = CAAT+1\n",
    "#                     print(ancestor[j])\n",
    "#                     print(target[j])\n",
    "                    if target[j][i] == 'C' and target[j][i+1] == 'A'and target[j][i+2] == 'G' and target[j][i+3] == 'T':\n",
    "                        CAGT = CAGT+1\n",
    "                        #print(CAAT)\n",
    "                #print(CAAT)\n",
    "\n",
    "    print('This is A-->G :{}'.format(AtoG))\n",
    "    print('This is A-->C :{}'.format(AtoC))\n",
    "    print('This is A-->T :{}'.format(AtoT))\n",
    "    print('This is G-->A :{}'.format(GtoA))\n",
    "    print('This is G-->C :{}'.format(GtoC))\n",
    "    print('This is G-->T :{}'.format(GtoT))\n",
    "    print('This is C-->A :{}'.format(CtoA))\n",
    "    print('This is C-->G :{}'.format(CtoG))\n",
    "    print('This is C-->T :{}'.format(CtoT))\n",
    "    print('This is T-->A :{}'.format(TtoA))\n",
    "    print('This is T-->G :{}'.format(TtoG))\n",
    "    print('This is T-->C :{}'.format(TtoC))\n",
    "    numMutation = AtoG+AtoC+AtoT+GtoA+GtoC+GtoT+CtoA+CtoG+CtoT+TtoA+TtoG+TtoC\n",
    "    transition = AtoG+GtoA+CtoT+TtoC\n",
    "    transversion = numMutation-transition\n",
    "    print('Total number of mutations: {}'.format(numMutation))\n",
    "    print('Number of transitions: {}'.format( transition))\n",
    "    print('Number of transversions: {}'.format(transversion))\n",
    "    if transversion !=0 :\n",
    "        print('Ratio of transition/transversion : {}'.format(transition/transversion))\n",
    "    print('Percentage of mutation : {}'.format(numMutation/(10*(len(X_test)))))\n",
    "    print('Percentage of transition predicted correct : {}'.format(transitionCorrect/transition))\n",
    "    if transversion !=0 :\n",
    "        print('Percentage of transversion predicted correct : {}'.format(transversionCorrect/transversion))\n",
    "    \n",
    "    print('This is the ratio of CG-->CA :{}'.format(CA/CG))\n",
    "    print('This is the ratio of CAAT-->CAGT :{}'.format(CAGT/CAAT))\n",
    "    \n",
    "def positionAccuracy(x_true, y_hat, y_true):\n",
    "    n = len(x_true[0])\n",
    "    totalMut = [0] * n\n",
    "    correctMut = [0] * n\n",
    "    posAccuracy = [0] * n\n",
    "    for i in range(n):\n",
    "        count = 0\n",
    "        countCorrect = 0\n",
    "        for j in range(len(x_true)):\n",
    "            if x_true[j][i] != y_true[j][i]:\n",
    "                count = count+1\n",
    "                if y_hat[j][i]== y_true[j][i]:\n",
    "                    countCorrect = countCorrect+1\n",
    "        #print(countCorrect, count)\n",
    "        totalMut[i]=count\n",
    "        correctMut[i]=countCorrect\n",
    "        posAccuracy[i]=countCorrect/float(count)\n",
    "        \n",
    "        \n",
    "    print ('This is the accuracy for each position : {}'.format(posAccuracy))\n",
    "\n",
    "#countMutation(x_true6, y_hat6, y_true6)\n",
    "#countMutation(x_true8, y_hat8, y_true8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countMutation(x_true3, y_hat3, y_true3)\n",
    "#positionAccuracy(x_true, y_hat, y_true)\n",
    "countMutation(x_true4, y_hat4, y_true4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMutation(x_true8, y_hat8, y_true8)\n",
    "#positionAccuracy(x_true2, y_hat2, y_true2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMutation(x_true, y_true, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test5=np.load('prepData/X_test_camFam3_1mutOnly_v3.npy')\n",
    "y_test5=np.load('prepData/y_test_camFam3_1mutOnly_v3.npy')\n",
    "x_true5, y_hat5, y_true5 = predict2(X_test5, y_test5, model1, encoder_model1, decoder_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true6, y_hat6, y_true6 = predict2(X_test5, y_test5, model2, encoder_model2, decoder_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countMutation(x_true5, y_hat5, y_true5)\n",
    "countMutation(x_true6, y_hat6, y_true6)\n",
    "countMutation(x_true5, y_true5, y_true5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
